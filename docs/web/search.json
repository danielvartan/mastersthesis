[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Is Latitude Associated with Chronotype?",
    "section": "",
    "text": "Welcome\nYou are viewing the web version of this thesis. To access the print version, click here or select the PDF icon from the menu.\nAll analyses presented in this document are fully reproducible and were conducted using the R programming language and the Quarto publishing system. To explore the code and repository for this thesis, click here or use the GitHub icon in the menu. The research compendium is also accessible via The Open Science Framework by clicking here.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Is Latitude Associated with Chronotype?",
    "section": "License",
    "text": "License\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License. This means you are free to share and adapt it for any purpose, including commercial use, as long as you provide appropriate credit, link to the license, and indicate any changes made..",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Is Latitude Associated with Chronotype?",
    "section": "Citation",
    "text": "Citation\nTo cite this work, please use the following format:\nVartanian, D. (2024). Is latitude associated with chronotype? [Master’s Thesis, University of São Paulo]. https://doi.org/10.17605/OSF.IO/YGKTS\nBibTeX citation:\n@mastersthesis{vartanian2024,\n  title = {Is latitude associated with chronotype?},\n  author = {Daniel Vartanian},\n  year = {2024},\n  address = {São Paulo},\n  school  = {University of São Paulo},\n  langid = {en},\n  url = {https://doi.org/10.17605/OSF.IO/YGKTS},\n  note = {Corrected version}\n}",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "qmd/acknowledgments.html",
    "href": "qmd/acknowledgments.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This work would not have been possible without the support, love, and guidance of many remarkable individuals and organizations. I extend my deepest gratitude to:\nMy beloved partner, Salete Perroni (Sal), whose unwavering support has been my constant source of strength. My mother, for her unconditional love, and my sister and brother, for their love and companionship throughout life’s journey.\nMy scientific collaborators and dear friends, Alicia Rafaelly Vilefort Sales and Maria Augusta Medeiros de Andrade. To Professor Humberto Miguel Garay Malpartida, for his steadfast support, unwavering principles, and integrity, which shone through when it was most needed.\nMy supervisor, Professor Camilo Rodrigues Neto, who introduced me to complexity science in 2012, guided my dissertation with patience, and demonstrated exceptional virtue in mediating a challenging supervisory transition. I am equally grateful to Professor Carlos Molina Mendes, who handled this transition with remarkable speed, impartiality, and professionalism.\nMy cherished friends: Alex Azevedo Martins, Augusto Amado, Carina (Cacau) Prado, Ítalo Alves Bezerra do Nascimento, Júlia Mafra, Letícia Nery de Figueiredo, Marcelo Ricardo Fernandes Roschel, Reginaldo Noveli, Sílvia Capelanes, and Vanessa Simon Silva.\nFinally, this journey would not have been possible without the institutional support of USP’s Support Program for Student Permanence and Education (PAPFE) and the Coordination for the Improvement of Higher Education Personnel (CAPES), whose contributions have been invaluable.\n\nThis study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001, Grant number 88887.703720/2022-00.",
    "crumbs": [
      "Preliminary Sections",
      "Acknowledgments"
    ]
  },
  {
    "objectID": "qmd/epigraph.html",
    "href": "qmd/epigraph.html",
    "title": "Epigraph",
    "section": "",
    "text": "Nullius in verba1",
    "crumbs": [
      "Preliminary Sections",
      "Epigraph"
    ]
  },
  {
    "objectID": "qmd/epigraph.html#footnotes",
    "href": "qmd/epigraph.html#footnotes",
    "title": "Epigraph",
    "section": "",
    "text": "The Royal Society. (n.d.). History of the Royal Society. https://royalsociety.org/about-us/history↩︎",
    "crumbs": [
      "Preliminary Sections",
      "Epigraph"
    ]
  },
  {
    "objectID": "qmd/vernacular-abstract.html",
    "href": "qmd/vernacular-abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Vartanian, D. (2024). Is latitude associated with chronotype? [Master’s Thesis, University of São Paulo]. https://doi.org/10.17605/OSF.IO/YGKTS\n\nAlthough significant progress has been made in understanding circadian rhythms, further research with larger and more diverse samples is needed to deepen our understanding of temporal phenotypes and their variability. This thesis examines the relationship between latitude and human chronotype expression, investigating whether variations in annual sunlight exposure between equatorial and non-equatorial regions influence circadian phenotypes. The underlying premise suggests that a stronger solar zeitgeber near the equator should promote greater entrainment to the light/dark cycle, potentially reducing phenotype diversity and favoring morningness in equatorial populations. To test this hypothesis, data from \\(65,824\\) individuals distributed across a \\(33.850°\\) latitute range in Brazil were analyzed. Data collection employed the Munich ChronoType Questionnaire (MCTQ) during a single spring week (October 15–21, 2017), minimizing seasonal variations in photoperiod across regions. The analysis employed nested regression models weighted according to population proportions at the time of data collection. Contrary to expectations, results revealed no meaningful relationship between latitude and chronotype (Cohen’s \\(f^2 = 0.00308\\), \\(95\\% \\ \\text{CI}[0, 0.01214]\\)), consistent with recent findings in the field. All analytical procedures, from raw data processing through effect size estimation, were conducted using reproducible methods. These findings contribute to our evidence-based understanding of circadian rhythm regulation while challenging established assumptions in chronobiology research. While this study does not refute the hypothesis outright, the association between latitude and chronotype should remain an open scientific question rather than settled knowledge until robust evidence confirms it. \n\nKeywords: Complexity science. Complex systems. Chronobiology. Biological rhythms. Chronotypes. Circadian phenotypes. Sleep. Entrainment. Latitude. MCTQ.",
    "crumbs": [
      "Preliminary Sections",
      "Abstract"
    ]
  },
  {
    "objectID": "qmd/foreign-abstract.html",
    "href": "qmd/foreign-abstract.html",
    "title": "Resumo",
    "section": "",
    "text": "Vartanian, D. V. (2024). A latitude está associada ao cronotipo? [Dissertação de Mestrado, Universidade de São Paulo].\n\nEmbora avanços significativos tenham sido feitos na compreensão dos ritmos circadianos, pesquisas adicionais com amostras maiores e mais diversas são necessárias para aprofundar o entendimento sobre os fenótipos temporais e sua variabilidade. Esta dissertação examina a relação entre latitude e a expressão do cronotipo humano, investigando se variações na exposição anual à luz solar entre regiões equatoriais e não equatoriais influenciam os fenótipos circadianos. A premissa subjacente sugere que um zeitgeber solar mais forte ao equador promove uma maior entrainment com o ciclo claro/escuro, potencialmente reduzindo a diversidade fenotípica e favorecendo a matutinidade em populações equatoriais. Para testar essa hipótese, foram analisados dados de \\(65.824\\) indivíduos distribuídos ao longo de um intervalo latitudinal de \\(33,85026°\\) no Brasil. A coleta de dados foi realizada com o Munich ChronoType Questionnaire (MCTQ) durante uma única semana de primavera (15–21 de outubro de 2017), minimizando variações sazonais no fotoperíodo entre as regiões. A análise empregou modelos de regressão aninhados ponderados de acordo com as proporções populacionais no momento da coleta. Contrariando as expectativas, os resultados não indicaram uma relação significativa entre latitude e cronotipo (\\(f^2\\) de Cohen \\(= 0,00308\\), \\(95\\% \\ \\text{IC}[0; 0,01214]\\)), em consonância com achados recentes da área. Todos os procedimentos analíticos, desde os dados brutos até a estimativa do tamanho do efeito, foram conduzidos por meio de métodos totalmente reprodutíveis. Esses achados contribuem para uma compreensão baseada em evidências da regulação dos ritmos circadianos, ao mesmo tempo que desafiam pressupostos estabelecidos na pesquisa em cronobiologia. Ainda que este estudo não refute completamente a hipótese, a associação entre latitude e cronotipo deve permanecer uma questão científica em aberto, em vez de ser considerada um conhecimento consolidado, até que evidências robustas a confirmem. \nPalavras-chave: Ciência da complexidade. Sistemas complexos. Cronobiologia. Ritmos biológicos. Cronotipos. Fenótipos circadianos. Sono. Entrainment. Latitude. MCTQ.",
    "crumbs": [
      "Preliminary Sections",
      "Resumo"
    ]
  },
  {
    "objectID": "qmd/chapter-1.html",
    "href": "qmd/chapter-1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "There has been a long-standing debate in the chronobiology community regarding the relationship between latitude and human circadian phenotypes (chronotypes) (e.g., Bohlen & Simpson (1973); Randler (2008); Leocadio-Miguel et al. (2017); Wang et al. (2023)), with many assuming that this association is well-established. The hypothesis is based on the varying amounts of solar radiation experienced by populations across different latitudes. Since light exposure serves as a primary zeitgeber—a periodic environmental cue that influences or regulates biological rhythms (Aschoff, 1960; Pittendrigh, 1960)—such variations, along with temperature differences, are thought to result in observable differences in chronotype distributions globally. This thesis investigates the so-called latitude or environmental hypothesis in human circadian phenotypes, addressing the question: Is latitude associated with chronotype?\nThe central hypothesis is that latitude is associated with human chronotype distributions, with populations closer to the equator exhibiting, on average, a shorter or more morning-oriented circadian phenotype compared to those living near the poles (Bohlen & Simpson, 1973; Horzum et al., 2015; Leocadio-Miguel et al., 2014, 2017; Randler, 2008). The primary objective of this study is to model and test this hypothesis by critically examining whether a meaningful association exists between latitude and circadian phenotypes in the Brazilian population.\nThis study emerged from an insightful debate with my former supervisor, sparked by results published in 2017 in the journal Scientific Reports (Leocadio-Miguel et al., 2017). In this paper, the authors conclude that there is a meaningful association between latitude and chronotype in the Brazilian population, consistent with theoretical predictions. However, the results were not as clear-cut as presented, and the methodology used to test the hypothesis was not optimal. This thesis revisits the hypothesis using an improved statistical approach, aiming to provide a more accurate and reliable answer to the research question.\nIn the following chapters, the latitude hypothesis is tested using Popper’s hypothetical-deductive method (Popper, 1972/1979) and an enhanced approach to Null Hypothesis Significance Testing (NHST), rooted in the original Neyman-Pearson framework for data testing (Neyman & Pearson, 1928a, 1928b; Perezgonzalez, 2015). This involves a series of analyses conducted on a large dataset of \\(65,824\\) individuals, collected from the Brazilian population in 2017. The dataset is based on the Munich Chronotype Questionnaire (MCTQ) (Roenneberg et al., 2003, 2012), and includes data on sleep habits and geeographical and demographic characteristics from all of Brazil’s states.\nIt is important to emphasize that this thesis does not aim to propose or discuss the mechanisms underlying the latitude-chronotype relationship. Instead, it focuses solely on the statistical association between them concerning only human populations. An association is a necessary precursor to any causal relationship—and this thesis aims to determine whether such an association exists.\nThe analyses utilized nested multiple regression models to assess the variance explained by latitude in predicting chronotype. This method of procedure builds on the method used in Leocadio-Miguel et al. (2017). The results will contribute to the ongoing debate on the latitude-chronotype relationship, offering new evidence on the influence of environmental factors on human circadian rhythms.\nIn accordance with the graduate program regulation, this thesis follows an article-based format, inspired by the structure of Reis (2020)’s PhD thesis. Chapters 2, 3, and 4 consist of essays and literature reviews related to the thesis topic that provide essential background for understanding the research. Chapter 5 presents the core investigation, including an article detailing the hypothesis test and addressing the research question. Finally, Chapter 6 offers conclusions, discusses limitations, and proposes directions for future research. Additionally, supplementary materials are provided to offer a richer, more comprehensive understanding of the research. The reader is encouraged to explore them in detail.\nAll analyses in this thesis are fully reproducible and were conducted using the R programming language (R Core Team, n.d.) alongside the Quarto publishing system (Allaire et al., n.d.).\n\n\n\n\n\nAllaire, J. J., Teague, C., Xie, Y., & Dervieux, C. (n.d.). Quarto [Computer software]. Zenodo. https://doi.org/10.5281/ZENODO.5960048\n\n\nAschoff, J. (1960). Exogenous and endogenous components in circadian rhythms. Cold Spring Harbor Symposia on Quantitative Biology, 25, 11–28. https://doi.org/10.1101/SQB.1960.025.01.004\n\n\nBohlen, J. G., & Simpson. (1973). Latitude and the human circadian system. In J. N. Mills (Ed.), Biological aspects of circadian rhythms (pp. 87–120). Plenum Press. https://doi.org/10.1007/978-1-4613-4565-7\n\n\nHorzum, M. B., Randler, C., Masal, E., Beşoluk, Ş., Önder, İ., & Vollmer, C. (2015). Morningness–eveningness and the environment hypothesis–a cross-cultural comparison of Turkish and German adolescents. Chronobiology International, 32(6), 814–821. https://doi.org/10.3109/07420528.2015.1041598\n\n\nLeocadio-Miguel, M. A., Louzada, F. M., Duarte, L. L., Areas, R. P., Alam, M., Freire, M. V., Fontenele-Araujo, J., Menna-Barreto, L., & Pedrazzoli, M. (2017). Latitudinal cline of chronotype. Scientific Reports, 7(1), 5437. https://doi.org/10.1038/s41598-017-05797-w\n\n\nLeocadio-Miguel, M. A., Oliveira, V. C. D., Pereira, D., & Pedrazzoli, M. (2014). Detecting chronotype differences associated to latitude: A comparison between Horne–Östberg and Munich Chronotype questionnaires. Annals of Human Biology, 41(2), 107–110. https://doi.org/10.3109/03014460.2013.832795\n\n\nNeyman, J., & Pearson, E. S. (1928a). On the use and interpretation of certain test criteria for purposes of statistical inference: Part I. Biometrika, 20A(1/2), 175–240. https://doi.org/10.2307/2331945\n\n\nNeyman, J., & Pearson, E. S. (1928b). On the use and interpretation of certain test criteria for purposes of statistical inference: Part II. Biometrika, 20A(3/4), 263–294. https://doi.org/10.2307/2332112\n\n\nPerezgonzalez, J. D. (2015). Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.00223\n\n\nPittendrigh, C. S. (1960). Circadian rhythms and the circadian organization of living systems. Cold Spring Harbor Symposia on Quantitative Biology, 25, 159–184. https://doi.org/10.1101/SQB.1960.025.01.015\n\n\nPopper, K. R. (1979). Objective knowledge: An evolutionary approach. Oxford University Press. (Original work published 1972)\n\n\nR Core Team. (n.d.). R: A language and environment for statistical computing [Computer software]. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nRandler, C. (2008). Morningness-eveningness comparison in adolescents from different countries around the world. Chronobiology International, 25(6), 1017–1028. https://doi.org/10.1080/07420520802551519\n\n\nReis, C. (2020). Sleep patterns in Portugal [PhD thesis, Universidade de Lisboa]. http://hdl.handle.net/10451/54147\n\n\nRoenneberg, T., Allebrandt, K. V., Merrow, M., & Vetter, C. (2012). Social jetlag and obesity. Current Biology, 22(10), 939–943. https://doi.org/10.1016/j.cub.2012.03.038\n\n\nRoenneberg, T., Wirz-Justice, A., & Merrow, M. (2003). Life between clocks: Daily temporal patterns of human chronotypes (.). Journal of Biological Rhythms, 18(1), 80–90. https://doi.org/10.1177/0748730402239679\n\n\nWang, H., Wang, S., Yu, W., & Lei, X. (2023). Consistency of chronotype measurements is affected by sleep quality, gender, longitude, and latitude. Chronobiology International, 40(7), 952–960. https://doi.org/10.1080/07420528.2023.2237118",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-2.html",
    "href": "qmd/chapter-2.html",
    "title": "2  On Chronobiology",
    "section": "",
    "text": "The dimension of time, manifest in the form of rhythms and cycles, such as the alternation of day and night and the annual transition of seasons, has consistently influenced the evolutionary trajectory of humans and all other life forms on our planet. These rhythms and cycles brought with them evolutionary pressures, resulting in the development of a temporal organization enabling organisms to survive and reproduce in response to the conditions imposed within their environments (Aschoff, 1989b; Paranjpe & Sharma, 2005; Pittendrigh, 1981, 1993). An example of this organization can be observed in the presence of different activity-rest patterns among living beings as they adapt to certain temporal niches, such as the diurnal behavior of humans and the crepuscular or nocturnal behavior of cats and certain rodents (Aschoff, 1989a; Kronfeld-Schor et al., 2017).\nFor years, scientists debated whether this organization was solely in response to environmental stimuli or if it was also present endogenously, internally, within organisms (Shackelford, 2022). One of the seminal studies describing a potential endogenous rhythmicity in living beings was conducted in 1729 by the French astronomer Jean Jacques d’Ortous de Mairan. De Mairan observed the movement of the sensitive plant (Mimosa pudica) by isolating it from the light/dark cycle and found that the plant continued to move its leaves periodically (Figure 2.1) (Mairan, 1729; Shackelford, 2022). The search for this internal timekeeper in living beings only began to solidify in the 20th century through the efforts of scientists like Jürgen Aschoff, Colin Pittendrigh, Franz Halberg, and Erwin Bünning, culminating in the establishment of the science known as chronobiology, with a significant milestone being the Cold Spring Harbor Symposium on Quantitative Biology: Biological Clocks in 1960 (Cold Spring Harbor Laboratory, n.d.; Shackelford, 2022)12. However, the recognition of endogenous rhythmicity by the global scientific community truly came in 2017 when Jeffrey Hall, Michael Rosbash, and Michael Young were awarded the Nobel Prize in Physiology or Medicine for their discoveries of molecular mechanisms that regulate the circadian rhythm3 in fruit flies (Nobel Prize Outreach AB, n.d.).\nVarious biological rhythms have already been shown and described by science. These rhythms can occur at different description levels, whether at a higher level, such as the menstrual cycle (Ecochard et al., 2024), or even at a lower level, such as rhythms expressed within cells (Buhr & Takahashi, 2013; Sartor et al., 2023). Like many other biological phenomena, these are emergent properties of complex systems found in all living beings—stable macroscopic patterns arising from the collective behavior of the system’s parts, resulting in properties not attainable by the aggregate summation (Epstein, 1999; Holland, 2014). Today, it is understood that endogenous rhythms provide organisms with an anticipatory capacity, enabling them to pre-emptively organize resources and activities (Aschoff, 1989a).\nDespite the endogenous nature of these rhythms, they can still be regulated by the external environment. Signals (cues) from the environment that occur cyclically in nature and have the ability to regulate biological rhythmic expression are called zeitgebers4. These zeitgebers act as synchronizers by entraining the phases of the rhythms (Khalsa et al., 2003; Minors et al., 1991) (Figure 2.2). Among the known zeitgebers are, for example, meal timing (Flanagan et al., 2021) and changes in environmental temperature. However, the most influential of them is the light/dark cycle (or, simply, light exposure) (Aschoff, 1960; Pittendrigh, 1960; Roenneberg & Merrow, 2016). It is understood that the day/night cycle, resulting from the rotation of the Earth, has provided the vast majority of organisms with an oscillatory system with a periodic duration of approximately 24 hours (Aschoff, 1989a; Roenneberg, Kuehnle, et al., 2007).\nThe expression of this temporal organization varies among organisms, even within the same species (Duffy et al. (2011); Silvério et al. (2024)). These variations can be attributed to differences in how organisms experience their environment or to differences in their endogenous rhythmicity, a characteristic ultimately influenced by gene expression (Roenneberg, Kumar, et al., 2007). The interplay between environmental influences and genetic predisposition results in an observable characteristic: the phenotype (Frommlet et al., 2016).\nThe various temporal characteristics of an organism can be linked to different oscillatory periods. Among these are circadian phenotypes, which refer to characteristics observed in rhythms with periods lasting about a day (Foster & Kreitzman, 2005). Another term used for these temporal phenotypes, as the name suggests, is chronotype (Ehret, 1974; Pittendrigh, 1993). This term is also often used to differentiate phenotypes on a spectrum ranging from morningness to eveningness (Horne & Östberg, 1976; Roenneberg et al., 2019).\nSleep is a phenomenon that exhibits circadian expression. By observing the sleep characteristics of individuals, it is possible to assess the distribution of circadian phenotypes within a population, thereby investigating their covariates and other relevant associations (Roenneberg et al., 2003). This is because sleep is understood to result from the interaction of two processes: a homeostatic process (The \\(\\text{S}\\) process), which is sleep-dependent and accumulates with sleep deprivation, and a circadian process (The \\(\\text{C}\\) process), whose expression can be influenced by zeitgebers such as the light/dark cycle (Borbély, 1982; Borbély et al., 2016). These two processes are illustrated in Figure 2.3. Because the circadian rhythm is a component of sleep, its characteristics can be inferred by isolating its effects from those of the \\(\\text{S}\\) process.\nBuilding on this idea, Roenneberg et al. (2003) developed the Munich Chronotype Questionnaire (MCTQ) to measure the circadian phenotype through sleep patterns. The MCTQ asks individuals about their sleep habits, such as the times they go to bed and wake up on workdays and work-free days. From this information, the MCTQ derives the midpoint of sleep on work-free days, representing the average of sleep onset and offset times (Figure 2.4). If sleep deprivation is detected on workdays, the scale adjusts the measurement accordingly. This midpoint, reflecting sleep under minimal social constraints, is considered a closer approximation of the intrinsic circadian rhythm and, therefore, a useful proxy for estimating the circadian phenotype (the \\(\\text{C}\\) process) (Leocadio-Miguel et al., 2014).\nThe MCTQ facilitates the evaluation of chronotype in population studies. This thesis employs the MCTQ to assess chronotype using data from a 2017 online survey conducted by the author, which includes responses from \\(65,824\\) Brazilians and geographical information such as postal codes. This dataset enables the investigation of potential associations between chronotype and geographic factors.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>On Chronobiology</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-2.html#footnotes",
    "href": "qmd/chapter-2.html#footnotes",
    "title": "2  On Chronobiology",
    "section": "",
    "text": "Some say the term chronobiology was coined by Franz Halberg during the Cold Spring Harbor Symposium (Menna-Barreto & Marques, 2023, p. 21).↩︎\nFrom the Greek chrónos, meaning time/duration, and biology, pertaining to the study of life (Merriam-Webster, n.d.).↩︎\nFrom the Latin circā, meaning around, and dĭes, meaning day (Latinitium, n.d.)—a rhythm with an approximately 24-hour period.↩︎\nFrom the German zeit, meaning time, and geber, meaning donor (Cambridge University Press, n.d.).↩︎",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>On Chronobiology</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-3.html",
    "href": "qmd/chapter-3.html",
    "title": "3  On Complexity Science",
    "section": "",
    "text": "Complexity science is the science dedicated to understanding emergent phenomena (Krakauer & Wolpert, 2024). Like computer science and chronobiology, it began to take shape in the second half of the 20th century1, by the convergence of several fields, such as systems theory, game theory, and nonlinear dynamics (Sayama, 2015).\nAt a fundamental level, emergence can be defined as stable macroscopic patterns arising from local interactions (Epstein, 1999). These patterns emerge from the collective actions of a system’s parts, which cannot be attained by simply summing them up (Holland, 2014) (Figure 3.1). They may give rise to new properties in a system, which can only be studied by observing the interactions within it.\nSystems that exhibit emergent properties are considered complex systems (Holland, 2014; Mitchell, 2009). In a general sense, a system can be defined as a set of interacting parts that, through their interactions, produce a global behavior (von Bertalanffy, 1968). While both complicated and complex systems consist of many interacting parts, the defining characteristic of complex systems is that they cannot be fully understood by analyzing their components in isolation (Holland, 1992). This distinction poses significant challenges, as traditional methods for studying systems are often inadequate for capturing the intricate dynamics of complex systems (Holland, 2006).\nBiological rhythms are an example of emergent properties produced by a complex system with multiple levels of interaction (Partch et al., 2014). Molecular oscillations are generated at the cellular level (Buhr & Takahashi, 2013; Merrow et al., 2005). These oscillations interact and couple with one another, forming a complex circadian network that coordinates rhythmic physiology and behavior (Foster, 2020; Raj & van Oudenaarden, 2008). Although science has not fully mapped all the pathways, it is understood that in this kaleidoscopic array of simultaneous interactions, a global rhythm emerges. Each rhythm, or clock, is itself an emergent phenomenon, interacting with others to produce a global behavior (Figure 3.2). As the parts generate these emergences, the emergent feedback to the parts, regulating and modulating functions at all levels (Roenneberg et al., 2007).\nThe entrainment of these rhythms with environmental periodicities can involve different mechanisms. For the light/dark cycle, the main zeitgeber, this involves a network of photosensitive retinal ganglion cells (pRGCs) that send signals to the suprachiasmatic nucleus (SCN) in the hypothalamus (Brainard et al., 2001; Thapan et al., 2001). The SCN then sends signals to the pineal gland, which produces melatonin, a hormone that regulates sleep-wake cycles, among other functions (Foster, 2021).\nTo model this phenomenon, one must understand how complex systems behave and can be studied. This thesis adopts a global approach to understanding the effect of light/dark cycle entrainment on circadian expressions of populations, considering potential interactions for proper system control. Given the thesis’s aim to test the latitude hypothesis, a global approach is appropriate. Alternatively, a local approach could explore entrainment in populations by modeling individuals, each with their own circadian clock, and their interactions with the environment.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>On Complexity Science</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-3.html#footnotes",
    "href": "qmd/chapter-3.html#footnotes",
    "title": "3  On Complexity Science",
    "section": "",
    "text": "Brian Castellani & Lasse Gerrits created a visual map to illustrate the different fields and components of complexity science. You can find it at https://www.art-sciencefactory.com/complexity-map_feb09.html.↩︎",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>On Complexity Science</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-4.html",
    "href": "qmd/chapter-4.html",
    "title": "4  On the Latitude Hypothesis",
    "section": "",
    "text": "The first mention of this hypothesis regarding human populations dates back to at least 1973 (Bohlen & Simpson, 1973), with earlier hints of the idea coming from Erhard Haus and Franz Halberg in 1970 (Haus & Halberg, 1970, p. 101), building on discussions initiated by Jürgen Aschoff (Aschoff, 1969). Since then, numerous studies have explored this topic, yielding somewhat conflicting results1.\nThe hypothesis, also called the environment hypothesis (Horzum et al., 2015), posits that regions closer to the poles receive, on average, less annual sunlight compared to regions near the equator (Figure 4.1). Consequently, regions around latitude \\(0°\\) are thought to have a stronger solar zeitgeber. According to chronobiological theories, this stronger zeitgeber would enhance the entrainment of circadian rhythms with the light/dark cycle, resulting in lower variability of circadian phenotypes (Aschoff (1960); Pittendrigh (1960) Aschoff (1981); Pittendrigh & Takamura (1989); Pittendrigh et al. (1991)). This reduced influence of individual endogenous periods is illustrated in Figure 4.2.\nIn contrast, populations near the poles would experience a weaker solar zeitgeber, leading to greater variability for the expression of circadian phenotypes. This disparity also would translate into differences in mean chronotype: Equatorial populations would tend to exhibit a morningness orientation, while populations at higher and low latitudes would tend toward eveningness (Bohlen & Simpson, 1973; Roenneberg et al., 2003).\nIt’s important to emphasize that the latitude hypothesis is grounded in underlying circadian rhythms, not in self-reported morningness-eveningness (ME) preference. Self-reported preference can be influenced by extraneous factors, such as social constraints. Reducing this hypothesis to individual preferences undermines its theoretical foundation and introduces unnecessary confounders. Therefore, chronotype scales focusing on the preference aspect of ME may be unsuitable for testing this hypothesis. This is illustrated by Leocadio-Miguel et al. (2014) when discussing differences between the Horne-Östberg (HO) ME questionnaire (Horne & Östberg, 1976), which treats chronotype as a psychological construct (Roenneberg et al., 2019), and the Munich Chronotype Questionnaire (Roenneberg et al., 2003), which addresses chronotype as a biological construct, in the context of the latitude hypothesis.\nWhile there is some compelling evidence for this hypothesis in some insect species (Hut et al., 2013), the same cannot be said for this association in humans. Some authors claim to found such an association (Randler (2008); Leocadio-Miguel et al. (2014); Horzum et al. (2015); Leocadio-Miguel et al. (2017); Wang et al. (2023)), but a closer look at the data reveals that the evidence is not as clear as it seems.\nFor example, Leocadio-Miguel et al. (2017) claimed to find a meaningful association between latitude and chronotype in a sample of \\(12,884\\) Brazilian participants using the HO questionnaire. However, the reported effect size was too small to be considered practically significant (even by lenient standards), with latitude explaining only approximately \\(0.388\\%\\) of the variance in chronotype (Cohen’s \\(f^2 = 0.004143174\\)) (Figure 4.3) (See the Supplemental Materials for an in-depth analysis of this result). Considering the particular emphasis that the solar zeitgeber has on the entrainment of biological rhythms (as demonstrated by numerous studies), it is unreasonable to assume that the latitude hypothesis could be supported without at least a non-negligible effect size.\nThe results from the latitude hypothesis highlight common limitations of studies relying on Null Hypothesis Significance Testing (NHST). A p-value does not measure effect size; rather, it represents the conditional probability of observing the test statistic (or a more extreme value) given that the null hypothesis is true (Cohen, 1994; Wasserstein & Lazar, 2016). As Cohen (1988, p. 16) noted, the goal in NHST is not to test whether the population effect size is literally zero, but rather whether it is negligible or trivial.\nSeveral factors may undermine this hypothesis, such as selective light exposure and social constraints (Skeldon & Dijk, 2021). To gain a more accurate understanding of the mechanisms underlying chronotype expression, it remains crucial to test this hypothesis in larger samples and with robust statistical procedures. This study aims to address this gap.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>On the Latitude Hypothesis</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-4.html#footnotes",
    "href": "qmd/chapter-4.html#footnotes",
    "title": "4  On the Latitude Hypothesis",
    "section": "",
    "text": "A systematic review on the subject is provided by Randler & Rahafar (2017).↩︎",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>On the Latitude Hypothesis</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html",
    "href": "qmd/chapter-5.html",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "",
    "text": "5.1 Abstract\nChronotypes are temporal phenotypes that reflect our internal temporal organization, a product of evolutionary pressures enabling organisms to anticipate events. These intrinsic rhythms are entrain by zeitgebers—periodical environmental stimuli with the ability to regulate biological rhythmic expression, with light exposure being the primary mechanism. Given light’s role in these systems, previous research hypothesized that latitude might significantly influence chronotypes, suggesting that populations near the equator would exhibit more morning-leaning characteristics due to more consistent light/dark cycles, while populations near the poles might display more evening-leaning tendencies with a potentially freer expression of intrinsic rhythms. To test this hypothesis, we analyzed chronotype data from a large sample of \\(65,824\\) subjects across diverse latitudes in Brazil. Our results revealed a negligeble effect size of latitude on chronotype (\\(f^2 = 0.00314, 95\\% \\ \\text{CI}[0, 0.0122]\\)), indicating that the entrainment phenomenon is far more complex than previously conceived. These findings challenge simplified environmental models of biological timing and underscore the need for more nuanced investigations into the mechanisms underlying temporal phenotypes, opening new avenues for understanding the intricate relationship between environmental cues and individual circadian rhythms.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#introduction",
    "href": "qmd/chapter-5.html#introduction",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.2 Introduction",
    "text": "5.2 Introduction\nHumans exhibit a variety of observable traits, such as eye or hair color, which are referred to as phenotypes. These phenotypes also manifest in the way our bodies function.\nA chronotype is a temporal phenotype (Ehret, 1974; Pittendrigh, 1993), typically used to refer to endogenous circadian rhythms—biological rhythms with periods close to 24 hours. Chronobiology, the science that studies biological rhythms, suggests that the evolution of these internal oscillators is closely linked to our environment, particularly to the day/night cycle. Is believed that this cycle, alongside human evolution, created environmental pressures that led to the development of temporal organization within organisms (Aschoff, 1989b; Paranjpe & Sharma, 2005; Pittendrigh, 1981). Such organization allowed organisms to predict events and better manage their needs, such as storing food for winter (Aschoff, 1989a).\nFor a temporal system to be effective, it must adapt to environmental changes. Environmental cues that regulate biological rhythms are termed zeitgebers (from the German zeit, meaning time, and geber, meaning giver (Cambridge University Press, n.d.)). These zeitgebers act as inputs that shift and synchronize biological rhythms through a process known as entrainment (Khalsa et al., 2003; Minors et al., 1991).\nThe primary zeitgeber influencing biological rhythms is the light/dark cycle, or, simply, light exposure (Aschoff, 1960; Pittendrigh, 1960; Roenneberg & Merrow, 2016). Given its significant role in entraining the biological clock, several studies have hypothesized that the latitudinal shift of the sun, due to the Earth’s axial tilt, might lead to different temporal traits in populations near the equator compared to those closer to the poles (Bohlen & Simpson, 1973; Horzum et al., 2015; Leocadio-Miguel et al., 2014, 2017; Randler, 2008). This is based on the idea that populations at low or higher latitudes experience greater fluctuations in sunlight and a weaker overall solar zeitgeber. This concept is known as the latitude hypothesis, or the environmental hypothesis of circadian rhythm regulation.\nSeveral studies have claimed to find this association in humans, but the evidence they provide is of very low quality or simply misleading (Horzum et al., 2015; Leocadio-Miguel et al., 2014, 2017; e.g., Randler, 2008; Wang et al., 2023). A notable attempt was made by Leocadio-Miguel et al. (2017), who measured the chronotype of \\(12,884\\) Brazilian subjects across a wide latitudinal range using the Horne-Östberg (HO) Morningness–Eveningness questionnaire (Horne & Östberg, 1976). Although the authors concluded that there was a meaningful association between latitude and chronotype, their results were too small to be considered practically significant (even by lenient standards), with latitude explaining only approximately \\(0.388\\%\\) of the variance in chronotype (Cohen’s \\(f^2 = 0.00414\\)). One possible explanation for this result is that the HO measures psychological traits rather than the biological states of circadian rhythms themselves (Roenneberg, Pilz, et al., 2019), suggesting it may not be the most suitable tool for testing the hypothesis (Leocadio-Miguel et al., 2014).\nBuilding on Leocadio-Miguel et al. (2017), this study offers a novel attempt to test the latitude hypothesis by employing a biological approach through the Munich ChronoType Questionnaire (MCTQ) (Roenneberg et al., 2003) and an enhanced statistical methodology. Additionally, it utilizes the largest dataset on chronotype from a single country, as far as the existing literature suggests, comprising \\(65,824\\) respondents, all residing within the same timezone in Brazil and completing the survey within a one-week window (Figure 5.1).\nCodeweighted_data |&gt;\n  plotr:::plot_brazil_municipality(\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    alpha = 0.75,\n    breaks = c(10, 500, 1000, 5000, 7500),\n    point = TRUE\n  )\n\n\nFigure 5.1: Geographical distribution of the sample used in the analysis (\\(n = 65,824\\)).  Each point represents a municipality, with its size proportional to the number of participants and color intensity increasing with participant count. The sample includes Brazilian individuals aged 18 or older, residing in the UTC-3 timezone, who completed the survey between October 15th and 21st, 2017. The size and color scale are logarithmic (\\(\\log_{10}\\)).\n\n\n\n\n\n\n\n\n\nSource: Created by the author.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#results",
    "href": "qmd/chapter-5.html#results",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.3 Results",
    "text": "5.3 Results\nThe Munich Chronotype Questionnaire (MCTQ) uses the midpoint between sleep onset (SO) and sleep end (SE) on work-free days (MSFsc), with a sleep correction (sc) applied if sleep debt is detected, as a proxy for chronotype (Roenneberg et al., 2003). For example, if an individual sleeps from \\(\\text{00:00}\\) to \\(\\text{08:00}\\), the midpoint would be \\(\\text{04:00}\\). This measure is based on the current understanding of sleep regulation, which comprises a homeostatic/sleep-dependent process (\\(\\text{S}\\) process) and a circadian process (\\(\\text{C}\\) process) (Borbély, 1982; Borbély et al., 2016). The midpoint of sleep on free days offers a way to observe unrestrained sleep behavior, thereby minimizing the influence of the \\(\\text{S}\\) process and providing a better approximation of the circadian phenotype (i.e., the \\(\\text{C}\\) process).\nOur analysis revealed an overall mean MSFsc of \\(\\text{04:28:41}\\), with an standard deviation of \\(\\text{01:26:13}\\). The distribution is shown in Figure 5.2.\nCodeweighted_data |&gt; plotr:::plot_chronotype()\n\n\nFigure 5.2: Observed distribution of the local time of the sleep-corrected midpoint between sleep onset and sleep end on work-free days (MSFsc), a proxy for chronotype.  Chronotypes are categorized into quantiles, ranging from extremely early (\\(0 |- 0.11\\)) to extremely late (\\(0.88 - 1\\)).\n\n\n\n\n\n\n\n\n\nSource: Created by the author based on a data visualization from Roenneberg, Wirz-Justice, et al. (2019, Figure 1).\n\n\n\nThis represents the midsleep point for Brazilian subjects living in the UTC-3 timezone, with an intermediate or average chronotype. Considering the \\(7–9\\) hours of sleep recommended for healthy adults by the American Academy of Sleep Medicine (AASM) (Watson et al., 2015), one might infer that this average individual, in the absence of social constraints, would typically wake up at approximately \\(\\text{08:28:41}\\).\nThe study hypothesis was tested using nested multiple regressions, based on the design of the models presented in Leocadio-Miguel et al. (2017). The core idea of nested models is to evaluate the effect of including one or more predictors on the model’s variance explanation (\\(\\text{R}^2\\)) (Maxwell et al., 2018). This is achieved by comparing a restricted model (without the latitude) with a full model (with the latitude). Cell weights, based on sex, age group, and state of residence, were applied to account for sample imbalances.\nTo ensure practical significance, the hypothesis test incorporated a minimum effect size (MES) criterion, aligning with the original Neyman-Pearson framework for data testing (Neyman & Pearson, 1928a, 1928b; Perezgonzalez, 2015). The MES was set at a Cohen’s \\(f^2\\) of \\(0.02\\) (equivalent to an \\(\\text{R}^2\\) of \\(0.01960784\\)), a lenient threshold (Cohen, 1988). Given the well-established influence of the solar zeitgeber on biological rhythm entrainment, it is unlikely that the latitude hypothesis could be meaningfully supported without demonstrating at least a non-trivial effect.\nTwo tests were conducted, both starting with the same restricted model, which included age, sex, longitude, and the average monthly Global Horizontal Irradiance (GHI) at the time of questionnaire completion as predictors related to the latitude/longitude of each respondent as predictors (\\(\\text{R}^2_{\\text{adj}} = 0.08517\\), \\(\\text{F}(4, 65818) = 1531.808\\), \\(p\\text{-value} &lt; 1e-05\\)). The first full model (A) added the average annual GHI and daylight duration for the nearest March equinox, as well as the June and December solstices, as proxies for latitude, following the methods of Leocadio-Miguel et al. (2017) (\\(\\text{R}^2_{\\text{adj}} = 0.08803\\), \\(\\text{F}(8, 65814) = 794.12\\), \\(p\\text{-value} &lt; 1e-05\\)). The second full model (B) added only latitude as a predictor (\\(\\text{R}^2_{\\text{adj}} = 0.08568\\), \\(\\text{F}(5, 65817) = 1233.588\\), \\(p\\text{-value} &lt; 1e-05\\)). All coefficients were statistically different from zero (\\(p\\text{-value} &lt; 0.05\\)). Assumption checking and residual diagnostics primarily relied on visual inspection, as formal assumption tests (e.g., Anderson-Darling) are often not recommended for large samples (Shatz, 2024). All validity assumptions were met, and no serious multicollinearity was found among the predictor variables.\nSunrise times for the nearest March and September equinoxes, as well as the June and December solstices, were excluded due to high multicollinearity. Daylight duration for the September equinox was excluded for its collinearity with daylight duration during the March equinox.\nAn ANOVA for nested models revealed a significant reduction in the residual sum of squares in both tests (A \\(\\text{F}(4, 65814) = 51.71\\), \\(p\\text{-value} &lt; 1e-05\\)) (B \\(\\text{F}(1, 65817) = 37.325\\), \\(p\\text{-value} &lt; 1e-05\\)). However, similarly to Leocadio-Miguel et al. (2017), when estimating Cohen’s \\(f^2\\) effect size, the results were below the MES (i.e., negligible) (A \\(f^2 = 0.00314, 95\\% \\ \\text{CI}[0, 0.0122]\\)) (B \\(f^2 = 0.00057, 95\\% \\ \\text{CI}[0, 0.00954]\\)).",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#discussion",
    "href": "qmd/chapter-5.html#discussion",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.4 Discussion",
    "text": "5.4 Discussion\nWe emphasize that the assumption of a causal, linear relationship between latitude and chronotype constitutes an a priori hypothesis, which this study seeks to falsify.\nDespite a broad latitudinal range (\\(33.85026\\) degrees) and a large, balanced sample, our results indicate that the effect of latitude on chronotype is negligible. Indeed, despite suggestions of a potential link in several studies, robust empirical evidence supporting this claim in humans is lacking.\nOur results align with those of Leocadio-Miguel et al. (2017), who reported a similar effect size (Cohen’s \\(f^2 = 0.004143174\\)). However, their analysis did not incorporate a minimum effect size criterion, leading to misleading interpretations.\nThe small and inconsistent nature of the latitude effect is illustrated in Figure 5.3, while Figure 5.4 displays the mean chronotype by Brazilian state. The distribution of chronotypes across latitudes is further illustrated in Figure 5.5.\nCodeweighted_data |&gt; plotr:::plot_latitude_series()\n\n\nFigure 5.3: Boxplots of observed mean MSFsc values aggregated by \\(1°\\) latitude intervals, illustrating the relationship between latitude and chronotype.  MSFsc represents the local time of the sleep-corrected midpoint between sleep onset and sleep end on work-free days, a proxy for chronotype. Higher MSFsc values indicate later chronotypes. The × symbol points to the mean. The orange line represents a linear regression. The differences in mean/median values across latitudes are minimal relative to the Munich ChronoType Questionnaire (MCTQ) scale.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.\n\n\n\nThe absence of a clear relationship between latitude and chronotype can be attributed to multiple factors. As Jürgen Aschoff might have put it, this may reflect a lack of “ecological significance” (Aschoff et al., 1972). Even if latitude does influence circadian rhythms, the effect could be too minor to detect or might be overshadowed by other, more prominent factors like social behaviors, work hours, or the widespread use of artificial lighting. Furthermore, the variations in sunlight exposure between latitudes may not be substantial enough to meaningfully impact the circadian system, which is highly responsive to light. Given that even minor light fluctuations can lead to measurable physiological changes (Khalsa et al., 2003; Minors et al., 1991), latitude alone may not be a decisive factor in determining chronotype.\nCodelimits &lt;- # Interquartile range (IQR): Q3 - Q1\n  c(\n    weighted_data |&gt;\n      dplyr::pull(msf_sc) |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric() |&gt;\n      stats::quantile(0.25, na.rm = TRUE),\n    weighted_data |&gt;\n      dplyr::pull(msf_sc) |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric() |&gt;\n      stats::quantile(0.75, na.rm = TRUE)\n  )\n\nweighted_data |&gt;\n  dplyr::mutate(\n    msf_sc =\n      msf_sc |&gt;\n      lubritime:::link_to_timeline() |&gt;\n      as.numeric()\n  ) |&gt;\n  plotr:::plot_brazil_state(\n    col_fill = \"msf_sc\",\n    year = 2017,\n    scale_type = \"continuous\",\n    breaks = seq(limits[1], limits[2], length.out = 6) |&gt; groomr::rm_caps(),\n    labels = plotr:::format_as_hm,\n    limits = limits, # !!!\n    quiet = TRUE,\n    reverse = TRUE\n  )\n\n\nFigure 5.4: Observed geographical distribution of MSFsc values by Brazilian state, illustrating how chronotype varies with latitude in Brazil.  MSFsc is a proxy for chronotype, representing the midpoint of sleep on work-free days, adjusted for sleep debt. Higher MSFsc values correspond to later chronotypes. The color scale is bounded by the first and third quartiles. Differences in mean MSFsc values across states are small and fall within a narrow range relative to the scale of the Munich ChronoType Questionnaire (MCTQ), limiting the significance of these variations.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.\n\n\n\n\n\nThe results highlight the complex nature of the human chronotype and emphasize the importance of investigating alternative factors that may influence them. The perceived link between these variables may be a consequence of prioritizing statistical rituals over statistical thinking and a tendency toward confirmation bias, rather than rigorous and unbiased data analysis.\nCodeplot &lt;-\n  weighted_data |&gt;\n  dplyr::mutate(\n    msf_sc_category = plotr:::categorize_msf_sc(msf_sc),\n    msf_sc_category = factor(\n      msf_sc_category,\n      levels = c(\n        \"Extremely early\", \"Moderately early\", \"Slightly early\",\n        \"Intermediate\", \"Slightly late\", \"Moderately late\",\n        \"Extremely late\"\n      ),\n      ordered = TRUE\n    )\n  ) |&gt;\n  plotr:::plot_brazil_point(\n    col_group = \"msf_sc_category\",\n    year = 2017,\n    size = 0.1,\n    alpha = 1,\n    print = FALSE,\n    scale_type = \"d\"\n  ) +\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    panel.grid.major = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    legend.position = \"none\"\n  )\n\nplot |&gt;\n  plotr:::rm_ggspatial_scale() +\n  ggplot2::facet_wrap(~msf_sc_category, ncol = 4, nrow = 2)\n\n\nFigure 5.5: Observed geographical distribution of MSFsc values by a spectrum of extremely early and extremely late, illustrating how chronotype varies with latitude in Brazil.  MSFsc is a proxy for chronotype, representing the midpoint of sleep on work-free days, adjusted for sleep debt. Chronotypes are categorized into quantiles, ranging from extremely early (\\(0 |- 0.11\\)) to extremely late (\\(0.88 - 1\\)). No discernible pattern emerges from the distribution of chronotypes across latitudes.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.\n\n\n\nWhile this study does not outright refute the hypothesis, the association between latitude and chronotype should remain an open scientific question rather than be treated as established knowledge until supported by robust evidence.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#methods",
    "href": "qmd/chapter-5.html#methods",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.5 Methods",
    "text": "5.5 Methods\n\n5.5.1 Measurement Instrument\nChronotypes were assessed using a sleep log based on the standard version of the Munich ChronoType Questionnaire (MCTQ) (Roenneberg et al., 2003), a well-validated and widely used self-report tool for measuring sleep-wake behavior and determining chronotype (Roenneberg, Pilz, et al., 2019). The MCTQ derives chronotype from the sleep-corrected midpoint of sleep on free days (MSFsc), which compensates for sleep debt incurred during the workweek (Roenneberg, 2012). Figure 5.6 illustrates the variables collected by the MCTQ.\n\n\nFigure 5.6: Variables measured by the Munich Chronotype Questionnaire (MCTQ). In its standard version, these variables are collected in the context of workdays and work-free days.  BT = Local time of going to bed. SPrep = Local time of preparing to sleep. SLat = Sleep latency (Duration. Time to fall asleep after preparing to sleep). SO = Local time of sleep onset. SD = Sleep duration. MS = Local time of mid-sleep. SE = Local time of sleep end. Alarm = Indicates whether the respondent uses an alarm clock. SI = “Sleep inertia” (Duration. Despite the name, this variable represents the time the respondent takes to get up after sleep end). GU = Local time of getting out of bed. TBT = Total time in bed.\n\n\nSource: Created by the author.\n\n\n\nParticipants completed an online questionnaire, which included the sleep log as well as sociodemographic (e.g., age, sex), geographic (e.g., full residential address), anthropometric (e.g., weight, height), and data on work and study routines. A version of the questionnaire, stored independently by the Internet Archive organization, can be viewed at https://web.archive.org/web/20171018043514/each.usp.br/gipso/mctq.\n\n5.5.2 Geographic Parameters\nWe obtained latitude and longitude data by geocoding participants’ residential addresses using two main resources:\n\n\nQualoCEP (Qual o CEP, 2024): A dataset of Brazilian postal codes with integrated geocoding via the Google Geocoding API. This served as our primary source.\n\nGoogle Geocoding API: Used for addresses not included in QualoCEP. We employed the tidygeocoder R package (Cambon et al., 2021) to facilitate this process.\n\nTo ensure consistency, we randomly compared results from QualoCEP and Google Geocoding API. This can be seen in the supplementary materials.\n\n5.5.3 Solar Irradiance Data\nThe solar irradiance data came from the 2017 Solar Energy Atlas of Brazil’s National Institute for Space Research (INPE) (E. B. Pereira et al., 2017). We used the Global Horizontal Irradiance (GHI) data, representing the total amount of irradiance received from above by a surface horizontal to the ground.\n\n5.5.4 Astronomical Calculations\nThe suntools R package (Bivand & Luque, n.d.) was employed to calculate sunrise, sunset times, and daylight duration for each participant’s location. These calculations are based on equations provided by Meeus (1991) and the National Oceanic and Atmospheric Administration (NOAA).\nThe dates and times of equinoxes and solstices were acquired from the Time and Date AS service (Time and Date AS, n.d.). To verify accuracy, we compared this data with the equations from Meeus (1991) and the results from the National Aeronautics and Space Administration (NASA) ModelE AR5 Simulations (National Aeronautics and Space Administration & Goddard Institute for Space Studies, n.d.).\n\n5.5.5 Sample Characteristics\nThe analysis dataset consisted of \\(65,824\\) participants aged 18 or older residing in the UTC-3 timezone. These individuals completed the survey during a one-week period from October 15th to 21st, 2017, providing a snapshot of the population at that specific time.\nThe unfiltered valid sample included \\(115,166\\) participants from all Brazilian states. The raw dataset contained \\(120,265\\) individuals, with \\(98.173\\%\\) of the responses collected between October 15th and 21st, 2017. This data collection period coincided with the promotion of the online questionnaire via a broadcast on a nationally televised Sunday show in Brazil (Rede Globo, 2017).\nBased on 2017 data from the Brazilian Institute of Geography and Statistics’s (IBGE) Continuous National Household Sample Survey (PNAD Contínua) (Instituto Brasileiro de Geografia e Estatística, n.d.), Brazil had \\(51.919\\%\\) of females and \\(48.081\\%\\) of males with an age equal to or greater than 18 years old. The sample is skewed for female subjects, with \\(66.433\\%\\) of females and \\(33.567\\%\\) of male subjects. The mean age was \\(32.109\\) (\\(\\text{SD} = 9.258\\)), ranging from \\(18\\) to \\(58.95\\) years.\nTo balance the sample, weights were incorporated into the models. These weights were calculated through cell weighting, using sex, age group, and state of residence as references, based on population estimates from IBGE for the same year as the sample.\nA survey conducted in 2019 by IBGE (2021) found that \\(82.17\\%\\) of Brazilian households had access to an internet connection. Therefore, this sample is likely to have a good representation of Brazil’s population.\nThe sample latitudinal range is \\(33.85026°\\) (\\(\\text{Min.} = -33.52156°\\), \\(\\text{Max.} = 0.32869°\\)) with a longitudinal span of \\(22.74063°\\) (\\(\\text{Min.} = -57.5531°\\), \\(\\text{Max.} = -34.81247°\\)). For comparison, Brazil has a latitudinal range of \\(39.02299°\\) (\\(\\text{Min.} = -33.75115°\\); \\(\\text{Max.} = 5.27184°\\)) and a longitudinal span of \\(45.15451°\\) (\\(\\text{Min.} = -73.99045°\\); \\(\\text{Max.} = -28.83594°\\)), according to data from IBGE collected via the geobr R package (R. H. M. Pereira & Goncalves, n.d.).\nAdditional details about the sample are available in the supplementary materials.\n\n5.5.6 Power Analysis\nTo assess the adequacy of the sample size for detecting effects reaching the Minimum Effect Size (MES) threshold (\\(f^2 = 0.02\\)), we conducted an a posteriori power analysis using the pwrss R package (Bulus, n.d.). This analysis revealed a minimum sample size of \\(1,895\\) observations per variable to achieve a power (\\(1 - \\beta\\)) of \\(0.99\\) with a significance level (\\(\\alpha\\)) of \\(0.01\\). Our sample size (\\(n = 65,824\\)) comfortably surpasses this threshold, ensuring adequate power.\n\n5.5.7 Data Munging\nData munging and analysis followed the data science framework proposed by Hadley Wickham and Garrett Grolemund (Wickham et al., 2023). All processes were conducted using the R programming language (R Core Team, n.d.) and several R packages. The tidyverse and rOpenSci peer-reviewed package ecosystem and other R packages adherents of the tidy tools manifesto (Wickham, 2023) were prioritized.\nThe MCTQ data was analyzed using the mctq R package (Vartanian, n.d.), which is part of the rOpenSci peer-reviewed ecosystem. The data pipeline was built using the rOpenSci peer-reviewed targets R package (Landau, 2021), which provides a reproducible and efficient workflow for data analysis.\nAll processes were designed to ensure result reproducibility and adherence to the FAIR principles (Findability, Accessibility, Interoperability, and Reusability) (Wilkinson et al., 2016). All analyses are fully reproducible and were conducted using Quarto computational notebooks. The renv R package (Ushey & Wickham, n.d.) was employed to ensure that the R analysis environment can be reliably restored.\n\n5.5.8 Hypothesis Test\nTo test the study hypothesis, nested multiple linear regression models were compared: a restricted model (excluding latitude) and a full model (including latitude). The restricted model included sex, age, longitude, and the average monthly Global Horizontal Irradiance (GHI) at the time of questionnaire completion related to the latitude/longitude of each respondent as predictors. Two full models were tested. The first one including the restricted predictors plus the average annual GHI (proxy for the latitude) and daylight duration for the nearest March equinox, as well as the June and December solstices—all related to the latitude/longitude of each respondent.The second one included the restricted model predictors with only the latitude decimal degrees as a predictor.\nIt is important to notice that the design of the models were based on Leocadio-Miguel et al. (2017) study. Cell weights were applied to account for sample imbalances. The models were compared using an F-test for nested models, with a Type I error probability (\\(\\alpha\\)) of \\(0.05\\).\nTo ensure practical significance, a Minimum Effect Size (MES) criterion was applied, in line with the original Neyman-Pearson framework for data testing (Neyman & Pearson, 1928a, 1928b; Perezgonzalez, 2015). The MES was set at a Cohen’s threshold for small effects (\\(f^2 = 0.02\\), equivalent to \\(\\text{R}^2 = 0.01960784\\)) (“just barely escaping triviality” (Cohen, 1988, p. 413)). Consequently, latitude was considered meaningful only if its inclusion explained at least \\(1.960784\\%\\) of the variance in the dependent variable.\nThe hypothesis can be outlined as follows:\n\nNull hypothesis (\\(\\text{H}_{0}\\)): Adding latitude does not meaningfully improve the model’s fit, indicated by a negligible change in adjusted \\(\\text{R}^{2}\\).\nAlternative Hypothesis (\\(\\text{H}_{a}\\)): Adding latitude meaningfully improves the model’s fit, indicated by an increase in adjusted \\(\\text{R}^{2}\\) exceeding the MES.\n\nFormally:\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} &lt; \\text{MES} \\\\\n\\text{H}_{a}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} \\geq \\text{MES}\n\\end{cases}\n\\]\nWhere:\n\\[\n\\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} = \\text{Adjusted} \\ \\text{R}^{2}_{\\text{full}} - \\text{Adjusted} \\ \\text{R}^{2}_{\\text{restricted}}\n\\]",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#data-availability",
    "href": "qmd/chapter-5.html#data-availability",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.6 Data Availability",
    "text": "5.6 Data Availability\nSome restrictions apply to the availability of the main research data, which contain personal and sensitive information. As a result, this data cannot be publicly shared. Data are, however, available from the author upon reasonable request.\nThe code repository is available on GitHub at https://github.com/danielvartan/mastersthesis, and the research compendium can be accessed via The Open Science Framework at the following link: https://doi.org/10.17605/OSF.IO/YGKTS.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#acknowledgments",
    "href": "qmd/chapter-5.html#acknowledgments",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.7 Acknowledgments",
    "text": "5.7 Acknowledgments\nThis study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001, Grant number 88887.703720/2022-00.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#ethics-declaration",
    "href": "qmd/chapter-5.html#ethics-declaration",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.8 Ethics Declaration",
    "text": "5.8 Ethics Declaration\nThe author declares that the study was carried out without any commercial or financial connections that could be seen as a possible competing interest.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#additional-information",
    "href": "qmd/chapter-5.html#additional-information",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.9 Additional Information",
    "text": "5.9 Additional Information\nSee the supplementary material for more information.\nCorrespondence can be sent to Daniel Vartanian (danvartan@gmail.com).",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-5.html#rights-and-permissions",
    "href": "qmd/chapter-5.html#rights-and-permissions",
    "title": "5  Is Latitude Associated with Chronotype?",
    "section": "\n5.10 Rights and Permissions",
    "text": "5.10 Rights and Permissions\nThis article is released under the Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution, and reproduction in any medium or format, as long as be given appropriate credit to the original author and the source, provide a link to the Creative Commons license, and indicate if changes were made.\n\n\n\n\nAschoff, J. (1960). Exogenous and endogenous components in circadian rhythms. Cold Spring Harbor Symposia on Quantitative Biology, 25, 11–28. https://doi.org/10.1101/SQB.1960.025.01.004\n\n\nAschoff, J. (1989a). Circadian temporal adaptation and the perception of time. International Journal of Psychophysiology, 7(2), 121–123. https://doi.org/10.1016/0167-8760(89)90071-8\n\n\nAschoff, J. (1989b). Temporal orientation: Circadian clocks in animals and humans. Animal Behaviour, 37, 881–896. https://doi.org/10.1016/0003-3472(89)90132-2\n\n\nAschoff, J., Daan, S., Figala, J., & Müller, K. (1972). Precision of entrained circadian activity rhythms under natural photoperiodic conditions. Naturwissenschaften, 59(6), 276–277. https://research.rug.nl/files/14698568/1972NaturwissAschoff.pdf\n\n\nBivand, R., & Luque, S. (n.d.). suntools: Calculate sun position, sunrise, sunset, solar noon and twilight [Computer software]. https://doi.org/10.32614/CRAN.package.suntools\n\n\nBohlen, J. G., & Simpson. (1973). Latitude and the human circadian system. In J. N. Mills (Ed.), Biological aspects of circadian rhythms (pp. 87–120). Plenum Press. https://doi.org/10.1007/978-1-4613-4565-7\n\n\nBorbély, A. A. (1982). A two process model of sleep regulation. Human Neurobiology, 1(3), 195–204. https://pubmed.ncbi.nlm.nih.gov/7185792\n\n\nBorbély, A. A., Daan, S., Wirz-Justice, A., & Deboer, T. (2016). The two-process model of sleep regulation: A reappraisal. Journal of Sleep Research, 25(2), 131–143. https://doi.org/10.1111/jsr.12371\n\n\nBulus, M. (n.d.). {pwrss}: Statistical power and sample size calculation [Computer software]. https://doi.org/10.32614/CRAN.package.pwrss\n\n\nCambon, J., Hernangómez, D., Belanger, C., & Possenriede, D. (2021). tidygeocoder: An R package for geocoding. Journal of Open Source Software, 6(65), 3544. https://doi.org/10.21105/joss.03544\n\n\nCambridge University Press. (n.d.). Cambridge dictionary. Retrieved September 21, 2023, from https://dictionary.cambridge.org/\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates.\n\n\nEhret, C. F. (1974). The sense of time: Evidence for its molecular basis in the eukaryotic gene-action system. In Advances in Biological and Medical Physics (Vol. 15, pp. 47–77). Elsevier. https://doi.org/10.1016/B978-0-12-005215-8.50009-7\n\n\nHorne, J. A., & Östberg, O. (1976). A self-assessment questionnaire to determine morningness-eveningness in human circadian rhythms. International Journal of Chronobiology, 4(2), 97–110. https://www.ncbi.nlm.nih.gov/pubmed/1027738\n\n\nHorzum, M. B., Randler, C., Masal, E., Beşoluk, Ş., Önder, İ., & Vollmer, C. (2015). Morningness–eveningness and the environment hypothesis–a cross-cultural comparison of Turkish and German adolescents. Chronobiology International, 32(6), 814–821. https://doi.org/10.3109/07420528.2015.1041598\n\n\nInstituto Brasileiro de Geografia e Estatística. (n.d.). Tabela 6407: População residente, por sexo e grupos de idade [Table 6407: Resident population, by sex and age groups] [Data set]. SIDRA. Retrieved November 16, 2023, from https://sidra.ibge.gov.br/tabela/6407\n\n\nInstituto Brasileiro de Geografia e Estatística. (2021). Pesquisa Nacional por Amostra de Domicílios Contínua: Acesso à internet e à televisão e posse de telefone móvel celular para uso pessoal 2019 [Continuous National Household Sample Survey: Internet and television access and ownership of mobile phones for personal use 2019] (p. 12). Instituto Brasileiro de Geografia e Estatística. https://biblioteca.ibge.gov.br/visualizacao/livros/liv101794_informativo.pdf\n\n\nKhalsa, S. B. S., Jewett, M. E., Cajochen, C., & Czeisler, C. A. (2003). A phase response curve to single bright light pulses in human subjects. The Journal of Physiology, 549(3), 945–952. https://doi.org/10.1113/jphysiol.2003.040477\n\n\nLandau, W. (2021). The targets R package: A dynamic make-like function-oriented pipeline toolkit for reproducibility and high-performance computing. Journal of Open Source Software, 6(57), 2959. https://doi.org/10.21105/joss.02959\n\n\nLeocadio-Miguel, M. A., Louzada, F. M., Duarte, L. L., Areas, R. P., Alam, M., Freire, M. V., Fontenele-Araujo, J., Menna-Barreto, L., & Pedrazzoli, M. (2017). Latitudinal cline of chronotype. Scientific Reports, 7(1), 5437. https://doi.org/10.1038/s41598-017-05797-w\n\n\nLeocadio-Miguel, M. A., Oliveira, V. C. D., Pereira, D., & Pedrazzoli, M. (2014). Detecting chronotype differences associated to latitude: A comparison between Horne–Östberg and Munich Chronotype questionnaires. Annals of Human Biology, 41(2), 107–110. https://doi.org/10.3109/03014460.2013.832795\n\n\nMaxwell, S. E., Delaney, H. D., & Kelley, K. (2018). Designing experiments and analyzing data: A model comparison perspective (3rd ed.). Routledge.\n\n\nMeeus, J. (1991). Astronomical algorithms. Willmann-Bell.\n\n\nMinors, D. S., Waterhouse, J. M., & Wirz-Justice, A. (1991). A human phase-response curve to light. Neuroscience Letters, 133(1), 36–40. https://doi.org/10.1016/0304-3940(91)90051-T\n\n\nNational Aeronautics and Space Administration, & Goddard Institute for Space Studies. (n.d.). Data.GISS: Time and date of vernal equinox. Retrieved November 24, 2024, from https://data.giss.nasa.gov/modelE/ar5plots/srvernal.html\n\n\nNeyman, J., & Pearson, E. S. (1928a). On the use and interpretation of certain test criteria for purposes of statistical inference: Part I. Biometrika, 20A(1/2), 175–240. https://doi.org/10.2307/2331945\n\n\nNeyman, J., & Pearson, E. S. (1928b). On the use and interpretation of certain test criteria for purposes of statistical inference: Part II. Biometrika, 20A(3/4), 263–294. https://doi.org/10.2307/2332112\n\n\nParanjpe, D. A., & Sharma, V. K. (2005). Evolution of temporal order in living organisms. Journal of Circadian Rhythms, 3. https://doi.org/10.1186/1740-3391-3-7\n\n\nPereira, E. B., Martins, F. R., Gonçalves, A. R., Costa, R., Lima, F. J. L., Rüther, R., Abreu, S. L., Tiepolo, G. M., Pereira, S. V., & Souza, J. G. (2017). Atlas brasileiro de energia solar [Brazilian atlas of solar energy] (2nd ed.). Instituto Nacional de Pesquisas Espaciais. https://doi.org/10.34024/978851700089\n\n\nPereira, R. H. M., & Goncalves, C. N. (n.d.). geobr: Download official spatial data sets of Brazil [Computer software]. https://doi.org/10.32614/CRAN.package.geobr\n\n\nPerezgonzalez, J. D. (2015). Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.00223\n\n\nPittendrigh, C. S. (1960). Circadian rhythms and the circadian organization of living systems. Cold Spring Harbor Symposia on Quantitative Biology, 25, 159–184. https://doi.org/10.1101/SQB.1960.025.01.015\n\n\nPittendrigh, C. S. (1981). Circadian systems: General perspective. In Biological rhythms (Vol. 4, pp. 57–80). Plenum Press. https://doi.org/10.1007/978-1-4615-6552-9\n\n\nPittendrigh, C. S. (1993). Temporal organization: Reflections of a darwinian clock-watcher. Annual Review of Physiology, 55(1), 17–54. https://doi.org/10.1146/annurev.ph.55.030193.000313\n\n\nQual o CEP. (2024). Banco de CEP e código IBGE [Database of ZIP codes and IBGE (Brazilian Institute of Geography and Statistics) codes] [Data set]. https://www.qualocep.com/\n\n\nR Core Team. (n.d.). R: A language and environment for statistical computing [Computer software]. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nRandler, C. (2008). Morningness-eveningness comparison in adolescents from different countries around the world. Chronobiology International, 25(6), 1017–1028. https://doi.org/10.1080/07420520802551519\n\n\nRede Globo. (2017, October 15). Metade da população se sente mal no horário de verão, revela pesquisa (TV program Fantástico) [Half of the population feels unwell during daylight saving time, research reveals] [Video recording]. Rede Globo. https://g1.globo.com/fantastico/noticia/2017/10/metade-da-populacao-se-sente-mal-no-horario-de-verao-revela-pesquisa.html\n\n\nRoenneberg, T. (2012). What is chronotype? Sleep and Biological Rhythms, 10(2), 75–76. https://doi.org/10.1111/j.1479-8425.2012.00541.x\n\n\nRoenneberg, T., & Merrow, M. (2016). The circadian clock and human health. Current Biology, 26(10), R432–R443. https://doi.org/10.1016/j.cub.2016.04.011\n\n\nRoenneberg, T., Pilz, L. K., Zerbini, G., & Winnebeck, E. C. (2019). Chronotype and social jetlag: A (self-) critical review. Biology, 8(3), 54. https://doi.org/10.3390/biology8030054\n\n\nRoenneberg, T., Wirz-Justice, A., & Merrow, M. (2003). Life between clocks: Daily temporal patterns of human chronotypes (.). Journal of Biological Rhythms, 18(1), 80–90. https://doi.org/10.1177/0748730402239679\n\n\nRoenneberg, T., Wirz-Justice, A., Skene, D. J., Ancoli-Israel, S., Wright, K. P., Dijk, D.-J., Zee, P., Gorman, M. R., Winnebeck, E. C., & Klerman, E. B. (2019). Why should we abolish daylight saving time? Journal of Biological Rhythms, 34(3), 227–230. https://doi.org/10.1177/0748730419854197\n\n\nShatz, I. (2024). Assumption-checking rather than (just) testing: The importance of visualization and effect size in statistical diagnostics. Behavior Research Methods, 56(2), 826–845. https://doi.org/10.3758/s13428-023-02072-x\n\n\nTime and Date AS. (n.d.). Solstices & equinoxes for UTC (2000–2049). Retrieved November 24, 2024, from https://www.timeanddate.com/calendar/seasons.html?year=2000&n=1440\n\n\nUshey, K., & Wickham, H. (n.d.). renv: Project environments [Computer software]. https://doi.org/10.32614/CRAN.package.renv\n\n\nVartanian, D. (n.d.). {mctq}: Munich ChronoType Questionnaire tools [Computer software]. https://docs.ropensci.org/mctq/\n\n\nWang, H., Wang, S., Yu, W., & Lei, X. (2023). Consistency of chronotype measurements is affected by sleep quality, gender, longitude, and latitude. Chronobiology International, 40(7), 952–960. https://doi.org/10.1080/07420528.2023.2237118\n\n\nWatson, N. F., Badr, M. S., Belenky, G., Bliwise, D. L., Buxton, O. M., Buysse, D., Dinges, D. F., Gangwisch, J., Grandner, M. A., Kushida, C., Malhotra, R. K., Martin, J. L., Patel, S. R., Quan, S. F., & Tasali, E. (2015). Recommended amount of sleep for a healthy adult: A joint consensus statement of the American Academy of Sleep Medicine and Sleep Research Society. Journal of Clinical Sleep Medicine, 11(6), 591–592. https://doi.org/10.5664/jcsm.4758\n\n\nWickham, H. (2023, February 23). The tidy tools manifesto. Tidyverse. https://tidyverse.tidyverse.org/articles/manifesto.html\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd ed.). O’Reilly Media. https://r4ds.hadley.nz\n\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1), 160018. https://doi.org/10.1038/sdata.2016.18",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Is Latitude Associated with Chronotype?</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-6.html",
    "href": "qmd/chapter-6.html",
    "title": "6  Conclusion",
    "section": "",
    "text": "6.1 Strengths\nThe preceding chapters have presented a comprehensive examination of evidence and analyses pertaining to the latitude hypothesis in chronobiology. At this juncture, it is essential to return to the fundamental research question that motivated this thesis: Is latitude associated with chronotype?\nThis study, using what is arguably one of the largest datasets on chronotype, found no evidence supporting the latitude hypothesis in humans.\nCurrent evidence does not support the latitude hypothesis, and some claims made in its favor warrant further examination (see Supplemental Materials). Nevertheless, the hypothesis is often cited in chronobiology research as if it were well-established. While this study does not definitively refute it, the relationship between latitude and chronotype should remain an open scientific question until strong evidence substantiates it.\nTesting any theory or hypothesis is inherently challenging and often sparks debate. In this context, it is important to acknowledge the significant strengths of this study:\nIt is also worth noting that, despite methodological improvements, this study is grounded in the same core principles and variables that underpin the latitude hypothesis. Consequently, any critique of its methods should be considered in light of the methods used by studies that support the hypothesis.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-6.html#strengths",
    "href": "qmd/chapter-6.html#strengths",
    "title": "6  Conclusion",
    "section": "",
    "text": "Large, focused sample: One of the largest chronotype datasets ever collected within a single time zone (\\(n = 65,824\\)).\n\nMinimal photoperiod variability: Data were collected over a single spring week as summer approached (October 15–21, 2017), effectively controlling for seasonal variations in daylight exposure across regions.\n\nBroad latitudinal range: The sample spans a considerable range (\\(33.85°\\)).\n\nPopulation balancing: The dataset was adjusted to reflect population proportions at the time of collection.\n\nRigorous statistical analysis: The study accounted for confounders and incorporated a practical significance threshold.\n\nFull reproducibility: All analyses, from raw data processing through effect size estimation, follow transparent and reproducible protocols available for verification.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-6.html#limitations",
    "href": "qmd/chapter-6.html#limitations",
    "title": "6  Conclusion",
    "section": "\n6.2 Limitations",
    "text": "6.2 Limitations\nWhile this study provides valuable evidence, certain limitations should be acknowledged, as they may influence the interpretation of the findings:\n\n\nSelf-report biases: The Munich Chronotype Questionnaire (MCTQ), despite being a validated instrument, is subject to recall and social desirability biases inherent in self-reported measures. The large sample size likely mitigates these biases, as suggested by the law of large numbers (DeGroot & Schervish, 2012, p. 352).\n\nValidation in portuguese: At the time of data collection, the MCTQ had not been officially validated in Portuguese (a process completed in 2020 by Reis et al. (2020)), potentially introducing minor inconsistencies. However, given its sleep log format, the impact is expected to be minimal.\n\nDaylight Saving Time (DST): The timing of data collection coincided with the onset of Daylight Saving Time (DST) in Brazil. On October 15th, 2017, the day data collection commenced (\\(80.153\\%\\) of the data used in this analysis were collected on this day), a significant portion of respondents adjusted their clocks forward by one hour. Although this adjustment could theoretically influence responses, the questions were designed to capture daily routines that were not directly affected by the DST shift. Furthermore, if DST had any effect, it would have been expected to bolster the latitude hypothesis; yet, this was not supported by the data.\n\nLatitudinal range: The latitudinal range of \\(33.85°\\), while substantial, could be questioned as potentially insufficient to detect latitude effects on chronotype. However, the absence of a meaningful association within this range suggests that any such effect, if present, would be minimal.\n\nUnexamined confounders: Although key confounders were controlled for, some variables remained unexamined (e.g., socioeconomic status, urbanization, social timing). While including these variables might enhance model precision, it could also introduce multicollinearity and overfitting concerns. In the context of the tested hypothesis, the exclusion of these additional predictors is unlikely to alter the overall conclusions.\n\nChronotype measurement: The study used sleep as a proxy for measuring chronotype, leveraging its underlying circadian processes. However, sleep is not the only marker of circadian phenotypes. More precise methods, such as Dim Light Melatonin Onset (DLMO) (Ruiz et al., 2020), can provide a direct measure of circadian phase. Yet, these approaches are often more invasive and costly, limiting both sample size and the generalizability of findings.\n\nGeneralizability: Finally, the study’s generalizability is limited to the Brazilian population. However, since the latitude hypothesis’s underlying principles are not geographically constrained, these findings provide valuable insights for future research in other regions.\n\nThese limitations, while important to consider, do not undermine the study’s findings; rather, they highlight areas where future research might further refine our understanding.",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "qmd/chapter-6.html#directions-for-future-research",
    "href": "qmd/chapter-6.html#directions-for-future-research",
    "title": "6  Conclusion",
    "section": "\n6.3 Directions for Future Research",
    "text": "6.3 Directions for Future Research\nThis thesis proposed using a global modeling approach to investigate the latitude-chronotype relationship. As demonstrated by the results of this study and others, no meaningful effect of latitude on chronotype was identified. That said, it remains possible that if such a phenomenon exists, it could be captured through a localized approach, such as agent-based modeling. This approach would simulate an environment where agents are exposed to varying light levels, while accounting for their endogenous rhythms and the circadian clock’s phase-response curve to light. The data from this thesis could serve to calibrate and validate this model.\n\n\n\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and statistics (OCLC: ocn502674206) (4th ed.). Addison-Wesley.\n\n\nPopper, K. R. (1979). Objective knowledge: An evolutionary approach. Oxford University Press. (Original work published 1972)\n\n\nPopper, K. R. (2002). Conjectures and refutations: The growth of scientific knowledge. Routledge. (Original work published 1963)\n\n\nReis, C., Madeira, S. G., Lopes, L. V., Paiva, T., & Roenneberg, T. (2020). Validation of the Portuguese variant of the Munich Chronotype Questionnaire (MCTQ-PT). Frontiers in Physiology, 11, 795. https://doi.org/10.3389/fphys.2020.00795\n\n\nRuiz, F. S., Beijamini, F., Beale, A. D., Gonçalves, B. D. S. B., Vartanian, D., Taporoski, T. P., Middleton, B., Krieger, J. E., Vallada, H., Arendt, J., Pereira, A. C., Knutson, K. L., Pedrazzoli, M., & Von Schantz, M. (2020). Early chronotype with advanced activity rhythms and dim light melatonin onset in a rural population. Journal of Pineal Research, 69(3). https://doi.org/10.1111/jpi.12675",
    "crumbs": [
      "Chapters",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "qmd/references.html",
    "href": "qmd/references.html",
    "title": "References",
    "section": "",
    "text": "In accordance with the American Psychological Association (APA) Style, 7th edition.\n\n\nAllaire, J. J., Teague, C., Xie, Y., & Dervieux, C. (n.d.).\nQuarto [Computer software]. Zenodo. https://doi.org/10.5281/ZENODO.5960048\n\n\nAllen, M. P. (1997). Understanding regression analysis. Plenum\nPress.\n\n\nAnderson, T. W. (1962). On the distribution of the two-sample Cramér-von Mises criterion. The Annals of\nMathematical Statistics, 33(3), 1148–1159. https://doi.org/10.1214/aoms/1177704477\n\n\nAnderson, T. W., & Darling, D. A. (1952). Asymptotic theory of\ncertain \"goodness of fit\" criteria based on stochastic processes.\nThe Annals of Mathematical Statistics, 23(2), 193–212.\nhttps://www.jstor.org/stable/2236446\n\n\nAnderson, T. W., & Darling, D. A. (1954). A test of goodness of fit.\nJournal of the American Statistical Association,\n49(268), 765–769. https://doi.org/10.1080/01621459.1954.10501232\n\n\nArif, S., & MacNeil, M. A. (2022). Predictive models aren’t for\ncausal inference. Ecology Letters, 25(8), 1741–1745.\nhttps://doi.org/10.1111/ele.14033\n\n\nAschoff, J. (1960). Exogenous and endogenous components in circadian\nrhythms. Cold Spring Harbor Symposia on Quantitative Biology,\n25, 11–28. https://doi.org/10.1101/SQB.1960.025.01.004\n\n\nAschoff, J. (1969). Phasenlage der Tagesperiodik in Abhängigkeit von\nJahreszeit und Breitengrad [Phasing of diurnal rhythms as a function of\nseason and latitude]. Oecologia, 3(2), 125–165. https://doi.org/10.1007/BF00416979\n\n\nAschoff, J. (Ed.). (1981). Biological rhythms. Plenum Press. https://doi.org/10.1007/978-1-4615-6552-9\n\n\nAschoff, J. (1989a). Circadian temporal adaptation and the perception of\ntime. International Journal of Psychophysiology, 7(2),\n121–123. https://doi.org/10.1016/0167-8760(89)90071-8\n\n\nAschoff, J. (1989b). Temporal orientation: Circadian clocks\nin animals and humans. Animal Behaviour, 37, 881–896.\nhttps://doi.org/10.1016/0003-3472(89)90132-2\n\n\nAschoff, J., Daan, S., Figala, J., & Müller, K. (1972). Precision of\nentrained circadian activity rhythms under natural photoperiodic\nconditions. Naturwissenschaften, 59(6), 276–277. https://research.rug.nl/files/14698568/1972NaturwissAschoff.pdf\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (2004). Regression\ndiagnostics: Identifying influential data and sources of\ncollinearity (Print ISBN: 9780471058564\nOnline ISBN: 9780471725152). John Wiley & Sons. https://doi.org/10.1002/0471725153\n\n\nBera, A. K., & Jarque, C. M. (1981). Efficient tests for normality,\nhomoscedasticity and serial independence of regression residuals:\nMonte Carlo evidence. Economics Letters,\n7(4), 313–318. https://doi.org/10.1016/0165-1765(81)90035-5\n\n\nBivand, R., & Luque, S. (n.d.). suntools: Calculate sun position,\nsunrise, sunset, solar noon and twilight [Computer software]. https://doi.org/10.32614/CRAN.package.suntools\n\n\nBohlen, J. G., & Simpson. (1973). Latitude and the human circadian\nsystem. In J. N. Mills (Ed.), Biological aspects of circadian\nrhythms (pp. 87–120). Plenum Press. https://doi.org/10.1007/978-1-4613-4565-7\n\n\nBonett, D. G., & Seier, E. (2002). A test of normality with high\nuniform power. Computational Statistics & Data Analysis,\n40(3), 435–445. https://doi.org/10.1016/S0167-9473(02)00074-9\n\n\nBorbély, A. A. (1982). A two process model of sleep regulation.\nHuman Neurobiology, 1(3), 195–204. https://pubmed.ncbi.nlm.nih.gov/7185792\n\n\nBorbély, A. A., Daan, S., Wirz-Justice, A., & Deboer, T. (2016). The\ntwo-process model of sleep regulation: A reappraisal.\nJournal of Sleep Research, 25(2), 131–143. https://doi.org/10.1111/jsr.12371\n\n\nBox, G. E. P., & Pierce, D. A. (1970). Distribution of residual\nautocorrelations in autoregressive-integrated moving average time series\nmodels. Journal of the American Statistical Association,\n65(332), 1509–1526. https://doi.org/10.1080/01621459.1970.10481180\n\n\nBrainard, G. C., Hanifin, J. P., Greeson, J. M., Byrne, B., Glickman,\nG., Gerner, E., & Rollag, M. D. (2001). Action spectrum for\nmelatonin regulation in humans: Evidence for a novel\ncircadian photoreceptor. Journal of Neuroscience,\n21(16), 6405–6412. https://doi.org/10.1523/JNEUROSCI.21-16-06405.2001\n\n\nBreusch, T. S., & Pagan, A. R. (1979). A simple test for\nheteroscedasticity and random coefficient variation.\nEconometrica, 47(5), 1287–1294. https://doi.org/10.2307/1911963\n\n\nBuhr, E. D., & Takahashi, J. S. (2013). Molecular components of the\nmammalian circadian clock. In A. Kramer & M. Merrow (Eds.),\nCircadian Clocks (Vol. 217, pp. 3–27). Springer.\nhttps://doi.org/10.1007/978-3-642-25950-0_1\n\n\nBulus, M. (n.d.). {pwrss}:\nStatistical power and sample size calculation\n[Computer software]. https://doi.org/10.32614/CRAN.package.pwrss\n\n\nCambon, J., Hernangómez, D., Belanger, C., & Possenriede, D. (2021).\ntidygeocoder: An R package for\ngeocoding. Journal of Open Source Software, 6(65),\n3544. https://doi.org/10.21105/joss.03544\n\n\nCambridge University Press. (n.d.). Cambridge dictionary.\nRetrieved September 21, 2023, from https://dictionary.cambridge.org/\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by\nexample (5th ed.). Wiley.\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral\nsciences (2nd ed.). Lawrence Erlbaum Associates.\n\n\nCohen, J. (1990). Things I have learned (so far).\nAmerican Psychologist, 45(12), 1304–1312. https://doi.org/10.1037/10109-028\n\n\nCohen, J. (1992). A power primer. Psychological Bulletin,\n112(1), 155–159. https://doi.org/10.1037/0033-2909.112.1.155\n\n\nCohen, J. (1994). The earth is round (p&lt;.05). American\nPsychologist, 49(12), 997–1003. https://doi.org/10.1037/0003-066X.49.12.997\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2002).\nApplied multiple regression/correlation analysis for the behavioral\nsciences (OCLC: ocm49903199) (3rd ed.). Lawrence Erlbaum\nAssociates.\n\n\nCold Spring Harbor Laboratory. (n.d.). 1960: Biological\nclocks, vol. XXV. Retrieved July 17, 2023, from https://symposium.cshlp.org/site/misc/topic25.xhtml\n\n\nCook, R. D. (1977). Detection of influential observation in linear\nregression. Technometrics, 19(1), 15–18. https://doi.org/10.1080/00401706.1977.10489493\n\n\nCook, R. D. (1979). Influential observations in linear regression.\nJournal of the American Statistical Association,\n74(365), 169–174. https://doi.org/10.1080/01621459.1979.10481634\n\n\nCramér, H. (1928). On the composition of elementary errors:\nFirst paper: Mathematical deductions.\nScandinavian Actuarial Journal, 1928(1), 13–74. https://doi.org/10.1080/03461238.1928.10416862\n\n\nD’Agostino, R. B. (1971). An omnibus test of normality for moderate and\nlarge size samples. Biometrika, 58(2), 341–348. https://doi.org/10.1093/biomet/58.2.341\n\n\nD’Agostino, R. B., & Belanger, A. (1990). A suggestion for using\npowerful and informative tests of normality. The American\nStatistician, 44(4), 316–321. https://doi.org/10.2307/2684359\n\n\nD’Agostino, R. B., & Pearson, E. S. (1973). Tests for departure from\nnormality. Empirical results for the distributions of b2\nand √b1. Biometrika, 60(3), 613–622. https://doi.org/10.1093/biomet/60.3.613\n\n\nDallal, G. E., & Wilkinson, L. (1986). An analytic approximation to\nthe distribution of Lilliefors’s test statistic for\nnormality. The American Statistician, 40(4), 294–296.\nhttps://doi.org/10.1080/00031305.1986.10475419\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and\nstatistics (OCLC: ocn502674206) (4th ed.). Addison-Wesley.\n\n\nDuffy, J. F., Cain, S. W., Chang, A.-M., Phillips, A. J. K., Münch, M.\nY., Gronfier, C., Wyatt, J. K., Dijk, D.-J., Wright, K. P., &\nCzeisler, C. A. (2011). Sex difference in the near-24-hour intrinsic\nperiod of the human circadian timing system. Proceedings of the\nNational Academy of Sciences, 108, 15602–15608. https://doi.org/10.1073/pnas.1010666108\n\n\nDurbin, J., & Watson, G. S. (1950). Testing for serial correlation\nin least squares regression. I. Biometrika,\n37(3–4), 409–428. https://doi.org/10.1093/biomet/37.3-4.409\n\n\nDurbin, J., & Watson, G. S. (1951). Testing for serial correlation\nin least squares regression. II. Biometrika,\n38(1–2), 159–178. https://doi.org/10.1093/biomet/38.1-2.159\n\n\nDurbin, J., & Watson, G. S. (1971). Testing for serial correlation\nin least squares regression. III. Biometrika,\n58(1), 1–19. https://doi.org/10.1093/biomet/58.1.1\n\n\nEcochard, R., Stanford, J. B., Fehring, R. J., Schneider, M., Najmabadi,\nS., & Gronfier, C. (2024). Evidence that the woman’s ovarian cycle\nis driven by an internal circamonthly timing system. Science\nAdvances, 10(15), eadg9646. https://doi.org/10.1126/sciadv.adg9646\n\n\nEhret, C. F. (1974). The sense of time: Evidence for its\nmolecular basis in the eukaryotic gene-action system. In Advances in\nBiological and Medical Physics (Vol. 15,\npp. 47–77). Elsevier. https://doi.org/10.1016/B978-0-12-005215-8.50009-7\n\n\nEpstein, J. M. (1999). Agent-based computational models and generative\nsocial science. Complexity, 4(5), 41–60. https://doi.org/10.1002/(SICI)1099-0526(199905/06)4:5&lt;41::AID-CPLX9&gt;3.0.CO;2-F\n\n\nFlanagan, A., Bechtold, D. A., Pot, G. K., & Johnston, J. D. (2021).\nChrono-nutrition: From molecular and neuronal mechanisms to\nhuman epidemiology and timed feeding patterns. Journal of\nNeurochemistry, 157(1), 53–72. https://doi.org/10.1111/jnc.15246\n\n\nFoster, R. G. (2020). Sleep, circadian rhythms and health. Interface\nFocus, 10(3), 20190098. https://doi.org/10.1098/rsfs.2019.0098\n\n\nFoster, R. G. (2021). Fundamentals of circadian entrainment by light.\nLighting Research & Technology, 53(5), 377–393. https://doi.org/10.1177/14771535211014792\n\n\nFoster, R. G., & Kreitzman, L. (2005). Rhythms of life:\nThe biological clocks that control the daily lives of every\nliving thing (Publicado originalmente em 2014.). Profile Books.\n\n\nFox, J. (2016). Applied regression analysis and generalized linear\nmodels (3rd ed.). Sage.\n\n\nFrey, B. B. (Ed.). (2022). The SAGE encyclopedia of\nresearch design (2nd ed.). SAGE Publications. https://doi.org/10.4135/9781071812082\n\n\nFrommlet, F., Bogdan, M., & Ramsey, D. (2016). Phenotype and\ngenotype: The search for influential genes (Vol. 18).\nSpringer London. https://doi.org/10.1007/978-1-4471-5310-8\n\n\nGigerenzer, G. (2004). Mindless statistics. The Journal of\nSocio-Economics, 33(5), 587–606. https://doi.org/10.1016/j.socec.2004.09.033\n\n\nGómez-de-Mariscal, E., Guerrero, V., Sneider, A., Jayatilaka, H.,\nPhillip, J. M., Wirtz, D., & Muñoz-Barrutia, A. (2021). Use of the\np-values as a size-dependent function to address practical differences\nwhen analyzing large datasets. Scientific Reports,\n11(1, 1), 20942. https://doi.org/10.1038/s41598-021-00199-5\n\n\nGoogle. (n.d.). Google Geocoding API [Computer\nsoftware]. Google. https://developers.google.com/maps/documentation/geocoding\n\n\nGreener, R. (2020, August 4). Stop testing for normality.\nMedium. https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90\n\n\nHair, J. F. (2019). Multivariate data analysis (8th ed.).\nCengage.\n\n\nHaus, E., & Halberg, F. (1970). Circannual rhythm in level and\ntiming of serum corticosterone in standardized inbred mature C-mice. Environmental Research,\n3(2), 81–106. https://doi.org/10.1016/0013-9351(70)90008-3\n\n\nHolland, J. H. (1992). Complex adaptive systems. Daedalus,\n121(1), 17–30. https://www.jstor.org/stable/20025416\n\n\nHolland, J. H. (2006). Studying complex adaptive systems. Journal of\nSystems Science and Complexity, 19(1), 1–8. https://doi.org/10.1007/s11424-006-0001-z\n\n\nHolland, J. H. (2014). Complexity: A very short\nintroduction (OCLC: ocn888011172). Oxford University Press.\n\n\nHorne, J. A., & Östberg, O. (1976). A self-assessment questionnaire\nto determine morningness-eveningness in human circadian rhythms.\nInternational Journal of Chronobiology, 4(2), 97–110.\nhttps://www.ncbi.nlm.nih.gov/pubmed/1027738\n\n\nHorzum, M. B., Randler, C., Masal, E., Beşoluk, Ş., Önder, İ., &\nVollmer, C. (2015). Morningness–eveningness and the environment\nhypothesis–a cross-cultural comparison of Turkish and\nGerman adolescents. Chronobiology International,\n32(6), 814–821. https://doi.org/10.3109/07420528.2015.1041598\n\n\nHut, R. A., Paolucci, S., Dor, R., Kyriacou, C. P., & Daan, S.\n(2013). Latitudinal clines: An evolutionary view on biological rhythms.\nProceedings of the Royal Society B: Biological Sciences,\n280(1765), 20130433. https://doi.org/10.1098/rspb.2013.0433\n\n\nInstituto Brasileiro de Geografia e Estatística. (n.d.-a). Tabela\n6407: População residente, por sexo e grupos de idade\n[Table 6407: Resident population, by sex and age groups] [Data set].\nSIDRA. Retrieved November 16, 2023, from https://sidra.ibge.gov.br/tabela/6407\n\n\nInstituto Brasileiro de Geografia e Estatística. (n.d.-b). Tabela\n6579: População residente estimada [Data set]. SIDRA. Retrieved\nNovember 16, 2023, from https://sidra.ibge.gov.br/Tabela/3939\n\n\nInstituto Brasileiro de Geografia e Estatística. (2021). Pesquisa\nNacional por Amostra de Domicílios Contínua: Acesso à internet e à\ntelevisão e posse de telefone móvel celular para uso pessoal 2019\n[Continuous National Household Sample Survey: Internet and television\naccess and ownership of mobile phones for personal use 2019] (p. 12).\nInstituto Brasileiro de Geografia e Estatística. https://biblioteca.ibge.gov.br/visualizacao/livros/liv101794_informativo.pdf\n\n\nJarque, C. M., & Bera, A. K. (1980). Efficient tests for normality,\nhomoscedasticity and serial independence of regression residuals.\nEconomics Letters, 6(3), 255–259. https://doi.org/10.1016/0165-1765(80)90024-5\n\n\nJarque, C. M., & Bera, A. K. (1987). A test for normality of\nobservations and regression residuals. International Statistical\nReview, 55(2), 163–172. https://doi.org/10.2307/1403192\n\n\nKalton, G., & Flores-Cervantes, I. (2003). Weighting methods.\nJournal of Official Statistics, 19(2), 81–97.\n\n\nKhalsa, S. B. S., Jewett, M. E., Cajochen, C., & Czeisler, C. A.\n(2003). A phase response curve to single bright light pulses in human\nsubjects. The Journal of Physiology, 549(3), 945–952.\nhttps://doi.org/10.1113/jphysiol.2003.040477\n\n\nKoenker, R. (1981). A note on studentizing a test for\nheteroscedasticity. Journal of Econometrics, 17(1),\n107–112. https://doi.org/10.1016/0304-4076(81)90062-2\n\n\nKolmogorov, A. (1933). Sulla determinazione empirica di una legge di\ndistribuzione. Giornale dell’Istituto Italiano degli Attuari,\n4.\n\n\nKozak, M., & Piepho, H.-P. (2018). What’s normal anyway?\nResidual plots are more telling than significance tests\nwhen checking ANOVA assumptions. Journal of Agronomy\nand Crop Science, 204(1), 86–98. https://doi.org/10.1111/jac.12220\n\n\nKrakauer, D., & Wolpert, D. (2024, September 4). The reality\nouroboros. Nautilus. https://nautil.us/the-reality-ouroboros-809153/\n\n\nKronfeld-Schor, N., Visser, M. E., Salis, L., & van Gils, J. A.\n(2017). Chronobiology of interspecific interactions in a changing world.\nPhilosophical Transactions of the Royal Society B: Biological\nSciences, 372(1734), 20160248. https://doi.org/10.1098/rstb.2016.0248\n\n\nKuhlman, S. J., Craig, L. M., & Duffy, J. F. (2018). Introduction to\nchronobiology. Cold Spring Harbor Perspectives in Biology,\n10(9), a033613. https://doi.org/10.1101/cshperspect.a033613\n\n\nLandau, W. (2021). The targets R package: A\ndynamic make-like function-oriented pipeline toolkit for reproducibility\nand high-performance computing. Journal of Open Source\nSoftware, 6(57), 2959. https://doi.org/10.21105/joss.02959\n\n\nLatinitium. (n.d.). Latin dictionaries. Latinitium. Retrieved\nSeptember 21, 2023, from https://latinitium.com/latin-dictionaries/\n\n\nLeocadio-Miguel, M. A., Louzada, F. M., Duarte, L. L., Areas, R. P.,\nAlam, M., Freire, M. V., Fontenele-Araujo, J., Menna-Barreto, L., &\nPedrazzoli, M. (2017). Latitudinal cline of chronotype. Scientific\nReports, 7(1), 5437. https://doi.org/10.1038/s41598-017-05797-w\n\n\nLeocadio-Miguel, M. A., Oliveira, V. C. D., Pereira, D., &\nPedrazzoli, M. (2014). Detecting chronotype differences associated to\nlatitude: A comparison between Horne–Östberg\nand Munich Chronotype questionnaires. Annals of Human\nBiology, 41(2), 107–110. https://doi.org/10.3109/03014460.2013.832795\n\n\nLewin, R. (1993). Complexity: Life at the edge of\nchaos. Collier Books.\n\n\nLilliefors, H. W. (1967). On the Kolmogorov-Smirnov test\nfor normality with mean and variance unknown. Journal of the\nAmerican Statistical Association, 62(318), 399–402. https://doi.org/10.1080/01621459.1967.10482916\n\n\nLin, M., Lucas, H. C., & Shmueli, G. (2013). Research\ncommentary—Too big to fail: Large samples and the p-value\nproblem. Information Systems Research, 24(4), 906–917.\nhttps://doi.org/10.1287/isre.2013.0480\n\n\nLjung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in\ntime series models. Biometrika, 65(2), 297–303. https://doi.org/10.1093/biomet/65.2.297\n\n\nMairan, J.-J. de. (1729). Observation botanique [Botanical observation].\nIn Histoire de l’Académie Royale des Sciences: Avec les mémoires de\nmathématique et de physique, pour la même année: Tirés des registres de\ncette académie (pp. 35–36). Imprimerie Royale. https://gallica.bnf.fr/ark:/12148/bpt6k3527h/f43.item\n\n\nMassey, F. J. (1951). The Kolmogorov-Smirnov test for\ngoodness of fit. Journal of the American Statistical\nAssociation, 46(253), 68–78. https://doi.org/10.1080/01621459.1951.10500769\n\n\nMatias, V. A., Serrano, C., Vartanian, D., Pedrazzoli, M., &\nBenedito-Silva, A. A. (2022, September 3). {actverse}: An R package for\nactigraphy data analysis [Poster]. 30th USP\nInternational Symposium of Undergraduate Research\n(SIICUSP), São Paulo. http://dx.doi.org/10.13140/RG.2.2.12760.16643\n\n\nMaxwell, S. E., Delaney, H. D., & Kelley, K. (2018). Designing\nexperiments and analyzing data: A model comparison\nperspective (3rd ed.). Routledge.\n\n\nMeeus, J. (1991). Astronomical algorithms. Willmann-Bell.\n\n\nMenna-Barreto, L., & Marques, N. (Eds.). (2023). História e\nperspectivas da cronobiologia no Brasil e na América Latina\n[History and perspectives of chronobiology in Brazil and Latin America].\nEditora da Universidade de São Paulo.\n\n\nMerriam-Webster. (n.d.). Merriam-Webster.com\ndictionary. Retrieved September 21, 2023, from https://www.merriam-webster.com/dictionary\n\n\nMerrow, M., Spoelstra, K., & Roenneberg, T. (2005). The circadian\ncycle: Daily rhythms from behaviour to genes. EMBO\nReports, 6(10), 930–935. https://doi.org/10.1038/sj.embor.7400541\n\n\nMinors, D. S., Waterhouse, J. M., & Wirz-Justice, A. (1991). A human\nphase-response curve to light. Neuroscience Letters,\n133(1), 36–40. https://doi.org/10.1016/0304-3940(91)90051-T\n\n\nMitchell, M. (2009). Complexity: A guided tour\n(OCLC: ocn216938473). Oxford University Press.\n\n\nNahhas, R. W. (2024). Introduction to regression methods for public\nhealth using R. https://www.bookdown.org/rwnahhas/RMPH/\n\n\nNational Aeronautics and Space Administration, & Goddard Institute\nfor Space Studies. (n.d.). Data.GISS: Time\nand date of vernal equinox. Retrieved November 24, 2024, from https://data.giss.nasa.gov/modelE/ar5plots/srvernal.html\n\n\nNewey, W. K., & West, K. D. (1987). A simple, positive\nsemi-definite, heteroskedasticity and autocorrelation consistent\ncovariance matrix. Econometrica, 55(3), 703–708. https://doi.org/10.2307/1913610\n\n\nNewey, W. K., & West, K. D. (1994). Automatic lag selection in\ncovariance matrix estimation. The Review of Economic Studies,\n61(4), 631–653. https://doi.org/10.2307/2297912\n\n\nNeyman, J., & Pearson, E. S. (1928a). On the use and interpretation\nof certain test criteria for purposes of statistical inference:\nPart I. Biometrika, 20A(1/2), 175–240. https://doi.org/10.2307/2331945\n\n\nNeyman, J., & Pearson, E. S. (1928b). On the use and interpretation\nof certain test criteria for purposes of statistical inference:\nPart II. Biometrika, 20A(3/4), 263–294.\nhttps://doi.org/10.2307/2332112\n\n\nNobel Prize Outreach AB. (n.d.). Press release. The Nobel\nPrize. Retrieved September 28, 2023, from https://www.nobelprize.org/prizes/medicine/2017/press-release/\n\n\nOpenStreetMap contributors. (n.d.). OpenStreetMap\n[Computer software]. OpenStreetMap Foundation. https://www.openstreetmap.org\n\n\nParanjpe, D. A., & Sharma, V. K. (2005). Evolution of temporal order\nin living organisms. Journal of Circadian Rhythms, 3.\nhttps://doi.org/10.1186/1740-3391-3-7\n\n\nPartch, C. L., Green, C. B., & Takahashi, J. S. (2014). Molecular\narchitecture of the mammalian circadian clock. Trends in Cell\nBiology, 24(2), 90–99. https://doi.org/10.1016/j.tcb.2013.07.002\n\n\nPearson, K. (1900). X. On the criterion that a given system\nof deviations from the probable in the case of a correlated system of\nvariables is such that it can be reasonably supposed to have arisen from\nrandom sampling. The London, Edinburgh, and Dublin Philosophical\nMagazine and Journal of Science, 50(302), 157–175. https://doi.org/10.1080/14786440009463897\n\n\nPereira, E. B., Martins, F. R., Gonçalves, A. R., Costa, R., Lima, F. J.\nL., Rüther, R., Abreu, S. L., Tiepolo, G. M., Pereira, S. V., &\nSouza, J. G. (2017). Atlas brasileiro de energia solar\n[Brazilian atlas of solar energy] (2nd ed.). Instituto Nacional de\nPesquisas Espaciais. https://doi.org/10.34024/978851700089\n\n\nPereira, R. H. M., & Goncalves, C. N. (n.d.). geobr: Download official spatial data\nsets of Brazil [Computer software]. https://doi.org/10.32614/CRAN.package.geobr\n\n\nPerezgonzalez, J. D. (2015). Fisher, Neyman-Pearson or\nNHST? A tutorial for teaching data testing.\nFrontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.00223\n\n\nPittendrigh, C. S. (1960). Circadian rhythms and the circadian\norganization of living systems. Cold Spring Harbor Symposia on\nQuantitative Biology, 25, 159–184. https://doi.org/10.1101/SQB.1960.025.01.015\n\n\nPittendrigh, C. S. (1981). Circadian systems: General\nperspective. In Biological rhythms (Vol. 4, pp. 57–80). Plenum\nPress. https://doi.org/10.1007/978-1-4615-6552-9\n\n\nPittendrigh, C. S. (1993). Temporal organization:\nReflections of a darwinian clock-watcher. Annual Review\nof Physiology, 55(1), 17–54. https://doi.org/10.1146/annurev.ph.55.030193.000313\n\n\nPittendrigh, C. S., Kyner, W. T., & Takamura, T. (1991). The\namplitude of circadian oscillations: Temperature\ndependence, latitudinal clines, and the photoperiodic time measurement.\nJournal of Biological Rhythms, 6(4), 299–313. https://doi.org/10.1177/074873049100600402\n\n\nPittendrigh, C. S., & Takamura, T. (1989). Latitudinal clines in the\nproperties of a circadian pacemaker. Journal of Biological\nRhythms, 4(2), 217–235. https://doi.org/10.1177/074873048900400209\n\n\nPopper, K. R. (1979). Objective knowledge: An\nevolutionary approach. Oxford University Press. (Original work\npublished 1972)\n\n\nPopper, K. R. (2002). Conjectures and refutations: The\ngrowth of scientific knowledge. Routledge. (Original work published\n1963)\n\n\nQual o CEP. (2024). Banco de CEP e código IBGE [Database of ZIP\ncodes and IBGE (Brazilian Institute of Geography and Statistics) codes]\n[Data set]. https://www.qualocep.com/\n\n\nR Core Team. (n.d.). R: A language and environment for\nstatistical computing [Computer software]. R Foundation for\nStatistical Computing. https://www.R-project.org\n\n\nRaj, A., & van Oudenaarden, A. (2008). Nature, nurture, or chance:\nStochastic gene expression and its consequences.\nCell, 135(2), 216–226. https://doi.org/10.1016/j.cell.2008.09.050\n\n\nRamsey, J. B. (1969). Tests for specification errors in classical linear\nleast-squares regression analysis. Journal of the Royal Statistical\nSociety. Series B (Methodological), 31(2), 350–371. https://doi.org/10.1111/j.2517-6161.1969.tb00796.x\n\n\nRandler, C. (2008). Morningness-eveningness comparison in adolescents\nfrom different countries around the world. Chronobiology\nInternational, 25(6), 1017–1028. https://doi.org/10.1080/07420520802551519\n\n\nRandler, C., & Rahafar, A. (2017). Latitude affects\nmorningness-eveningness: Evidence for the environment\nhypothesis based on a systematic review. Scientific Reports,\n7(1), 39976. https://doi.org/10.1038/srep39976\n\n\nRede Globo. (2017, October 15). Metade da população se sente mal no\nhorário de verão, revela pesquisa (TV program Fantástico) [Half of\nthe population feels unwell during daylight saving time, research\nreveals] [Video recording]. Rede Globo. https://g1.globo.com/fantastico/noticia/2017/10/metade-da-populacao-se-sente-mal-no-horario-de-verao-revela-pesquisa.html\n\n\nReis, C. (2020). Sleep patterns in Portugal [PhD thesis,\nUniversidade de Lisboa]. http://hdl.handle.net/10451/54147\n\n\nReis, C., Madeira, S. G., Lopes, L. V., Paiva, T., & Roenneberg, T.\n(2020). Validation of the Portuguese variant of the\nMunich Chronotype Questionnaire (MCTQ-PT).\nFrontiers in Physiology, 11, 795. https://doi.org/10.3389/fphys.2020.00795\n\n\nRoenneberg, T. (2012). What is chronotype? Sleep and Biological\nRhythms, 10(2), 75–76. https://doi.org/10.1111/j.1479-8425.2012.00541.x\n\n\nRoenneberg, T., Allebrandt, K. V., Merrow, M., & Vetter, C. (2012).\nSocial jetlag and obesity. Current Biology, 22(10),\n939–943. https://doi.org/10.1016/j.cub.2012.03.038\n\n\nRoenneberg, T., Kuehnle, T., Juda, M., Kantermann, T., Allebrandt, K.,\nGordijn, M., & Merrow, M. (2007). Epidemiology of the human\ncircadian clock. Sleep Medicine Reviews, 11(6),\n429–438. https://doi.org/10.1016/j.smrv.2007.07.005\n\n\nRoenneberg, T., Kumar, C. J., & Merrow, M. (2007). The human\ncircadian clock entrains to sun time. Current Biology,\n17(2), R44–R45. https://doi.org/10.1016/j.cub.2006.12.011\n\n\nRoenneberg, T., & Merrow, M. (2016). The circadian clock and human\nhealth. Current Biology, 26(10), R432–R443. https://doi.org/10.1016/j.cub.2016.04.011\n\n\nRoenneberg, T., Pilz, L. K., Zerbini, G., & Winnebeck, E. C. (2019).\nChronotype and social jetlag: A (self-) critical review.\nBiology, 8(3), 54. https://doi.org/10.3390/biology8030054\n\n\nRoenneberg, T., Wirz-Justice, A., & Merrow, M. (2003). Life between\nclocks: Daily temporal patterns of human chronotypes (.).\nJournal of Biological Rhythms, 18(1), 80–90. https://doi.org/10.1177/0748730402239679\n\n\nRoenneberg, T., Wirz-Justice, A., Skene, D. J., Ancoli-Israel, S.,\nWright, K. P., Dijk, D.-J., Zee, P., Gorman, M. R., Winnebeck, E. C.,\n& Klerman, E. B. (2019). Why should we abolish daylight saving time?\nJournal of Biological Rhythms, 34(3), 227–230. https://doi.org/10.1177/0748730419854197\n\n\nRuiz, F. S., Beijamini, F., Beale, A. D., Gonçalves, B. D. S. B.,\nVartanian, D., Taporoski, T. P., Middleton, B., Krieger, J. E., Vallada,\nH., Arendt, J., Pereira, A. C., Knutson, K. L., Pedrazzoli, M., &\nVon Schantz, M. (2020). Early chronotype with advanced activity rhythms\nand dim light melatonin onset in a rural population. Journal of\nPineal Research, 69(3). https://doi.org/10.1111/jpi.12675\n\n\nSartor, F., Xu, X., Popp, T., Dodd, A. N., Kovács, Á. T., & Merrow,\nM. (2023). The circadian clock of the bacterium B.\nSubtilis evokes properties of complex, multicellular\ncircadian systems. Science Advances, 9(31), eadh1308.\nhttps://doi.org/10.1126/sciadv.adh1308\n\n\nSayama, H. (2015). Introduction to the modeling and analysis of\ncomplex systems. Open SUNY Textbooks.\n\n\nSchucany, W. R., & Ng, H. K. T. (2006). Preliminary goodness-of-fit\ntests for normality do not validate the one-sample Student\nt. Communications in Statistics - Theory and Methods,\n35(12), 2275–2286. https://doi.org/10.1080/03610920600853308\n\n\nShackelford, J. (2022). An introduction to the history of\nchronobiology: Biological rhythms emerge as a subject of\nscientific research (Vol. 1). University of Pittsburgh Press.\n\n\nShapiro, S. S., & Francia, R. S. (1972). An approximate analysis of\nvariance test for normality. Journal of the American Statistical\nAssociation, 67(337), 215–216. https://doi.org/10.1080/01621459.1972.10481232\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\nfor normality (complete samples)†. Biometrika,\n52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nShatz, I. (2024). Assumption-checking rather than (just) testing:\nThe importance of visualization and effect size in\nstatistical diagnostics. Behavior Research Methods,\n56(2), 826–845. https://doi.org/10.3758/s13428-023-02072-x\n\n\nSilvério, J. T., Tachinardi, P., Langrock, R., Kramer-Sunderbrink, A.,\nOda, G. A., & Valentinuzzi, V. S. (2024). Changes in daily activity\npatterns throughout the year in a free-living South\nAmerican subterranean rodent (Ctenomys coludo).\nMammalian Biology. https://doi.org/10.1007/s42991-024-00470-y\n\n\nSkeldon, A. C., & Dijk, D.-J. (2021). Weekly and seasonal variation\nin the circadian melatonin rhythm in humans: Entrained to\nlocal clock time, social time, light exposure or sun time? Journal\nof Pineal Research, 71(1), e12746. https://doi.org/10.1111/jpi.12746\n\n\nSmirnov, N. (1948). Table for estimating the goodness of fit of\nempirical distributions. Annals of Mathematical Statistics,\n19, 279–281.\n\n\nStruck, J. (2024). Regression Diagnostics with\nR. University of Wisconsin-Madison. https://sscc.wisc.edu/sscc/pubs/RegDiag-R/\n\n\nThapan, K., Arendt, J., & Skene, D. J. (2001). An action spectrum\nfor melatonin suppression: Evidence for a novel non-rod,\nnon-cone photoreceptor system in humans. The Journal of\nPhysiology, 535(1), 261–267. https://doi.org/10.1111/j.1469-7793.2001.t01-1-00261.x\n\n\nThode, H. C. (2002). Testing for normality. Marcel Dekker.\n\n\nTime and Date AS. (n.d.). Solstices & equinoxes for\nUTC (2000–2049). Retrieved November 24, 2024, from https://www.timeanddate.com/calendar/seasons.html?year=2000&n=1440\n\n\nUshey, K., & Wickham, H. (n.d.). renv: Project environments\n[Computer software]. https://doi.org/10.32614/CRAN.package.renv\n\n\nvan der Loo, M. P. J. (n.d.). The stringdist package for approximate\nstring matching [Computer software]. https://CRAN.R-project.org/package=stringdist\n\n\nVartanian, D. (n.d.-a). {abnt}:\nQuarto format designed for theses and dissertations that\nadhere to the standards of the Brazilian Association of\nTechnical Standards (ABNT) [Computer\nsoftware]. https://github.com/danielvartan/abnt\n\n\nVartanian, D. (n.d.-b). {actverse}:\nTools for actigraphy data analysis [Computer software]. https://docs.ropensci.org/actverse/\n\n\nVartanian, D. (n.d.-c). {mctq}:\nMunich ChronoType Questionnaire tools [Computer\nsoftware]. https://docs.ropensci.org/mctq/\n\n\nVartanian, D. (2024). Is latitude associated with chronotype?\n[Data Management Plan]. DMPHub. https://doi.org/10.48321/D1B5C8068D\n\n\nViaCEP. (n.d.). ViaCEP API: Consulte CEPs\nde todo o Brasil [Computer software]. https://viacep.com.br\n\n\nViana-Mendes, J., Benedito-Silva, A. A., Andrade, M. A. M., Vartanian,\nD., Gonçalves, B. da S. B., Cipolla-Neto, J., & Pedrazzoli, M.\n(2023). Actigraphic characterization of sleep and circadian phenotypes\nof PER3 gene VNTR genotypes. Chronobiology\nInternational, 40(9), 1244–1250. https://doi.org/10.1080/07420528.2023.2256858\n\n\nvon Bertalanffy, L. (1968). General system theory:\nFoundations, development, applications. George\nBraziller.\n\n\nWang, H., Wang, S., Yu, W., & Lei, X. (2023). Consistency of\nchronotype measurements is affected by sleep quality, gender, longitude,\nand latitude. Chronobiology International, 40(7),\n952–960. https://doi.org/10.1080/07420528.2023.2237118\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA\nstatement on p-values: Context, process, and purpose.\nThe American Statistician, 70(2). https://doi.org/10.1080/00031305.2016.1154108\n\n\nWatson, N. F., Badr, M. S., Belenky, G., Bliwise, D. L., Buxton, O. M.,\nBuysse, D., Dinges, D. F., Gangwisch, J., Grandner, M. A., Kushida, C.,\nMalhotra, R. K., Martin, J. L., Patel, S. R., Quan, S. F., & Tasali,\nE. (2015). Recommended amount of sleep for a healthy adult:\nA joint consensus statement of the American\nAcademy of Sleep Medicine and Sleep Research\nSociety. Journal of Clinical Sleep Medicine,\n11(6), 591–592. https://doi.org/10.5664/jcsm.4758\n\n\nWelsch, R., & Kuh, E. (1977). Linear regression diagnostics\n(Working Paper 0173; p. 44). National Bureau of Economic Research. https://doi.org/10.3386/w0173\n\n\nWickham, H. (2023, February 23). The tidy tools manifesto.\nTidyverse. https://tidyverse.tidyverse.org/articles/manifesto.html\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for\ndata science: Import, tidy, transform, visualize, and model\ndata (2nd ed.). O’Reilly Media. https://r4ds.hadley.nz\n\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G.,\nAxton, M., Baak, A., Blomberg, N., Boiten, J.-W., Da Silva Santos, L.\nB., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M.,\nDillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B.\n(2016). The FAIR guiding principles for scientific data\nmanagement and stewardship. Scientific Data, 3(1),\n160018. https://doi.org/10.1038/sdata.2016.18",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-1.html",
    "href": "qmd/supplementary-material-1.html",
    "title": "Appendix A — Question, Objective and Hypothesis",
    "section": "",
    "text": "A.1 Overview\nThis document provides a detailed outline of the thesis’s research question, objective, and hypothesis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question, Objective and Hypothesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-1.html#question",
    "href": "qmd/supplementary-material-1.html#question",
    "title": "Appendix A — Question, Objective and Hypothesis",
    "section": "\nA.2 Question",
    "text": "A.2 Question\nEvery scientific inquiry is driven by a problem, often framed as a question. For this study, the guiding question is:\n\nIs latitude associated with chronotype?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question, Objective and Hypothesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-1.html#objective",
    "href": "qmd/supplementary-material-1.html#objective",
    "title": "Appendix A — Question, Objective and Hypothesis",
    "section": "\nA.3 Objective",
    "text": "A.3 Objective\nThe primary objective is to model and test the hypothesis underlying this question within the context of human circadian rhythms, by critically assessing whether there is a meaningful association between latitude and circadian phenotypes in the Brazilian population.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question, Objective and Hypothesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-1.html#hypothesis",
    "href": "qmd/supplementary-material-1.html#hypothesis",
    "title": "Appendix A — Question, Objective and Hypothesis",
    "section": "\nA.4 Hypothesis",
    "text": "A.4 Hypothesis\nTo address the research question, the study employs Popper’s hypothetico-deductive method. A simplified version of this approach can be view as follows:\n\n\n\n\nFigure A.1: Simplified schema of Popper’s hypothetico-deductive method.\n\n\nflowchart LR\n  A(P1) --&gt; B(TT)\n  B --&gt; C(EE)\n  C --&gt; D(P2)\n\n\n\n\n\n\n\nAs Popper explained:\n\nHere \\(\\text{P}_1\\), is the problem from which we start, \\(\\text{TT}\\) (the ‘tentative theory’) is the imaginative conjectural solution which we first reach, for example our first tentative interpretation. \\(\\text{EE}\\) (‘error- elimination’) consists of a severe critical examination of our conjecture, our tentative interpretation: it consists, for example, of the critical use of documentary evidence and, if we have at this early stage more than one conjecture at our disposal, it will also consist of a critical discussion and comparative evaluation of the competing conjectures. \\(\\text{P}_2\\) is the problem situation as it emerges from our first critical attempt to solve our problems. It leads up to our second attempt (and so on) (Popper, 1972/1979, p. 164).\n\nAs outlined in Chapter 1, the central hypothesis is the following:\n\nConjecture A.1 (Statement) Latitude is associated with chronotype distributions, with populations closer to the equator exhibiting, on average, a shorter or more morning-oriented circadian phenotype compared to those residing near the poles.\n\nThis hypothesis is grounded in early discussions in chronobiology and is supported by numerous studies, including: Bohlen & Simpson (1973), Randler (2008), Leocadio-Miguel et al. (2014), Horzum et al. (2015), and Leocadio-Miguel et al. (2017).\nTo evaluate the hypothesis, the study adopts an improved approach to Null Hypothesis Significance Testing (NHST), rooted in the original Neyman-Pearson framework for data testing (Neyman & Pearson, 1928a, 1928b; Perezgonzalez, 2015). In a simple way, the hypotheses can be stated as follows:\n\nConjecture A.2 (Data Test – Simple Version) \\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Latitude is not associated with chronotype} \\\\\n\\text{H}_{a}: \\text{Latitude is associated with chronotype}\n\\end{cases}\n\\]\n\nSupplementary Material B provides a detailed description of the statistical tests and procedures used to evaluate the hypothesis.\n\n\n\n\nBohlen, J. G., & Simpson. (1973). Latitude and the human circadian system. In J. N. Mills (Ed.), Biological aspects of circadian rhythms (pp. 87–120). Plenum Press. https://doi.org/10.1007/978-1-4613-4565-7\n\n\nHorzum, M. B., Randler, C., Masal, E., Beşoluk, Ş., Önder, İ., & Vollmer, C. (2015). Morningness–eveningness and the environment hypothesis–a cross-cultural comparison of Turkish and German adolescents. Chronobiology International, 32(6), 814–821. https://doi.org/10.3109/07420528.2015.1041598\n\n\nLeocadio-Miguel, M. A., Louzada, F. M., Duarte, L. L., Areas, R. P., Alam, M., Freire, M. V., Fontenele-Araujo, J., Menna-Barreto, L., & Pedrazzoli, M. (2017). Latitudinal cline of chronotype. Scientific Reports, 7(1), 5437. https://doi.org/10.1038/s41598-017-05797-w\n\n\nLeocadio-Miguel, M. A., Oliveira, V. C. D., Pereira, D., & Pedrazzoli, M. (2014). Detecting chronotype differences associated to latitude: A comparison between Horne–Östberg and Munich Chronotype questionnaires. Annals of Human Biology, 41(2), 107–110. https://doi.org/10.3109/03014460.2013.832795\n\n\nNeyman, J., & Pearson, E. S. (1928a). On the use and interpretation of certain test criteria for purposes of statistical inference: Part I. Biometrika, 20A(1/2), 175–240. https://doi.org/10.2307/2331945\n\n\nNeyman, J., & Pearson, E. S. (1928b). On the use and interpretation of certain test criteria for purposes of statistical inference: Part II. Biometrika, 20A(3/4), 263–294. https://doi.org/10.2307/2332112\n\n\nPerezgonzalez, J. D. (2015). Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.00223\n\n\nPopper, K. R. (1979). Objective knowledge: An evolutionary approach. Oxford University Press. (Original work published 1972)\n\n\nRandler, C. (2008). Morningness-eveningness comparison in adolescents from different countries around the world. Chronobiology International, 25(6), 1017–1028. https://doi.org/10.1080/07420520802551519",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Question, Objective and Hypothesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html",
    "href": "qmd/supplementary-material-2.html",
    "title": "Appendix B — Methods",
    "section": "",
    "text": "B.1 Overview\nThis document focuses on providing a detailed explanation of the methods and steps involved in building the models and testing the thesis hypothesis.\nFor a comprehensive review of the thesis question and hypothesis, please refer to Supplementary Material A.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#approach-and-procedure-method",
    "href": "qmd/supplementary-material-2.html#approach-and-procedure-method",
    "title": "Appendix B — Methods",
    "section": "\nB.2 Approach and Procedure Method",
    "text": "B.2 Approach and Procedure Method\nThis study adopted the hypothetical-deductive method, also known as the method of conjecture and refutation (Popper, 1972/1979, p. 164), to approach problem-solving. As a procedural method, it utilized an enhanced version of Null Hypothesis Significance Testing (NHST), grounded in the original Neyman-Pearson framework for data testing (Neyman & Pearson, 1928a, 1928b; Perezgonzalez, 2015).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#measurement-instrument",
    "href": "qmd/supplementary-material-2.html#measurement-instrument",
    "title": "Appendix B — Methods",
    "section": "\nB.3 Measurement Instrument",
    "text": "B.3 Measurement Instrument\nChronotypes were assessed using a sleep log based on the standard version of the standard Munich ChronoType Questionnaire (MCTQ) (Roenneberg et al., 2003), a well-validated and widely applied self-report tool for measuring sleep-wake cycles and chronotypes (Roenneberg et al., 2019). The MCTQ captures chronotype as a biological circadian phenotype, determined by the sleep-corrected midpoint of sleep (MS) (Figure B.1) on work-free days (MSF), accounting for any potential sleep compensation due to sleep deficits (sc = sleep correction) on workdays (Final abbreviation: MSFsc) (Roenneberg, 2012).\nParticipants completed an online questionnaire, which included the sleep log as well as sociodemographic (e.g., age, sex), geographic (e.g., full residential address), anthropometric (e.g., weight, height), and data on work or study routines. A sample version of the questionnaire, stored independently by the Internet Archive organization, can be viewed at https://web.archive.org/web/20171018043514/each.usp.br/gipso/mctq.\n\n\nFigure B.1: Variables of the Munich ChronoType Questionnaire scale (A sleep log). In its standard version, these variables are collected in the context of workdays and work-free days. BT = Local time of going to bed. SPrep = Local time of preparing to sleep. SLat = Sleep latency or time to fall asleep after preparing to sleep. SO = Local time of sleep onset. SD = Sleep duration. MS = Local time of mid-sleep. SE = Local time of sleep. Alarm = A logical value indicating if the respondent uses an alarm clock to wake up. SE = Local time of sleep end. SI = “Sleep inertia” (despite the name, this variable represents the time the respondent takes to get up after sleep end). GU = Local time of getting out of bed. TBT = Total time in bed.\n\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#sample",
    "href": "qmd/supplementary-material-2.html#sample",
    "title": "Appendix B — Methods",
    "section": "\nB.4 Sample",
    "text": "B.4 Sample\nThe dataset used for analysis is made up of \\(65,824\\) Brazilian individuals aged 18 or older, residing in the UTC-3 timezone, who completed the survey between October 15th and 21st, 2017.\nThe unfiltered valid sample comprises \\(115,166\\) participants from all Brazilian states, while the raw sample is composed of \\(120,265\\) individuals. The majority of the sample data was obtained in 2017 from October 15th to 21st by a broadcast of the online questionnaire on a popular Brazil’s Sunday TV show with national reach (Rede Globo, 2017). This amount of data collected in such a short time gave the sample a population cross-sectional characteristic.\n\n\nFigure B.2: Screenshots from the Fantástico TV show, aired on Rede Globo on October 15th, 2017, starting at 9 PM, where the online questionnaire was presented.\n\n\nSource: Reproduced from Rede Globo (2017).\n\n\n\nA survey conducted in 2019 by the Brazilian Institute of Geography and Statistics (IBGE) (2021) found that \\(82.17\\%\\) of Brazilian households had access to an internet connection. Therefore, this sample is likely to have a good representation of Brazil’s population.\nDaylight Saving Time (DST) began in Brazil at midnight on October 15th, 2017. Residents from the Midwest, Southeast, and South regions were instructed to set the clock forward by 1 hour. I believe that this event did not contaminate the data since it started on the same day of the data collection. It’s important to notice that we asked subjects to relate their routine behavior, not how they behaved in the last few days. A possible effect of the DST on the sample would be the production of an even later chronotype for populations near the planet’s poles, amplifying a possible latitude effect. However, this was not shown on the data.\nTo balance the sample, a weighting procedure was applied to the data. The weights were calculated by cell weighting, using the sex, age group and Brazil’s state as reference. This procedure can be found on Supplementary Material D.\nMore information about the sample can be found on Supplementary Material C.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#geographical-data",
    "href": "qmd/supplementary-material-2.html#geographical-data",
    "title": "Appendix B — Methods",
    "section": "\nB.5 Geographical Data",
    "text": "B.5 Geographical Data\nGeographic data were collected by the variables country, state, municipality, and postal code. The unique values of those data were manually inspected and adjusted using lookup tables. The municiplaity values were first matched using string distance algorithms present in the stringdist R package (van der Loo, n.d.) and data from the Brazilian Institute of Geography and Statistics (IBGE) via the geobr R package (R. H. M. Pereira & Goncalves, n.d.), other processes were then performed manually.\nThis was a hard task involving crossing information from different sources, such as the QualoCEP (Qual o CEP, 2024), Google Geocoding (Google, n.d.), ViaCEP (ViaCEP, n.d.), and OpenStreetMap (OpenStreetMap contributors, n.d.) databases, along with the Brazilian postal service (Correios) postal code documentation. Hence, the data matching shown in the lookup table was gathered considering not only one variable, but the whole set of geographical information provided by the respondent. Special cases were coded in the lookup table named special_cases\nAll values were also checked for ambiguities, including the municipalities names (e.g., the name Aracoiba could refer to the municipality of Aracoiaba in the state of Ceará, but could also refer to the municipality of Araçoiaba da Serra in the state of São Paulo). All values that had a similarity or pattern matching with one or more municipalities were manually inspected to avoid errors.\n\nB.5.1 Postal Codes\nAfter removing all non-numeric characters from the Brazilian postal codes (Código de Endereçamento Postal (CEP)), they were processed by the following rules:\n\nIf they had 9 or more digits, they were truncated to 8 digits (the first 8 digits are the postal code).\nIf they between 5 and 7 digits, they were complemented with 0s at the end.\nIf they had less than 5 digits, they were discarded.\n\nIn addition, a visual inspection was performed to check for inconsistencies.\nAfter this process, the postal codes were matched with the QualoCEP database (Qual o CEP, 2024). Existing postal codes were than validated by the following rules:\n\nIf the postal code had not been modified and the state or the municipality was the same, it was considered valid.\nIf the postal code had been modified and the state and municipality were the same, it was considered valid.\nElse, it was considered invalid. Invalid CEPs were discarded in the processed dataset.\n\nInvalid postal codes were then matched with data derived from geocoding using Google Geocoding API (Google, n.d.) via the tidygeocoder R package (Cambon et al., 2021), which is stored in the thesis lookup tables. After that, the same process of validation was performed. The API data was able to reduce invalid postal codes from 563 ro 292, a 48.13% reduction. The postal codes that were not validated were discarded from the final data.\nIt’s important to note that some postal codes could not be evaluated because they were missing the state or municipality information. These postal codes were maintained, but no geocode data was associated with them.\nFinally, the state and municipality variables were adjusted using the data from the valid postal codes.\nNon-Brazilian postal codes were not validated, but they were cleaned by removing non-digit characters and codes with 3 digits or less. They also went through a process da cleaning via visual inspection. Their values can be found in the special_cases lookup table.\n\nB.5.2 Latitudes and Longitudes\nLatitudes and longitudes values consider only the municipality and postal_code variables. They were extracted from the QualoCEP database, which is the result of a geocoding using the Google Geocoding API. See Supplementar Material G to a side by side comparison of the latitudes and longitudes from QualoCEP and Google Geocoding API.\nFor respondents who did not provide a valid postal code, the latitude and longitude were extracted via the mean of the latitudes and longitudes associated with the municipality in the QualoCEP database.\nFinally, Google Geocoding API, via the tidygeocoder R package, was used on cases that didn’t had a match in the QualoCEP database.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#solar-irradiance-data",
    "href": "qmd/supplementary-material-2.html#solar-irradiance-data",
    "title": "Appendix B — Methods",
    "section": "\nB.6 Solar Irradiance Data",
    "text": "B.6 Solar Irradiance Data\nThe solar irradiance data is based on Brazil’s National Institute for Space Research (INPE) 2017 Laboratory of Modeling and Studies of Renewable Energy Resources (LABREN) 2017 Solar Energy Atlas (E. B. Pereira et al., 2017). In particular, it was used the Global Horizontal Irradiance (GHI) data, which is the total amount of irradiance received from above by a surface horizontal to the ground.\nThe data comprise annual and monthly averages of the daily total irradiation in Wh/m².day with spatial resolution of 0.1° x 0.1° (latitude/longitude) (about 10km x 10km). It’s important to note that the sample was collected in the same year of the radiance data.\n\n\nFigure B.3: Components of solar irradiance. \\(G_{0}\\) = Extraterrestrial irradiance. \\(G_{n}\\) = Direct normal irradiance. \\(G_{dif}\\) = Diffuse horizontal irradiance. \\(G_{dir}\\) = Direct horizontal irradiance. \\(G\\) = Global horizontal irradiance. \\(G_{i}\\) = Inclined plane irradiance.\n\n\nSource: Adapted by the author from E. B. Pereira et al. (2017).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#solar-time-data",
    "href": "qmd/supplementary-material-2.html#solar-time-data",
    "title": "Appendix B — Methods",
    "section": "\nB.7 Solar Time Data",
    "text": "B.7 Solar Time Data\nThe suntools R package (Bivand & Luque, n.d.) was used to calculate the local of time of sunrise, sunset, and daylight duration for a given date and location. suntools functions are based on equations provided by Meeus (1991) and by the United States’s National Oceanic & Atmospheric Administration (NOAA).\nThe data and time of the equinox and solstices were gathered from the Time and Date AS service (Time and Date AS, n.d.). For validity, this data was checked with the equations from Meeus (1991) and the results of the National Aeronautics and Space Administration (NASA) ModelE AR5 Simulations (National Aeronautics and Space Administration & Goddard Institute for Space Studies, n.d.).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#data-management",
    "href": "qmd/supplementary-material-2.html#data-management",
    "title": "Appendix B — Methods",
    "section": "\nB.8 Data Management",
    "text": "B.8 Data Management\nA data management plan for this study was created and published using the California Digital Library’s DMP Tool (Vartanian, 2024). It is available at https://doi.org/10.17605/OSF.IO/3JZ7K.\nAll data are stored in the study’s research compendium on the Open Source Framework (OSF), hosted on Google Cloud servers in the USA. Access to the compendium is restricted, and data are encrypted using a 4096-bit RSA key pair (Rivest-Shamir-Adleman), along with a unique 32-byte project password.\nAccess to the data requires authorization from the author on the Open Science Framework (OSF) and the installation of the encryption keys.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#data-wrangling",
    "href": "qmd/supplementary-material-2.html#data-wrangling",
    "title": "Appendix B — Methods",
    "section": "\nB.9 Data Wrangling",
    "text": "B.9 Data Wrangling\nData wrangling and analysis followed the data science program proposed by Hadley Wickham and Garrett Grolemund (Wickham et al., 2023) (Figure B.4). All processes were made with the help of the R programming language (R Core Team, n.d.) and several R packages. The tidyverse and rOpenSci peer-reviewed package ecosystem and other R packages adherents of the tidy tools manifesto (Wickham, 2023) were prioritized. The MCTQ data was analyzed using the mctq rOpenSci peer-reviewed package (Vartanian, n.d.). All processes were made in order to provide result reproducibility and to be in accordance with the FAIR principles (Wilkinson et al., 2016).\n\n\nFigure B.4: Model of the data science process created by Wickham, Çetinkaya-Runde, and Grolemund (2023).\n\n\nSource: Reproduced from Wickham et al. (2023).\n\n\n\n\nB.9.1 Pipeline\nThe pipeline for data processing is based on the framework outlined in the targets R package manual. You can view and reproduce this pipeline in the _targets.R file located at the root of the thesis code repository.\n\nB.9.2 Lookup Tables\nAlong with the data cleaning procedures described in the pipeline, lookup tables were employed to clean text and character variables. These tables were created by manually inspecting the unique values of the raw data and correcting common misspellings, synonyms, and other inconsistencies. The tables are stored in the research compendium of this thesis.\nText/character variables: track, names, email, country, state, municiplality, postalcode, sleep_drugs_which, sleep_disorder_which, medication_which.\nThe lookup tables are available in the research compendium of this thesis. They had to be encrypted because of the sensitive information they contain. If you need access, please contact the author.\nThe matching of the variables sleep_drugs_which, sleep_disorder_which, medication_which is not complete. This variables were not used in the analysis, but they are available in the research compendium.\nRead the section about geographical information to learn more about the matching process.\n\nB.9.3 Circular Statistics\nMCTQ is based on local time data, which are circular in nature (i.e., it cycles every 24 hours). Performing statistics with this kind of variables is challenging, since it can have different values depending on each arc/interval of the circle is used.\nFor example, the distance between 23:00 (the values here are always in the 24 hour scale) and 01:00 can be 02:00 or 22:00 depending on the direction of the measurement. Hence, the analysis will always have to choose a direction (Figure B.5).\n\n\nFigure B.5: Circular time representation.\n\n               - &lt;--- h ---&gt; +\n                    origin\n                . . . 0 . . .\n             .                 .\n            .                   .\n           .                     .\n          .                       .\n         .                         .\n         18                        6\n         .                         .\n          .                       .\n           .                     .\n            .                   .\n             .                 .\n                . . . 12 . . .\n\n18 + 6 = 0h\nSource: Created by the author.\n\n\n\nThe analysis assumed as a method for dealing with this issue the adaptation of the values using the 12:00 hour as a reference point. This method is appropriated when dealing with sleep data.\nConsider the local time of sleep onset. We can observe that some subjects start sleeping before midnight, while others sleep after midnight. By the context of the data, the circle arc/interval (Figure B.6) here is the shorter one, since the great majority of people don’t usually sleep in daytime and for more than 12 hours.\n\n\nFigure B.6: Possible intervals for circular time.\n\n             day 1                        day 2\n     x                  y         x                  y\n   06:00              22:00     06:00              22:00\n-----|------------------|---------|------------------|-----&gt;\n              16h           8h             16h\n          longer int.  shorter int.   longer int.\nSource: Created by the author.\n\n\n\nUsing the 12:00 hour a threshold, it’s possible to calculate the correct distance between times. This method link the values in a two-day timeline, with values equal or greater than 12:00 allocated on day 1, and values with less than 12:00 allocated on day 2. This way, the distance between 23:00 and 01:00 is 02:00, the distance between 01:00 and 23:00 is 22:00 (i.e., there is no gap between data points) (Figure B.7).\nCode# library(dplyr)\n# library(ggplot2)\n# library(here)\n# library(lubritime)\n# library(patchwork)\n# library(tidyr)\n\nweighted_data &lt;- targets::tar_read(\n  \"weighted_data\",\n  store = here::here(\"_targets\")\n)\n\nplot_1 &lt;-\n  weighted_data |&gt;\n  dplyr::select(so_f) |&gt;\n  tidyr::drop_na() |&gt;\n  plotr:::plot_hist(\n    col = \"so_f\",\n    x_label = \"Local time of sleep onset\",\n    print = FALSE\n  )\n\nplot_2 &lt;-\n  weighted_data |&gt;\n  dplyr::select(so_f) |&gt;\n  dplyr::mutate(\n    so_f =\n      so_f |&gt;\n      lubritime:::link_to_timeline(threshold = hms::parse_hms(\"12:00:00\"))\n  ) |&gt;\n  tidyr::drop_na() |&gt;\n  plotr:::plot_hist(\n    col = \"so_f\",\n    x_label = \"Local time of sleep onset\",\n    print = FALSE\n  )\n\npatchwork::wrap_plots(\n  plot_1, plot_2,\n  ncol = 2\n)\n\n\nFigure B.7: Histogram of the local time of sleep onset (SO) with (right) and without (left) the 12:000 treatment.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.\n\n\n\n\nB.9.4 Round-Off Errors\nThe R programming language only stores values up to 53 binary bits, that’s about \\(15.955\\) digits of precision (\\(x = 53 \\log_{10}(2)\\)). Since MCTQ deals with self-reported local time of day (e.g., 02:30) and duration (e.g., 15 minutes), a greater floating-point precision is unnecessary. Even with multiple computations, round-off errors wouldn’t significantly impact the phenomenon under study.\nEven considering that POSIXct objects are used in the models, there is still a good margin of precision – POSIXct objects are data-time objects measured by the amount of seconds since the UNIX epoch (1970-01-01).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#hypothesis-test",
    "href": "qmd/supplementary-material-2.html#hypothesis-test",
    "title": "Appendix B — Methods",
    "section": "\nB.10 Hypothesis Test",
    "text": "B.10 Hypothesis Test\nThe study hypothesis was tested using nested multiple linear regressions. The primary concept of nested models is to evaluate the effect of adding one or more predictors on the model’s variance explanation (i.e., the \\(\\text{R}^{2}\\)) (Allen, 1997; Maxwell et al., 2018). This is achieved by creating a restricted model (\\(r\\)) and comparing it with a full model (\\(f\\)). The hypothesis can be outlined as follows:\n\nNull hypothesis (\\(\\text{H}_{0}\\))\n\nAdding latitude does not meaningfully improve the model’s fit. This implies that the change in adjusted \\(\\text{R}^{2}\\) is negligible, or the F-test is not significant, given a type I error probability (\\(\\alpha\\)) of \\(0.05\\).\n\nAlternative Hypothesis (\\(\\text{H}_{a}\\))\n\nAdding latitude meaningfully improves the model’s fit. This implies that the change in adjusted \\(\\text{R}^{2}\\) exceeds the expected Minimum Effect Size (MES), and the F-test is significant, given a type I error probability (\\(\\alpha\\)) of \\(0.05\\).\n\n\n\nConjecture B.1 (Data Test – Full Version) \\[\n\\begin{cases}\n\\text{H}_{0}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} &lt; \\text{MES} \\\\\n\\text{H}_{a}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} \\geq \\text{MES}\n\\end{cases}\n\\]\n\nWhere:\n\\[\n\\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} = \\text{Adjusted} \\ \\text{R}^{2}_{f} - \\text{Adjusted} \\ \\text{R}^{2}_{r}\n\\tag{B.1}\\]\nand \\(\\text{MES}\\) is a Minimum Effect Size, a threshold for detecting meaningful changes.\n\\(\\text{R}^{2}\\) is a statistic that represents the proportion of variance explained in the relationship between two or more variables (goodness of fit) (Frey, 2022, p. 1339). It ranges from \\(1\\) (perfect prediction) to \\(0\\) (no prediction) and can be defined as (DeGroot & Schervish, 2012, p. 748):\n\\[\n\\text{R}^{2} = 1 - \\cfrac{\\text{SS}_{\\text{residual}}}{\\text{SS}_{\\text{total}}} = 1 - \\cfrac{\\sum \\limits^{n}_{i = 1} (y_{i} - \\hat{y}_{i})^{2}}{\\sum \\limits^{n}_{i = 1} (y_{i} - \\overline{y})^{2}} = 1 -  \\cfrac{\\text{Unexplained Variance}}{\\text{Total Variance}} = \\cfrac{\\text{Explained Variance}}{\\text{Total Variance}}\n\\tag{B.2}\\]\nWhere:\n\n\n\\(y_{i}\\) = Observed value of the dependent variable;\n\n\\(\\hat{y}_{i}\\) = Predicted value of the dependent variable;\n\n\\(\\overline{y}\\) = Mean of the dependent variable;\n\n\\(\\text{SS}_{\\text{residual}}\\) = Sum of the squared prediction errors (residuals) across all observations (DeGroot & Schervish, 2012, p. 748; Hair, 2019, p. 264);\n\n\\(\\text{SS}_{\\text{total}}\\) = Total sum of squares or the total amount of variation that exists to be explained by the independent variables. Is equivalent by the sum of the squared difference between the observed value and the mean of the dependent variable (baseline prediction) (DeGroot & Schervish, 2012, p. 748; Hair, 2019, p. 265).\n\nHere, \\(\\text{R}^{2}\\) is the proportion of the variance in the dependent variable that is predictable from the independent variables.\nThe adjusted \\(\\text{R}^{2}\\) is a modified version of \\(\\text{R}^{2}\\) that adjusts for the number of predictors in the model. It can be defined as:\n\\[\n\\text{Adjusted} \\ \\text{R}^{2} = 1 - \\cfrac{\\text{SS}_{\\text{residual}} / \\text{df}_{\\text{residual}}}{\\text{SS}_{\\text{total}} / \\text{df}_{\\text{total}}} = 1 - \\cfrac{(1 - \\text{R}^{2}) \\times (\\text{n} - 1)}{\\text{n} - k - 1}\n\\tag{B.3}\\]\nWhere:\n\n\n\\(\\text{n}\\) = Number of observations in the sample;\n\n\\(k\\) = Number of independent variables in the model.\n\n\\(\\text{df}_{\\text{residual}}\\) = Degrees of freedom of the residual sum of squaress = \\(n - k - 1\\).\n\n\\(\\text{df}_{\\text{total}}\\) = Degrees of freedom of the total sum of squares = \\(n - 1\\).\n\nThe F-test serves as to determine wether the ratio of variances is different from zero (i.e., statistically significant) considering a baseline prediction (Hair, 2019, p. 300). In this case, the baseline prediction is the restricted model. The general equation for the F-test for nested models (Allen, 1997, p. 113) can be defined as:\n\\[\n\\text{F} = \\cfrac{\\text{R}^{2}_{f} - \\text{R}^{2}_{r} / (k_{f} - k_{R})}{(1 - \\text{R}^{2}_{f}) / (\\text{n} - k_{f} - 1)}\n\\tag{B.4}\\]\nWhere:\n\n\n\\(\\text{R}^{2}_{F}\\) = Coefficient of determination for the full model;\n\n\\(\\text{R}^{2}_{R}\\) = Coefficient of determination for the restricted model;\n\n\\(k_{f}\\) = Number of independent variables in the full model;\n\n\\(k_{r}\\) = Number of independent variables in the restricted model;\n\n\\(\\text{n}\\) = Number of observations in the sample.\n\n\\[\n\\text{F} = \\cfrac{\\text{Additional Var. Explained} / \\text{Additional d.f. Expended}}{\\text{Var. unexplained} / \\text{d.f. Remaining}}\n\\]\nA MES must always be present in any data testing. The effect-size was present in the original Neyman and Pearson framework (Neyman & Pearson, 1928a, 1928b), but unfortunately this practice fade away with the indiscriminate use of p-values, one of the many issues that came with the Null Hypothesis Significance Testing (NHST) (Perezgonzalez, 2015). While p-values are estimates of type 1 error (in Neyman–Pearson’s approaches, or like-approaches), that’s not the main thing we are interested while doing a hypothesis test. What is really being test is the effect size (i.e., a practical significance). Another major issue to only relying on p-values is that the estimated p-value tends to decrease when the sample size is increased, hence, focusing just on p-values with large sample sizes results in the rejection of the null hypothesis, making it not meaningful in this specific situation (Gómez-de-Mariscal et al., 2021; Hair, 2019; Lin et al., 2013).\n\nNeyman-Pearson’s approach considers, at least, two competing hypotheses, although it only tests data under one of them. The hypothesis which is the most important for the research (i.e., the one you do not want to reject too often) is the one tested (Neyman and Pearson, 1928; Neyman, 1942; Spielman, 1973). This hypothesis is better off written so as for incorporating the minimum expected effect size within its postulate (e.g., \\(\\text{HM} : \\text{M1} – \\text{M2} = 0 \\pm \\text{MES}\\)), so that it is clear that values within such minimum threshold are considered reasonably probable under the main hypothesis, while values outside such minimum threshold are considered as more probable under the alternative hypothesis. (Perezgonzalez, 2015).\n\n\nAlthough larger \\(\\text{R}^{2}\\) values result in higher \\(\\text{F}\\) values, the researcher must base any assessment of practical significance separate from statistical significance. Because statistical significance is really an assessment of the impact of sampling error, the researcher must be cautious of always assuming that statistically significant results are also practically significant. This caution is particularly relevant in the case of large samples where even small \\(\\text{R}^{2}\\) values (e.g., \\(5\\%\\) or \\(10\\%\\)) can be statistically significant, but such levels of explanation would not be acceptable for further action on a practical basis. (Hair, 2019, p. 300)\n\nPublications related to issues regarding the misuse of p-value are plenty. For more on the matter, I recommend Perezgonzalez (2015) review of Fisher’s and Neyman-Pearson’s data test proposals, Lin et al. (2013) and Gómez-de-Mariscal et al. (2021) studies about large Samples and the p-value problem, and Cohen’s essays on the subject (like Cohen (1990) and Cohen (1994)).\nIt’s important to note that, in addition to the F-test, it’s assumed that for \\(\\text{R}^{2}_{\\text{r}}\\) to differ significantly from \\(\\text{R}^{2}_{\\text{f}}\\), there must be a non-negligible effect size between them. This effect size can be calculated using Cohen’s \\(f^{2}\\) (Cohen, 1988, 1992):\n\\[\n\\text{Cohen's } f^2 = \\cfrac{\\text{R}^{2}}{1 - \\text{R}^{2}}\n\\]\nFor nested models, this can be adapted as follows:\n\\[\n\\text{Cohen's } f^2 = \\cfrac{\\text{R}^{2}_{f} - \\text{R}^{2}_{r}}{1 - \\text{R}^{2}_{f}} = \\cfrac{\\Delta \\text{R}^{2}}{1 - \\text{R}^{2}_{f}}\n\\]\n\\[\nf^{2} = \\cfrac{\\text{Additional Var. Explained}}{\\text{Var. unexplained}}\n\\]\nConsidering the particular emphasis that the solar zeitgeber has on the entrainment of biological rhythms (as demonstrated in many experiments), it would not be reasonable to assume that the latitude hypothesis could be supported without at least a non-negligible effect size. With this in mind, this analysis will use Cohen’s threshold for small/negligible effects, the Minimum Effect Size (MES) (\\(\\delta\\)) is defined as 0.02 (Cohen, 1988, p. 413; 1992, p. 157).\nIn Cohen’s words:\n\nWhat is really intended by the invalid affirmation of a null hypothesis is not that the population ES [Effect Size] is literally zero, but rather that it is negligible, or trivial (Cohen, 1988, p. 16).\n\n\nSMALL EFFECT SIZE: \\(f^2 = .02\\). Translated into ^{2} or partial ^{2} for Case 1, this gives \\(.02 / (1 + .02) = .0196\\). We thus define a small effect as one that accounts for 2% of the \\(\\text{Y}\\) variance (in contrast with 1% for \\(r\\)), and translate to an \\(\\text{R} = \\sqrt{0196} = .14\\) (compared to .10 for \\(r\\)). This is a modest enough amount, just barely escaping triviality and (alas!) all too frequently in practice represents the true order of magnitude of the effect being tested (Cohen, 1988, p. 413).\n\n\n[…] in many circumstances, all that is intended by “proving” the null hypothesis is that the ES is not necessarily zero but small enough to be negligible, i.e., no larger than \\(i\\). How large \\(i\\) is will vary with the substantive context. Assume, for example, that ES is expressed as \\(f^2\\), and that the context is such as to consider \\(f^2\\) no larger than \\(.02\\) to be negligible; thus \\(i\\) = .02 (Cohen, 1988, p. 461).\n\n\\[\n\\text{MES} = \\text{Cohen's } f^2 \\text{small threshold} = 0.02 \\\\\n\\]\nFor comparison, Cohen’s threshold for medium effects is \\(0.15\\), and for large effects is \\(0.35\\) (Cohen, 1988, pp. 413–414; 1992, p. 157).\nKnowing Cohen’s \\(f^2\\), is possible to calculated the equivalent \\(\\text{R}^{2}\\):\n\\[\n0.02 = \\cfrac{\\text{R}^{2}}{1 - \\text{R}^{2}} \\quad \\text{or} \\quad \\text{R}^{2} = \\cfrac{0.02}{1.02} \\eqsim 0.01960784\n\\]\nIn other words, the latitude must explain at least \\(1.960784\\%\\) of the variance in the dependent variable to be considered non-negligible. This is the Minimum Effect Size (MES) for this analysis.\nIn summary, the decision rule for the hypothesis test is as follows:\n\n\nReject \\(\\text{H}_{0}\\) if both of the following conditions are met:\n\nThe F-test is significant.\n\n\\(\\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} \\geq 0.01960784\\);\n\n\n\nFail to reject \\(\\text{H}_{0}\\) if either of the following conditions are met:\n\nThe F-test is not significant, or\nThe F-test is significant, but \\(\\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} &lt; 0.01960784\\).\n\n\n\nAs usual, the significance level (\\(\\alpha\\)) was set at \\(0.05\\), allowing a 5% chance of a Type I error. A power analysis was performed to determine the necessary sample size for detecting a significant effect, targeting a power (\\(1 - \\beta\\)) of \\(0.99\\).\nIt’s important to emphasize that this thesis is not investigating causality, only association. Predictive models alone should never be used to infer causal relationships (Arif & MacNeil, 2022).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#statistical-analyses",
    "href": "qmd/supplementary-material-2.html#statistical-analyses",
    "title": "Appendix B — Methods",
    "section": "\nB.11 Statistical Analyses",
    "text": "B.11 Statistical Analyses\nIn addition to the analyses described in the Hypothesis Test subsection, several other evaluations were conducted to ensure the validity of the results, including: a power analysis, the visual inspection of variable distributions (e.g., Q-Q plots), assessment of residual normality, checks for multicollinearity, and examination of leverage and influential points.\nAll analyses were performed using computational notebooks and are fully reproducible. Detailed documentation is available in the supplementary materials.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-2.html#model-predictors",
    "href": "qmd/supplementary-material-2.html#model-predictors",
    "title": "Appendix B — Methods",
    "section": "\nB.12 Model Predictors",
    "text": "B.12 Model Predictors\nTwo modeling approaches were used to assess the impact of latitude and related environmental variables on the outcome of interest. Each approach builds on predictors inspired by the methods in Leocadio-Miguel et al. (2017) while addressing methodological inconsistencies and data limitations.\n\nB.12.1 Identified Inconsistencies\nDuring the replication of Leocadio-Miguel et al. (2017), several inconsistencies were identified:\n\n\nVariable usage mismatch: The number of covariates listed in the results section does not align with those used in the \\(\\text{F}\\)-test parameters for the restricted and full models.\n\nMulticollinearity issues: The inclusion of sunrise, sunset, and daylight duration (derived as sunset - sunrise) for the March equinox and the June and December solstices introduces multicollinearity due to the interdependence of these variables.\n\nB.12.2 Selection of Predictors\nThe restricted model in Leocadio-Miguel et al. (2017) included age, longitude, and solar irradiation when the subjects filled the online questionnaire as covariates, with sex, daylight saving time (DST), and season as cofactors. The full model added, annual average solar irradiation, sunrise time, sunset time, and daylight duration for the March equinox and the June and December solstices. However, in this study:\n\n\nDST and season: Are excluded, as data were collected within a single week, rendering these variables redundant.\n\nLatitude proxies: Annual average solar irradiation and daylight duration for the March equinox and the June and December solstices were included. However, sunrise and sunset time coefficients were not estimable and had to be omitted. This was due to their high collinearity (\\(r &gt; 0.999\\)). Centralization or standardization of the predictors did not resolve this issue.\n\nAs in Leocadio-Miguel et al. (2017), the measure of daylight duration in September equinox was not included due to the high correlation with the March equinox (\\(r &gt; 0.993\\)), making it statistically indistinguishable from the latter (\\(p &gt; 0.05\\)). This was expected, since the day at the equinoxes must be approximately the same length (from the Latin, aequĭnoctĭum, meaning the time of equal days and nights (Latinitium, n.d.)).\nDaylight duration for the March equinox, June solstice, and December solstice exhibited high multicollinearity, with a variance inflation factor (VIF) exceeding \\(1000\\). However, since these variables are part of the same group, this does not pose a significant issue for the analysis. The focus is on the collective effect of the group rather than the contributions of individual variables.\n\nB.12.3 Tests and Predictors\nTo evaluate the hypotheses, two tests were conducted with distinct sets of predictors:\n\nB.12.3.1 Test A\n\n\nRestricted Model Predictors\n\nAge, sex, longitude, and Global Horizontal Irradiance (GHI) at the time of questionnaire completion (monthly GHI average for participants’ geographic coordinates).\n\n\n\nFull Model Predictors\n\n\nRestricted model predictors + annual GHI average and daylight duration for the nearest March and September equinoxes, and the June and December solstices.\n\n\n\nB.12.3.2 Test B\n\n\nRestricted Model Predictors\n\nAge, sex, longitude, and GHI at the time of questionnaire completion (monthly GHI average for participants’ geographic coordinates).\n\n\n\nFull Model Predictors\n\n\nRestricted model predictors + latitude.\n\n\n\n\n\n\n\nAllen, M. P. (1997). Understanding regression analysis. Plenum Press.\n\n\nArif, S., & MacNeil, M. A. (2022). Predictive models aren’t for causal inference. Ecology Letters, 25(8), 1741–1745. https://doi.org/10.1111/ele.14033\n\n\nBivand, R., & Luque, S. (n.d.). suntools: Calculate sun position, sunrise, sunset, solar noon and twilight [Computer software]. https://doi.org/10.32614/CRAN.package.suntools\n\n\nCambon, J., Hernangómez, D., Belanger, C., & Possenriede, D. (2021). tidygeocoder: An R package for geocoding. Journal of Open Source Software, 6(65), 3544. https://doi.org/10.21105/joss.03544\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates.\n\n\nCohen, J. (1990). Things I have learned (so far). American Psychologist, 45(12), 1304–1312. https://doi.org/10.1037/10109-028\n\n\nCohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155–159. https://doi.org/10.1037/0033-2909.112.1.155\n\n\nCohen, J. (1994). The earth is round (p\\(&lt;\\).05). American Psychologist, 49(12), 997–1003. https://doi.org/10.1037/0003-066X.49.12.997\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and statistics (OCLC: ocn502674206) (4th ed.). Addison-Wesley.\n\n\nFrey, B. B. (Ed.). (2022). The SAGE encyclopedia of research design (2nd ed.). SAGE Publications. https://doi.org/10.4135/9781071812082\n\n\nGómez-de-Mariscal, E., Guerrero, V., Sneider, A., Jayatilaka, H., Phillip, J. M., Wirtz, D., & Muñoz-Barrutia, A. (2021). Use of the p-values as a size-dependent function to address practical differences when analyzing large datasets. Scientific Reports, 11(1, 1), 20942. https://doi.org/10.1038/s41598-021-00199-5\n\n\nGoogle. (n.d.). Google Geocoding API [Computer software]. Google. https://developers.google.com/maps/documentation/geocoding\n\n\nHair, J. F. (2019). Multivariate data analysis (8th ed.). Cengage.\n\n\nInstituto Brasileiro de Geografia e Estatística. (2021). Pesquisa Nacional por Amostra de Domicílios Contínua: Acesso à internet e à televisão e posse de telefone móvel celular para uso pessoal 2019 [Continuous National Household Sample Survey: Internet and television access and ownership of mobile phones for personal use 2019] (p. 12). Instituto Brasileiro de Geografia e Estatística. https://biblioteca.ibge.gov.br/visualizacao/livros/liv101794_informativo.pdf\n\n\nLatinitium. (n.d.). Latin dictionaries. Latinitium. Retrieved September 21, 2023, from https://latinitium.com/latin-dictionaries/\n\n\nLeocadio-Miguel, M. A., Louzada, F. M., Duarte, L. L., Areas, R. P., Alam, M., Freire, M. V., Fontenele-Araujo, J., Menna-Barreto, L., & Pedrazzoli, M. (2017). Latitudinal cline of chronotype. Scientific Reports, 7(1), 5437. https://doi.org/10.1038/s41598-017-05797-w\n\n\nLin, M., Lucas, H. C., & Shmueli, G. (2013). Research commentary—Too big to fail: Large samples and the p-value problem. Information Systems Research, 24(4), 906–917. https://doi.org/10.1287/isre.2013.0480\n\n\nMaxwell, S. E., Delaney, H. D., & Kelley, K. (2018). Designing experiments and analyzing data: A model comparison perspective (3rd ed.). Routledge.\n\n\nMeeus, J. (1991). Astronomical algorithms. Willmann-Bell.\n\n\nNational Aeronautics and Space Administration, & Goddard Institute for Space Studies. (n.d.). Data.GISS: Time and date of vernal equinox. Retrieved November 24, 2024, from https://data.giss.nasa.gov/modelE/ar5plots/srvernal.html\n\n\nNeyman, J., & Pearson, E. S. (1928a). On the use and interpretation of certain test criteria for purposes of statistical inference: Part I. Biometrika, 20A(1/2), 175–240. https://doi.org/10.2307/2331945\n\n\nNeyman, J., & Pearson, E. S. (1928b). On the use and interpretation of certain test criteria for purposes of statistical inference: Part II. Biometrika, 20A(3/4), 263–294. https://doi.org/10.2307/2332112\n\n\nOpenStreetMap contributors. (n.d.). OpenStreetMap [Computer software]. OpenStreetMap Foundation. https://www.openstreetmap.org\n\n\nPereira, E. B., Martins, F. R., Gonçalves, A. R., Costa, R., Lima, F. J. L., Rüther, R., Abreu, S. L., Tiepolo, G. M., Pereira, S. V., & Souza, J. G. (2017). Atlas brasileiro de energia solar [Brazilian atlas of solar energy] (2nd ed.). Instituto Nacional de Pesquisas Espaciais. https://doi.org/10.34024/978851700089\n\n\nPereira, R. H. M., & Goncalves, C. N. (n.d.). geobr: Download official spatial data sets of Brazil [Computer software]. https://doi.org/10.32614/CRAN.package.geobr\n\n\nPerezgonzalez, J. D. (2015). Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.00223\n\n\nPopper, K. R. (1979). Objective knowledge: An evolutionary approach. Oxford University Press. (Original work published 1972)\n\n\nQual o CEP. (2024). Banco de CEP e código IBGE [Database of ZIP codes and IBGE (Brazilian Institute of Geography and Statistics) codes] [Data set]. https://www.qualocep.com/\n\n\nR Core Team. (n.d.). R: A language and environment for statistical computing [Computer software]. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nRede Globo. (2017, October 15). Metade da população se sente mal no horário de verão, revela pesquisa (TV program Fantástico) [Half of the population feels unwell during daylight saving time, research reveals] [Video recording]. Rede Globo. https://g1.globo.com/fantastico/noticia/2017/10/metade-da-populacao-se-sente-mal-no-horario-de-verao-revela-pesquisa.html\n\n\nRoenneberg, T. (2012). What is chronotype? Sleep and Biological Rhythms, 10(2), 75–76. https://doi.org/10.1111/j.1479-8425.2012.00541.x\n\n\nRoenneberg, T., Pilz, L. K., Zerbini, G., & Winnebeck, E. C. (2019). Chronotype and social jetlag: A (self-) critical review. Biology, 8(3), 54. https://doi.org/10.3390/biology8030054\n\n\nRoenneberg, T., Wirz-Justice, A., & Merrow, M. (2003). Life between clocks: Daily temporal patterns of human chronotypes (.). Journal of Biological Rhythms, 18(1), 80–90. https://doi.org/10.1177/0748730402239679\n\n\nTime and Date AS. (n.d.). Solstices & equinoxes for UTC (2000–2049). Retrieved November 24, 2024, from https://www.timeanddate.com/calendar/seasons.html?year=2000&n=1440\n\n\nvan der Loo, M. P. J. (n.d.). The stringdist package for approximate string matching [Computer software]. https://CRAN.R-project.org/package=stringdist\n\n\nVartanian, D. (n.d.). {abnt}: Quarto format designed for theses and dissertations that adhere to the standards of the Brazilian Association of Technical Standards (ABNT) [Computer software]. https://github.com/danielvartan/abnt\n\n\nVartanian, D. (2024). Is latitude associated with chronotype? [Data Management Plan]. DMPHub. https://doi.org/10.48321/D1B5C8068D\n\n\nViaCEP. (n.d.). ViaCEP API: Consulte CEPs de todo o Brasil [Computer software]. https://viacep.com.br\n\n\nWickham, H. (2023, February 23). The tidy tools manifesto. Tidyverse. https://tidyverse.tidyverse.org/articles/manifesto.html\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd ed.). O’Reilly Media. https://r4ds.hadley.nz\n\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1), 160018. https://doi.org/10.1038/sdata.2016.18",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html",
    "href": "qmd/supplementary-material-3.html",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "",
    "text": "C.1 Overview\nThis document focus in providing a comprehensive overview of the data collected in the survey.\nIt focuses on the sample used on the analysis, with explicit indications provided whenever the full sample is used. The analysis sample is a subset of the full sample, and includes only Brazilian individuals aged 18 or older, residing in the UTC-3 timezone, who completed the survey between October 15th and 21st, 2017.\nPlease note that the models were created using cell weights to account for sample unbalances. For more information on the sample balance, see the Supplementary Material D.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#setting-the-environment",
    "href": "qmd/supplementary-material-3.html#setting-the-environment",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.2 Setting the Environment",
    "text": "C.2 Setting the Environment\n\nCodelibrary(cli)\nlibrary(dplyr)\nlibrary(fBasics)\nlibrary(geobr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(hms)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(lubritime) # github.com/danielvartan/lubritime\nlibrary(magrittr)\nlibrary(moments)\nlibrary(nortest)\nlibrary(patchwork)\nlibrary(prettycheck) # github.com/danielvartan/prettycheck\nlibrary(purrr)\nlibrary(rlang)\nlibrary(rutils) # github.com/danielvartan/rutils\nlibrary(sidrar)\nlibrary(stats)\nlibrary(stringr)\nlibrary(targets)\nlibrary(tidyr)\nlibrary(tseries)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#loading-the-data",
    "href": "qmd/supplementary-material-3.html#loading-the-data",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.3 Loading the Data",
    "text": "C.3 Loading the Data\n\ntargets::tar_make(script = here::here(\"_targets.R\"))\n\n\nC.3.1 Full Sample\n\nanonymized_data &lt;-\n  targets::tar_read(\"anonymized_data\", store = here::here(\"_targets\"))\n\n\nC.3.2 Analysis Sample\n\nweighted_data &lt;-\n  targets::tar_read(\"weighted_data\", store = here::here(\"_targets\"))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#distribution-of-main-variables",
    "href": "qmd/supplementary-material-3.html#distribution-of-main-variables",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.4 Distribution of Main Variables",
    "text": "C.4 Distribution of Main Variables\n\n\nMSF~sc~ (Chronotype proxy) (local time)\nAge (years)\nLatitude (decimal degrees)\nLongitude (decimal degrees)\nMonthly average global horizontal irradiance (Wh/m²)\nAnnual average global horizontal irradiance (Wh/m²)\nSunrise on the March equinox (date-time seconds)\nSunset on the March equinox (seconds)\nDaylight on the March equinox (seconds)\nSunrise on the June solstice (seconds)\nSunset on the June solstice (seconds)\nDaylight on the June solstice (seconds)\nSunrise on the September solstice (seconds)\nSunset on the September solstice (seconds)\nDaylight on the September solstice (seconds)\nSunrise on the December solstice (seconds)\nSunset on the December solstice (seconds)\nDaylight on the December solstice (seconds)\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"msf_sc\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.1: Statistics for the msf_sc variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"msf_sc\",\n    x_label = \"MSF~sc~ (Chronotype proxy) (local time)\"\n  )\n\n\nFigure C.1: Histogram of the msf_sc variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"msf_sc\")\n\n\nFigure C.2: Box plot of the msf_sc variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"age\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.2: Statistics for the age variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"age\",\n    x_label = \"Age (years)\"\n  )\n\n\nFigure C.3: Histogram of the age variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"age\")\n\n\nFigure C.4: Box plot of the age variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"latitude\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.3: Statistics for the latitude variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"latitude\",\n    x_label = \"Latitude (decimal degrees)\"\n  )\n\n\nFigure C.5: Histogram of the latitude variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"latitude\")\n\n\nFigure C.6: Box plot of the latitude variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"longitude\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.4: Statistics for the longitude variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"longitude\",\n    x_label = \"Longitude (decimal degrees)\"\n  )\n\n\nFigure C.7: Histogram of the longitude variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"longitude\")\n\n\nFigure C.8: Box plot of the longitude variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"ghi_month\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.5: Statistics for the ghi_month variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"ghi_month\",\n    x_label = \"Monthly average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nFigure C.9: Histogram of the ghi_month variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"ghi_month\")\n\n\nFigure C.10: Box plot of the ghi_month variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"ghi_annual\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.6: Statistics for the ghi_annual variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"ghi_annual\",\n    x_label = \"Annual average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nFigure C.11: Histogram of the ghi_annual variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"ghi_annual\")\n\n\nFigure C.12: Box plot of the ghi_annual variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"march_equinox_sunrise\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.7: Statistics for the march_equinox_sunrise variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"march_equinox_sunrise\",\n    x_label = \"Sunrise on the March equinox (date-time seconds)\"\n  )\n\n\nFigure C.13: Histogram of the march_equinox_sunrise variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"march_equinox_sunrise\")\n\n\nFigure C.14: Box plot of the march_equinox_sunrise variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"march_equinox_sunset\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.8: Statistics for the march_equinox_sunset variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"march_equinox_sunset\",\n    x_label = \"Sunset on the March equinox (seconds)\"\n  )\n\n\nFigure C.15: Histogram of the march_equinox_sunset variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"march_equinox_sunset\")\n\n\nFigure C.16: Box plot of the march_equinox_sunset variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"march_equinox_daylight\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.9: Statistics for the march_equinox_daylight variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"march_equinox_daylight\",\n    x_label = \"Daylight on the March equinox (seconds)\"\n  )\n\n\nFigure C.17: Histogram of the march_equinox_daylight variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"march_equinox_daylight\")\n\n\nFigure C.18: Box plot of the march_equinox_daylight variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"june_solstice_sunrise\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.10: Statistics for the june_solstice_sunrise variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"june_solstice_sunrise\",\n    x_label = \"Sunrise on the June solstice (seconds)\"\n  )\n\n\nFigure C.19: Histogram of the june_solstice_sunrise variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"june_solstice_sunrise\")\n\n\nFigure C.20: Box plot of the june_solstice_sunrise variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"june_solstice_sunset\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.11: Statistics for the june_solstice_sunset variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"june_solstice_sunset\",\n    x_label = \"Sunset on the June solstice (seconds)\"\n  )\n\n\nFigure C.21: Histogram of the june_solstice_sunset variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"june_solstice_sunset\")\n\n\nFigure C.22: Box plot of the june_solstice_sunset variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"june_solstice_daylight\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.12: Statistics for the june_solstice_daylight variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"june_solstice_daylight\",\n    x_label = \"Daylight on the June solstice (seconds)\"\n  )\n\n\nFigure C.23: Histogram of the june_solstice_daylight variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"june_solstice_daylight\")\n\n\nFigure C.24: Box plot of the june_solstice_daylight variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"september_equinox_sunrise\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.13: Statistics for the september_equinox_sunrise variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"september_equinox_sunrise\",\n    x_label = \"Sunrise on the September solstice (seconds)\"\n  )\n\n\nFigure C.25: Histogram of the september_equinox_sunrise variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"september_equinox_sunrise\")\n\n\nFigure C.26: Box plot of the september_equinox_sunrise variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"september_equinox_sunset\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.14: Statistics for the september_equinox_sunset variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"september_equinox_sunset\",\n    x_label = \"Sunset on the September solstice (seconds)\"\n  )\n\n\nFigure C.27: Histogram of the september_equinox_sunset variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"september_equinox_sunset\")\n\n\nFigure C.28: Box plot of the september_equinox_sunset variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"september_equinox_daylight\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.15: Statistics for the september_equinox_daylight variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"september_equinox_daylight\",\n    x_label = \"Daylight on the September solstice (seconds)\"\n  )\n\n\nFigure C.29: Histogram of the september_equinox_daylight variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"september_equinox_daylight\")\n\n\nFigure C.30: Box plot of the september_equinox_daylight variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"december_solstice_sunrise\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.16: Statistics for the december_solstice_sunrise variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"december_solstice_sunrise\",\n    x_label = \"Sunrise on the December solstice (seconds)\"\n  )\n\n\nFigure C.31: Histogram of the december_solstice_sunrise variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"december_solstice_sunrise\")\n\n\nFigure C.32: Box plot of the december_solstice_sunrise variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"december_solstice_sunset\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.17: Statistics for the december_solstice_sunset variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"december_solstice_sunset\",\n    x_label = \"Sunset on the December solstice (seconds)\"\n  )\n\n\nFigure C.33: Histogram of the december_solstice_sunset variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"december_solstice_sunset\")\n\n\nFigure C.34: Box plot of the december_solstice_sunset variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\n    col = \"december_solstice_daylight\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable C.18: Statistics for the december_solstice_daylight variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_dist(\n    col = \"december_solstice_daylight\",\n    x_label = \"Daylight on the December solstice (seconds)\"\n  )\n\n\nFigure C.35: Histogram of the december_solstice_daylight variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_box_plot(col = \"december_solstice_daylight\")\n\n\nFigure C.36: Box plot of the december_solstice_daylight variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#correlation-matrix-of-the-main-variables",
    "href": "qmd/supplementary-material-3.html#correlation-matrix-of-the-main-variables",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.5 Correlation Matrix of the Main Variables",
    "text": "C.5 Correlation Matrix of the Main Variables\n\nC.5.1 Full Sample\nCodeanonymized_data |&gt;\n  plotr:::plot_ggally(\n    cols = c(\"sex\", \"age\", \"latitude\", \"longitude\", \"msf_sc\"),\n    mapping = ggplot2::aes(colour = sex)\n  ) |&gt;\n  rutils::shush()\n\n\nFigure C.37: Correlation matrix of main variables (Full sample).\n\n\n\n\n\n\n\n\n\nSource: Created by the author.\n\n\n\n\nC.5.2 Analysis Sample\nCodeweighted_data |&gt;\n  plotr:::plot_ggally(\n    cols = c(\"sex\", \"age\", \"latitude\", \"longitude\", \"msf_sc\"),\n    mapping = ggplot2::aes(colour = sex)\n  ) |&gt;\n  rutils::shush()\n\n\nFigure C.38: Correlation matrix of the main variables (Analysis sample).\n\n\n\n\n\n\n\n\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#latitudinal-and-longitudinal-range",
    "href": "qmd/supplementary-material-3.html#latitudinal-and-longitudinal-range",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.6 Latitudinal and Longitudinal Range",
    "text": "C.6 Latitudinal and Longitudinal Range\n\nC.6.0.1 Brazil\nClick here to learn more about Brazil’s extreme points.\nCodebox &lt;-\n  geobr::read_country(2017, showProgress = FALSE) |&gt;\n  rutils::shush() |&gt;\n  dplyr::pull(geom) |&gt;\n  sf::st_bbox() |&gt;\n  as.list()\n\nbrazil_lat_lon &lt;- dplyr::tibble(\n    name = c(\"min\", \"max\", \"range\"),\n    latitude = c(\n      box$ymin,\n      box$ymax,\n      box$ymax - box$ymin\n    ),\n    longitude = c(\n      box$xmin,\n      box$xmax,\n      box$xmax - box$xmin\n    )\n)\n\nbrazil_lat_lon\n\n\nTable C.19: Brazil’s extreme points (full sample).\n\n\n\n\n  \n\n\n\nSource: Brazilian Institute of Geography and Statistics (IBGE), via the shapefiles provided by the geobr R package.\n\n\n\n\nC.6.0.2 Full Sample\nCodebox &lt;-\n  anonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  dplyr::summarise(\n    xmin = min(longitude, na.rm = TRUE),\n    xmax = max(longitude, na.rm = TRUE),\n    xrange = xmax - xmin,\n    ymin = min(latitude, na.rm = TRUE),\n    ymax = max(latitude, na.rm = TRUE),\n    yrange = ymax - ymin\n  ) |&gt;\n  as.list()\n\nfull_sample_lat_lon &lt;- dplyr::tibble(\n    name = c(\"min\", \"max\", \"range\"),\n    latitude = c(\n      box$ymin,\n      box$ymax,\n      box$ymax - box$ymin\n    ),\n    longitude = c(\n      box$xmin,\n      box$xmax,\n      box$xmax - box$xmin\n    )\n)\n\nfull_sample_lat_lon\n\n\nTable C.20: Latitude and longitude statistics of respondents (Full sample).\n\n\n\n\n  \n\n\n\nSource: Created by the author.\n\n\n\n\nC.6.0.3 Analysis Sample\nCodebox &lt;-\n  weighted_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  dplyr::summarise(\n    xmin = min(longitude, na.rm = TRUE),\n    xmax = max(longitude, na.rm = TRUE),\n    xrange = xmax - xmin,\n    ymin = min(latitude, na.rm = TRUE),\n    ymax = max(latitude, na.rm = TRUE),\n    yrange = ymax - ymin\n  ) |&gt;\n  as.list()\n\nanalysis_sample_lat_lon &lt;- dplyr::tibble(\n    name = c(\"min\", \"max\", \"range\"),\n    latitude = c(\n      box$ymin,\n      box$ymax,\n      box$ymax - box$ymin\n    ),\n    longitude = c(\n      box$xmin,\n      box$xmax,\n      box$xmax - box$xmin\n    )\n)\n\nanalysis_sample_lat_lon\n\n\nTable C.21: Latitude and longitude statistics of respondents (Analysis sample).\n\n\n\n\n  \n\n\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#population-distributions",
    "href": "qmd/supplementary-material-3.html#population-distributions",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.7 Population Distributions",
    "text": "C.7 Population Distributions\n\n\n\n\n\n\nFor numerical comparisons, refer to Supplementary Material D.\n\n\n\n\nC.7.1 Brazil\nBrazilian Institute of Geography and Statistics’s (IBGE) population estimates of the population distribution is used in the same timeframe of the sample (2017) (Instituto Brasileiro de Geografia e Estatística, n.d.). The data can be accessed at the IBGE’s SIDRA platform (IBGE’s Table 6579).\nIBGE’s Table 6579 was used instead of Table 6407 because the later does not have the same level of detail for the municipalities.\n\nCodeibge_6579_data_state &lt;-\n  sidrar::get_sidra(api =\"/t/6579/n3/all/v/all/p/2017\") |&gt;\n  rutils::shush() |&gt;\n  dplyr::as_tibble() |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::select(unidade_da_federacao_codigo, valor) |&gt;\n  dplyr::rename(\n    state_code = unidade_da_federacao_codigo,\n    n = valor\n  ) |&gt;\n  dplyr::mutate(state_code = as.integer(state_code)) |&gt;\n  dplyr::relocate(state_code, n)\n\n\n\nCodeplot_6579_ibge_1 &lt;-\n  ibge_6579_data_state |&gt;\n  plotr:::plot_brazil_state(\n    col_fill = \"n\",\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    scale_type = \"binned\"\n  )\n\n\n\n\n\n\n\n\nCodeibge_6579_data_municipality &lt;-\n  sidrar::get_sidra(api =\"/t/6579/n6/all/v/all/p/2017\") |&gt;\n  rutils::shush() |&gt;\n  dplyr::as_tibble() |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::select(municipio_codigo, valor) |&gt;\n  dplyr::rename(\n    municipality_code = municipio_codigo,\n    n = valor\n  ) |&gt;\n  dplyr::mutate(municipality_code = as.integer(municipality_code)) |&gt;\n  dplyr::relocate(municipality_code, n)\n\n\n\nCodemax_limit &lt;-\n  ibge_6579_data_municipality |&gt;\n  dplyr::pull(n) |&gt;\n  rutils:::inverse_log_max(10)\n\nplot_6579_ibge_2 &lt;-\n  ibge_6579_data_municipality |&gt;\n  plotr:::plot_brazil_municipality(\n    col_fill = \"n\",\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    breaks = 10^(seq(1, log10(max_limit) - 1)),\n    reverse = FALSE\n  )\n\n\n\n\n\n\n\n\nCodeplot_6579_ibge_3 &lt;-\n  ibge_6579_data_municipality |&gt;\n  plotr:::plot_brazil_municipality(\n    col_fill = \"n\",\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    alpha = 0.75,\n    breaks = c(100000, 500000, 1000000, 5000000, 10000000, 12000000),\n    point = TRUE\n  )\n\n\n\n\n\n\n\n\nCodeplot_ibge_panel &lt;-\n  patchwork::wrap_plots(\n    plot_6579_ibge_1 |&gt; plotr:::rm_ggspatial_scale(),\n    plot_6579_ibge_2 |&gt; plotr:::rm_ggspatial_scale(),\n    plot_6579_ibge_3 |&gt; plotr:::rm_ggspatial_scale(),\n    ncol = 2,\n    nrow = 2,\n    widths = c(1, 1),\n    heights = c(1, 1)\n  ) +\n  patchwork::plot_annotation(tag_levels = \"A\") &\n  ggplot2::theme_void() &\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    legend.key.size = ggplot2::unit(0.5, \"cm\"),\n    text = ggplot2::element_text(size = 9)\n  )\n\nplot_ibge_panel\n\n\n\n\n\n\n\n\nC.7.2 Full Sample\n\nCodeplot_full_1 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_world_countries(\n    transform = \"log10\",\n    direction = -1,\n    scale_type = \"binned\"\n  )\n\n\n\n\n\n\n\n\nCodeplot_full_2 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_brazil_state(\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    scale_type = \"binned\"\n  )\n\n\n\n\n\n\n\n\nCodemax_limit &lt;-\n  anonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  dplyr::count(municipality_code) |&gt;\n  dplyr::pull(n) |&gt;\n  rutils:::inverse_log_max(10)\n\nplot_full_3 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_brazil_municipality(\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    breaks = 10^(seq(1, log10(max_limit) - 1))\n  )\n\n\n\n\n\n\n\n\nCodemax_limit &lt;-\n  anonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  dplyr::count(municipality_code) |&gt;\n  dplyr::pull(n) |&gt;\n  max()\n\nplot_full_4 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_brazil_municipality(\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    alpha = 0.75,\n    breaks = c(10, 500, 1000, 5000, 10000, 12000),\n    point = TRUE,\n    reverse = TRUE\n  )\n\n\n\n\n\n\n\n\nCodeplot_full_5 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_brazil_point(\n    year = 2017,\n    scale_type = \"discrete\"\n  )\n\n\n\n\n\n\n\n\n\nCodepatchwork::wrap_plots(\n   plot_full_2 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_full_3 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_full_4 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_full_5 |&gt; plotr:::rm_ggspatial_scale(),\n   ncol = 2,\n   nrow = 2,\n   widths = c(1, 1),\n   heights = c(1, 1)\n) +\n  patchwork::plot_annotation(tag_levels = \"A\") &\n  ggplot2::theme_void() &\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    legend.key.size = ggplot2::unit(0.5, \"cm\"),\n    text = ggplot2::element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nC.7.3 Analysis Sample\n\nCodeplot_analysis_1 &lt;-\n  weighted_data |&gt;\n  plotr:::plot_brazil_state(\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    scale_type = \"binned\"\n  )\n\n\n\n\n\n\n\n\nCodemax_limit &lt;-\n  weighted_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  dplyr::count(municipality_code) |&gt;\n  dplyr::pull(n) |&gt;\n  rutils:::inverse_log_max(10)\n\nplot_analysis_2 &lt;-\n  weighted_data |&gt;\n  plotr:::plot_brazil_municipality(\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    breaks = 10^(seq(1, log10(max_limit)))\n  )\n\n\n\n\n\n\n\n\nCodeplot_analysis_3 &lt;-\n  weighted_data |&gt;\n  plotr:::plot_brazil_municipality(\n    year = 2017,\n    transform = \"log10\",\n    direction = -1,\n    alpha = 0.75,\n    breaks = c(10, 500, 1000, 5000, 7500),\n    point = TRUE,\n    reverse = TRUE\n  )\n\n\n\n\n\n\n\n\nCodeplot_analysis_4 &lt;-\n  weighted_data |&gt;\n  plotr:::plot_brazil_point(\n    year = 2017,\n    scale_type = \"discrete\"\n  )\n\n\n\n\n\n\n\n\n\nCodepatchwork::wrap_plots(\n   plot_analysis_1 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_analysis_2 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_analysis_3 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_analysis_4 |&gt; plotr:::rm_ggspatial_scale(),\n   ncol = 2,\n   nrow = 2\n) +\n  patchwork::plot_annotation(tag_levels = \"A\") &\n  ggplot2::theme_void() &\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    legend.key.size = ggplot2::unit(0.5, \"cm\"),\n    text = ggplot2::element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nC.7.4 Brazil (Population) versus Sample\n\nCodepatchwork::wrap_plots(\n   plot_6579_ibge_1 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_full_2 |&gt; plotr:::rm_ggspatial_scale(),\n   ncol = 2,\n   nrow = 1\n) +\n  patchwork::plot_annotation(tag_levels = \"A\") &\n  ggplot2::theme_void() &\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    legend.key.size = ggplot2::unit(0.5, \"cm\"),\n    text = ggplot2::element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nCodepatchwork::wrap_plots(\n   plot_6579_ibge_2 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_full_3 |&gt; plotr:::rm_ggspatial_scale(),\n   ncol = 2,\n   nrow = 1\n) +\n  patchwork::plot_annotation(tag_levels = \"A\") &\n  ggplot2::theme_void() &\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    legend.key.size = ggplot2::unit(0.5, \"cm\"),\n    text = ggplot2::element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nCodepatchwork::wrap_plots(\n   plot_6579_ibge_3 |&gt; plotr:::rm_ggspatial_scale(),\n   plot_full_4 |&gt; plotr:::rm_ggspatial_scale(),\n   ncol = 2,\n   nrow = 1\n) +\n  patchwork::plot_annotation(tag_levels = \"A\") &\n  ggplot2::theme_void() &\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    legend.key.size = ggplot2::unit(0.5, \"cm\"),\n    text = ggplot2::element_text(size = 9)\n  )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#age-distributions",
    "href": "qmd/supplementary-material-3.html#age-distributions",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.8 Age Distributions",
    "text": "C.8 Age Distributions\n\nC.8.1 Brazil\nSee https://sidra.ibge.gov.br/tabela/6407 to learn more about the data.\n\nCodeprettycheck:::assert_internet()\n\nibge_6407_data &lt;-\n  sidrar::get_sidra(\n    api = paste0(\n      \"/t/6407/n3/all/v/606/p/2017/c2/allxt/c58/1140,1141,1144,1145,1152,\",\n      \"2793,3299,3300,3301,3350,6798,40291,118282\"\n    )\n  ) |&gt;\n  dplyr::as_tibble() |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::select(\n    valor, unidade_da_federacao_codigo, unidade_da_federacao, ano, sexo,\n    grupo_de_idade\n  ) |&gt;\n  dplyr::rename(\n    n = valor,\n    state_code = unidade_da_federacao_codigo,\n    state = unidade_da_federacao,\n    year = ano,\n    sex = sexo,\n    age_group = grupo_de_idade\n  ) |&gt;\n  dplyr::arrange(state, sex, age_group) |&gt;\n  dplyr::mutate(\n    year = as.integer(year),\n    country = \"Brazil\",\n    region = orbis::get_brazil_region(state),\n    state_code = as.integer(state_code),\n    sex = dplyr::case_match(\n      sex,\n      \"Homens\" ~ \"Male\",\n      \"Mulheres\" ~ \"Female\"\n    ),\n    sex = factor(sex, ordered = FALSE),\n    age_group = dplyr::case_match(\n      age_group,\n      \"0 a 4 anos\" ~ \"0-4\",\n      \"5 a 9 anos\" ~ \"5-9\",\n      \"10 a 13 anos\" ~ \"10-13\",\n      \"14 a 15 anos\" ~ \"14-15\",\n      \"16 a 17 anos\" ~ \"16-17\",\n      \"18 a 19 anos\" ~ \"18-19\",\n      \"20 a 24 anos\" ~ \"20-24\",\n      \"25 a 29 anos\" ~ \"25-29\",\n      \"30 a 39 anos\" ~ \"30-39\",\n      \"40 a 49 anos\" ~ \"40-49\",\n      \"50 a 59 anos\" ~ \"50-59\",\n      \"60 a 64 anos\" ~ \"60-64\",\n      \"65 anos ou mais\" ~ \"65+\"\n    ),\n    age_group = factor(age_group, ordered = TRUE),\n    age_group_midpoint = dplyr::case_when(\n      age_group == \"0-4\" ~ 2,\n      age_group == \"5-9\" ~ 7,\n      age_group == \"10-13\" ~ 11.5,\n      age_group == \"14-15\" ~ 14.5,\n      age_group == \"16-17\" ~ 16.5,\n      age_group == \"18-19\" ~ 18.5,\n      age_group == \"20-24\" ~ 22,\n      age_group == \"25-29\" ~ 27,\n      age_group == \"30-39\" ~ 34.5,\n      age_group == \"40-49\" ~ 44.5,\n      age_group == \"50-59\" ~ 54.5,\n      age_group == \"60-64\" ~ 62,\n      age_group == \"65+\" ~ 65 + 62 - 54.5 # 65 + 62 - 54.5\n    ),\n    n = as.integer(n * 1000)\n  ) |&gt;\n  dplyr::relocate(\n    year, country, region, state_code, state, sex, age_group,\n    age_group_midpoint, n\n  )\n\nibge_6407_data\n\n\n  \n\n\n\n\n\n\n\n\n\nThe statistics shown is this section are estimations based on the age group midpoints, hence, it must be interpreted with caution. \\(n\\) is presented in thousands of people.\n\n\n\n\nCodeibge_6407_data |&gt;\n  dplyr::rename(age = age_group_midpoint) |&gt;\n  dplyr::mutate(n = n / 1000) |&gt;\n  dplyr::select(age, n) |&gt;\n  tidyr::uncount(n) |&gt;\n  rutils:::stats_summary(\"age\")\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt;\n  dplyr::rename(age = age_group_midpoint) |&gt;\n  dplyr::mutate(n = n / 1000) |&gt;\n  rutils:::summarize_by(\"age\", \"sex\", \"n\")\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt;\n  dplyr::rename(age = age_group_midpoint) |&gt;\n  dplyr::mutate(n = n / 1000) |&gt;\n  rutils:::summarize_by(\"age\", \"region\", \"n\")\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt;\n  dplyr::rename(age = age_group_midpoint) |&gt;\n  dplyr::mutate(n = n / 1000) |&gt;\n  rutils:::summarize_by(\"age\", \"state\", \"n\")\n\n\n  \n\n\n\n\nCodeplot_ibge_6407_age_1 &lt;-\n  ibge_6407_data |&gt;\n  dplyr::rename(age = age_group_midpoint) |&gt;\n  dplyr::mutate(n = n / 1000) |&gt;\n  dplyr::select(sex, age, n) |&gt;\n  tidyr::uncount(n) |&gt;\n  plotr:::plot_age_pyramid(\n    breaks = c(0, 10, 20, 30, 40, 50, 60, 65, 90)\n  )\n\n\n\n\n\n\n\n\nCodeplot_ibge_6407_age_2 &lt;-\n  ibge_6407_data |&gt;\n  dplyr::rename(age = age_group_midpoint) |&gt;\n  dplyr::mutate(n = n / 1000) |&gt;\n  dplyr::select(state_code, age, n) |&gt;\n  tidyr::uncount(n) |&gt;\n  plotr:::plot_brazil_state(\n    col_fill = \"age\",\n    year = 2017,\n    transform = \"identity\",\n    direction = -1,\n    quiet = TRUE,\n    scale_type = \"binned\"\n  )\n\n\n\n\n\n\n\n\nC.8.2 Full Sample\n\nCodeanonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  rutils:::stats_summary(\"age\")\n\n\n  \n\n\n\n\nCodeanonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  rutils:::summarize_by(\"age\", \"sex\")\n\n\n  \n\n\n\n\nCodeanonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  rutils:::summarize_by(\"age\", \"region\")\n\n\n  \n\n\n\n\nCodeanonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  rutils:::summarize_by(\"age\", \"state\")\n\n\n  \n\n\n\n\nCodeplot_full_age_1&lt;-\n  anonymized_data |&gt;\n  plotr:::plot_age_pyramid()\n\n\n\n\n\n\n\n\nCodeplot_full_age_2 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_brazil_state(\n    col_fill = \"age\",\n    year = 2017,\n    transform = \"identity\",\n    direction = -1,\n    quiet = TRUE,\n    scale_type = \"binned\"\n  )\n\n\n\n\n\n\n\n\nCodeplot_full_age_3 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_brazil_municipality(\n    col_fill = \"age\", # Means\n    year = 2017,\n    direction = -1,\n    quiet = TRUE\n  )\n\n\n\n\n\n\n\nCodeplot_age_sex_weigth_series &lt;-\n  anonymized_data |&gt;\n  dplyr::filter(age &lt;= 50) |&gt;\n  plotr:::plot_series(\n    col_y = \"weight\",\n    y_label = \"Weight (kg)\"\n  )\n\n\nFigure C.39: Relation between age and weight (kg), divided by sex and aggregated by the mean. The gray line represents both sex. Vertical lines represent the standard error of the mean (SEM).\n\n\n\n\n\n\n\n\n\n[Source: Created by the author, based on a data visualization from Roenneberg et al. (2007, Figure 4).{.legend}\n\n\n\n\nC.8.3 Analysis Sample\n\nCodeweighted_data |&gt; rutils:::stats_summary(\"age\")\n\n\n  \n\n\n\n\nCodeweighted_data |&gt; rutils:::summarize_by(\"age\", \"sex\")\n\n\n  \n\n\n\n\nCodeweighted_data |&gt; rutils:::summarize_by(\"age\", \"region\")\n\n\n  \n\n\n\n\nCodeweighted_data |&gt; rutils:::summarize_by(\"age\", \"state\")\n\n\n  \n\n\n\n\nCodeplot_analysis_age_1 &lt;-\n  weighted_data |&gt;\n  plotr:::plot_age_pyramid()\n\n\n\n\n\n\n\n\nCodeplot_analysis_age_2 &lt;-\n  anonymized_data |&gt;\n  plotr:::plot_brazil_state(\n    col_fill = \"age\", # Means\n    year = 2017,\n    direction = -1,\n    quiet = TRUE,\n    scale_type = \"binned\"\n  )\n\n\n\n\n\n\n\n\nCodeplot_analysis_age_3 &lt;-\n  weighted_data |&gt;\n  plotr:::plot_brazil_municipality(\n    col_fill = \"age\", # Means\n    year = 2017,\n    direction = -1,\n    quiet = TRUE\n  )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#weight-distributions",
    "href": "qmd/supplementary-material-3.html#weight-distributions",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.9 Weight Distributions",
    "text": "C.9 Weight Distributions\n\nC.9.1 Full Sample\nCodeweighted_data |&gt;\n  dplyr::filter(!rutils::test_outlier(weight)) |&gt;\n  plotr:::plot_latitude_series(\n    col = \"weight\",\n    y_label = \"Weight (kg)\"\n  )\n\n\nFigure C.40: Boxplots of mean weight values (kg) aggregated by 1° latitude intervals, illustrating the relationship between latitude and weight. The × symbol points to the mean. The orange line represents a linear regression.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-3.html#chronotype-distributions",
    "href": "qmd/supplementary-material-3.html#chronotype-distributions",
    "title": "Appendix C — Exploratory Data Analysis",
    "section": "\nC.10 Chronotype distributions",
    "text": "C.10 Chronotype distributions\n\nC.10.1 Full Sample\n\nCodeanonymized_data |&gt;\n  dplyr::filter(country == \"Brazil\") |&gt;\n  rutils:::stats_summary(\"msf_sc\", as_list = TRUE) |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::mutate(\n    dplyr::across(\n      .cols = dplyr::where(hms::is_hms),\n      .fns = lubritime::round_time\n    ),\n    dplyr::across(\n      .cols = dplyr::everything(),\n      .fns = as.character\n    )\n  ) |&gt;\n  tidyr::pivot_longer(cols = dplyr::everything())\n\n\n  \n\n\n\n\nCodeanonymized_data |&gt; plotr:::get_msf_sc_cutoffs()\n\n\n  \n\n\n\nCodeplot_age_sex_series &lt;-\n  anonymized_data |&gt;\n  dplyr::filter(age &lt;= 50) |&gt;\n  plotr:::plot_series()\n\n\nFigure C.41: Observed relation between age and chronotype, divided by sex and aggregated by the mean. Chronotype is represented by the local time of the sleep corrected midpoint between sleep onset and sleep end on work-free days (MSFsc), MCTQ proxy for measuring the chronotype. The gray line represents both sex. Vertical lines represent the standard error of the mean (SEM).\n\n\n\n\n\n\n\n\n\nSource: Created by the author based on a data visualization from Roenneberg et al. (2007, Figure 4).\n\n\n\n\n\nFigure C.42: Distribution of European chronotypes by age, as shown in Roenneberg et al. (2007), for comparison.\n\n\nSource: Reproduced from Roenneberg et al. (2007, Figure 4).\n\n\n\n\nCodeplot_age_sex_series &lt;-\n  anonymized_data |&gt;\n  dplyr::filter(!rutils::test_outlier(weight)) |&gt;\n  plotr:::plot_series(\n    col_x = \"weight\",\n    x_label = \"Weigth\",\n    date_breaks = \"30 min\"\n  )\n\n\n\n\n\n\n\n\nC.10.2 Analysis Sample\n\nCodeweighted_data |&gt;\n  rutils:::stats_summary(\"msf_sc\", as_list = TRUE) |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::mutate(\n    dplyr::across(\n      .cols = dplyr::where(hms::is_hms),\n      .fns = lubritime::round_time\n    ),\n    dplyr::across(\n      .cols = dplyr::everything(),\n      .fns = as.character\n    )\n  ) |&gt;\n  tidyr::pivot_longer(cols = dplyr::everything())\n\n\n  \n\n\n\n\nCodeweighted_data |&gt; plotr:::get_msf_sc_cutoffs()\n\n\n  \n\n\n\n\nCodeweighted_data |&gt;\n  dplyr::mutate(\n    msf_sc_category = plotr:::categorize_msf_sc(msf_sc),\n    msf_sc_category = factor(\n      msf_sc_category,\n      levels = c(\n        \"Extremely early\", \"Moderately early\", \"Slightly early\",\n        \"Intermediate\", \"Slightly late\", \"Moderately late\",\n        \"Extremely late\"\n      ),\n      ordered = TRUE\n    )\n  ) |&gt;\n  rutils:::summarize_by(\"msf_sc\", \"msf_sc_category\")\n\n\n  \n\n\n\nCodeweighted_data |&gt; plotr:::plot_chronotype()\n\n\nFigure C.43: Observed distribution of the local time of the sleep-corrected midpoint between sleep onset and sleep end on work-free days (MSFsc), a proxy for chronotype.  Chronotypes are categorized into quantiles, ranging from extremely early (\\(0 |- 0.11\\)) to extremely late (\\(0.88 - 1\\)).\n\n\n\n\n\n\n\n\n\n[Source: Created by the author based on a data visualization from Roenneberg et al. (2019, Figure 1).{.legend}\n\n\n\n\n\nFigure C.44: Distribution of European chronotypes, as shown in Roenneberg et al. (2019) (for comparison).\n\n\n(Source: Reproduced from Roenneberg et al., 2019[Figure 1, right part].) {.legend}\n\n\n\n\nCodeweighted_data |&gt; rutils:::summarize_by(\"msf_sc\", \"sex\")\n\n\n  \n\n\n\n\nCodeweighted_data |&gt;\n  dplyr::mutate(\n    age_group = dplyr::case_when(\n      dplyr::between(age, 0, 4) ~ \"0-4\",\n      dplyr::between(age, 5, 9) ~ \"5-9\",\n      dplyr::between(age, 10, 13) ~ \"10-13\",\n      dplyr::between(age, 14, 15) ~ \"14-15\",\n      dplyr::between(age, 16, 17) ~ \"16-17\",\n      dplyr::between(age, 18, 19) ~ \"18-19\",\n      dplyr::between(age, 20, 24) ~ \"20-24\",\n      dplyr::between(age, 25, 29) ~ \"25-29\",\n      dplyr::between(age, 30, 39) ~ \"30-39\",\n      dplyr::between(age, 40, 49) ~ \"40-49\",\n      dplyr::between(age, 50, 59) ~ \"50-59\",\n      dplyr::between(age, 60, 64) ~ \"60-64\",\n      age &gt;= 65 ~ \"65+\"\n    )\n  ) |&gt;\n  rutils:::summarize_by(\"msf_sc\", \"age_group\")\n\n\n  \n\n\n\nCodeplot_age_sex_series &lt;-\n  weighted_data |&gt;\n  dplyr::filter(age &lt;= 50) |&gt;\n  plotr:::plot_series()\n\n\nFigure C.45: Observed relation between age and chronotype, divided by sex and aggregated by the mean (Analysis sample). Chronotype is represented by the local time of the sleep corrected midpoint between sleep onset and sleep end on work-free days (MSFsc), MCTQ proxy for measuring the chronotype. The gray line represents both sex. Vertical lines represent the standard error of the mean (SEM).\n\n\n\n\n\n\n\n\n\nSource: Created by the author based on a data visualization from Roenneberg et al. (2007, Figure 4).\n\n\n\n\nCodeplot_age_sex_series &lt;-\n  weighted_data |&gt;\n  dplyr::filter(!rutils::test_outlier(weight), weight &gt; 45) |&gt;\n  plotr:::plot_series(\n    col_x = \"weight\",\n    x_label = \"Weigth\",\n    date_breaks = \"30 min\"\n  )\n\n\n\n\n\n\n\n\nCodeweighted_data |&gt; rutils:::summarize_by(\"msf_sc\", \"region\")\n\n\n  \n\n\n\n\nCodeweighted_data |&gt; rutils:::summarize_by(\"msf_sc\", \"state\")\n\n\n  \n\n\n\n\nCodelimits &lt;- # Interquartile range (IQR): Q3 - Q1\n  c(\n    weighted_data |&gt;\n      dplyr::pull(msf_sc) |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric() |&gt;\n      stats::quantile(0.25, na.rm = TRUE),\n    weighted_data |&gt;\n      dplyr::pull(msf_sc) |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric() |&gt;\n      stats::quantile(0.75, na.rm = TRUE)\n  )\n\nweighted_data |&gt;\n  dplyr::mutate(\n    msf_sc =\n      msf_sc |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric()\n  ) |&gt;\n  plotr:::plot_brazil_state(\n    col_fill = \"msf_sc\",\n    year = 2017,\n    breaks = seq(limits[1], limits[2], length.out = 6) |&gt; groomr::rm_caps(),\n    labels = plotr:::format_as_hm,\n    limits = limits, # !!!\n    quiet = TRUE\n  )\n\n\n\n\n\n\n\n\nCodelimits &lt;- # Interquartile range (IQR): Q3 - Q1\n  c(\n    weighted_data |&gt;\n      dplyr::pull(msf_sc) |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric() |&gt;\n      quantile(0.25, na.rm = TRUE),\n    weighted_data |&gt;\n      dplyr::pull(msf_sc) |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric() |&gt;\n      quantile(0.75, na.rm = TRUE)\n  )\n\nweighted_data |&gt;\n  dplyr::mutate(\n    msf_sc =\n      msf_sc |&gt;\n      lubritime::link_to_timeline() |&gt;\n      as.numeric()\n  ) |&gt;\n  plotr:::plot_brazil_municipality(\n    col_fill = \"msf_sc\",\n    year = 2017,\n    breaks = seq(limits[1], limits[2], length.out = 6) |&gt; groomr::rm_caps(),\n    labels = plotr:::format_as_hm,\n    limits = limits,\n    quiet = TRUE,\n    reverse = TRUE\n  )\n\n\n\n\n\n\n\n\nCodeweighted_data |&gt;\n  dplyr::mutate(\n    msf_sc_category = plotr:::categorize_msf_sc(msf_sc),\n    msf_sc_category = factor(\n      msf_sc_category,\n      levels = c(\n        \"Extremely early\", \"Moderately early\", \"Slightly early\",\n        \"Intermediate\", \"Slightly late\", \"Moderately late\",\n        \"Extremely late\"\n      ),\n      ordered = TRUE\n    )\n  ) |&gt;\n  plotr:::plot_brazil_point(\n    col_group = \"msf_sc_category\",\n    year = 2017,\n    scale_type = \"discrete\"\n  )\n\n\n\n\n\n\n\nCodeplot &lt;-\n  weighted_data |&gt;\n  dplyr::mutate(\n    msf_sc_category = plotr:::categorize_msf_sc(msf_sc),\n    msf_sc_category = factor(\n      msf_sc_category,\n      levels = c(\n        \"Extremely early\", \"Moderately early\", \"Slightly early\",\n        \"Intermediate\", \"Slightly late\", \"Moderately late\",\n        \"Extremely late\"\n      ),\n      ordered = TRUE\n    )\n  ) |&gt;\n  plotr:::plot_brazil_point(\n    col_group = \"msf_sc_category\",\n    year = 2017,\n    size = 0.1,\n    alpha = 1,\n    print = FALSE,\n    scale_type = \"discrete\"\n  ) +\n  ggplot2::theme(\n    axis.title = ggplot2::element_blank(),\n    axis.text= ggplot2::element_blank(),\n    axis.ticks = ggplot2::element_blank(),\n    panel.grid.major = ggplot2::element_blank(),\n    panel.grid.minor = ggplot2::element_blank(),\n    legend.position = \"none\"\n  )\n\nplot |&gt;\n  plotr:::rm_ggspatial_scale() +\n  ggplot2::facet_wrap(~msf_sc_category, ncol = 4, nrow = 2)\n\n\nFigure C.46: Observed geographical distribution of MSFsc values by a spectrum of extremely early and extremely late, illustrating how chronotype varies with latitude in Brazil.  MSFsc is a proxy for chronotype, representing the midpoint of sleep on work-free days, adjusted for sleep debt. Chronotypes are categorized into quantiles, ranging from extremely early (\\(0 |- 0.11\\)) to extremely late (\\(0.88 - 1\\)). No discernible pattern emerges from the distribution of chronotypes across latitudes.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.\n\n\n\nCodeweighted_data |&gt; plotr:::plot_latitude_series()\n\n\nFigure C.47: Boxplots of observed mean MSFsc values aggregated by 1° latitude intervals, illustrating the relationship between latitude and chronotype. MSFsc represents the local time of the sleep-corrected midpoint between sleep onset and sleep end on work-free days, a proxy for chronotype. Higher MSFsc values indicate later chronotypes. The × symbol points to the mean. The orange line represents a linear regression. The differences in mean/median values across latitudes are minimal relative to the Munich ChronoType Questionnaire (MCTQ) scale.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.\n\n\n\n\nCodeweighted_data |&gt;\n  plotr:::plot_series(\n    col_x = \"latitude\",\n    x_label = \"Latitude\",\n    date_breaks = \"15 min\",\n    reverse = TRUE,\n    change_sign = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\n\nInstituto Brasileiro de Geografia e Estatística. (n.d.). Tabela 6579: População residente estimada [Data set]. SIDRA. Retrieved November 16, 2023, from https://sidra.ibge.gov.br/Tabela/3939\n\n\nRoenneberg, T., Kuehnle, T., Juda, M., Kantermann, T., Allebrandt, K., Gordijn, M., & Merrow, M. (2007). Epidemiology of the human circadian clock. Sleep Medicine Reviews, 11(6), 429–438. https://doi.org/10.1016/j.smrv.2007.07.005\n\n\nRoenneberg, T., Wirz-Justice, A., Skene, D. J., Ancoli-Israel, S., Wright, K. P., Dijk, D.-J., Zee, P., Gorman, M. R., Winnebeck, E. C., & Klerman, E. B. (2019). Why should we abolish daylight saving time? Journal of Biological Rhythms, 34(3), 227–230. https://doi.org/10.1177/0748730419854197",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-4.html",
    "href": "qmd/supplementary-material-4.html",
    "title": "Appendix D — Sample Balance",
    "section": "",
    "text": "D.1 Overview\nThis document outlines the procedure for balancing the sample used in the hypothesis test.\nAs with all analyses in this thesis, the process is fully reproducible and was conducted using the R programming language (R Core Team, n.d.) along with the Quarto publishing system (Allaire et al., n.d.).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Sample Balance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-4.html#setting-the-enviroment",
    "href": "qmd/supplementary-material-4.html#setting-the-enviroment",
    "title": "Appendix D — Sample Balance",
    "section": "\nD.2 Setting the Enviroment",
    "text": "D.2 Setting the Enviroment\n\nCodelibrary(broom)\nlibrary(dplyr)\nlibrary(here)\nlibrary(janitor)\nlibrary(magrittr)\nlibrary(parsnip)\nlibrary(report)\nlibrary(rlang)\nlibrary(sidrar)\nlibrary(targets)\nlibrary(tidyr)\nlibrary(workflows)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Sample Balance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-4.html#loading-and-processing-the-sample-data",
    "href": "qmd/supplementary-material-4.html#loading-and-processing-the-sample-data",
    "title": "Appendix D — Sample Balance",
    "section": "\nD.3 Loading and Processing the Sample Data",
    "text": "D.3 Loading and Processing the Sample Data\nYou can visualize the data wrangling pipeline at the code repository in the _targets.R file.\n\n\n\n\n\n\nPlease note that these values refer to the group of states in the Brazilian time zone UTC-3, not the whole country.\n\n\n\n\nCodetargets::tar_make(script = here::here(\"_targets.R\"))\n\n\n\nCodedata &lt;-\n  targets::tar_read(\"filtered_data\", store = here::here(\"_targets\")) |&gt;\n  dplyr::filter(state %in% orbis::get_brazil_state_by_utc(-3, \"state\")) |&gt;\n  dplyr::select(country, region, state, sex, age, msf_sc) |&gt;\n  tidyr::drop_na(state, sex, age) |&gt;\n  dplyr::mutate(\n    age_group = dplyr::case_when(\n      age &gt;= 18 & age &lt; 20 ~ \"18-19\",\n      age &gt;= 20 & age &lt; 25 ~ \"20-24\",\n      age &gt;= 25 & age &lt; 30 ~ \"25-29\",\n      age &gt;= 30 & age &lt; 40 ~ \"30-39\",\n      age &gt;= 40 & age &lt; 50 ~ \"40-49\",\n      age &gt;= 50 & age &lt; 60 ~ \"50-59\",\n      age &gt;= 60 & age &lt; 65 ~ \"60-64\",\n      age &gt;= 65 ~ \"65+\"\n    ),\n    age_group = factor(\n      age_group,\n      levels = c(\n        \"18-19\", \"20-24\", \"25-29\", \"30-39\", \"40-49\", \"50-59\", \"60-64\",\n        \"65+\"\n      ),\n      ordered = TRUE\n    )\n  ) |&gt;\n  dplyr::relocate(age_group, .after = age)\n\ndata",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Sample Balance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-4.html#getting-the-population-data",
    "href": "qmd/supplementary-material-4.html#getting-the-population-data",
    "title": "Appendix D — Sample Balance",
    "section": "\nD.4 Getting the Population Data",
    "text": "D.4 Getting the Population Data\nThe sample data was obtained in 2017 from October 15th to 21st by a broadcast of the online questionnaire on a popular Brazil’s Sunday TV show with national reach (Rede Globo, 2017). Here the Brazilian Institute of Geography and Statistics’s (IBGE) Continuous National Household Sample Survey (PNAD Contínua) its used for a estimate of the population distribution in this timeframe (Instituto Brasileiro de Geografia e Estatística, n.d.). The data can be accessed at the IBGE’s SIDRA platform (IBGE’s Table 6407).\nThe balance is made considering the distribution of the population related only to the variables state, sex and age.\n\n\n\n\n\n\nPlease note that these values refer to the group of states in the Brazilian time zone UTC-3, not the whole country.\n\n\n\n\nCodeprettycheck:::assert_internet()\n\nibge_6407_data &lt;-\n  sidrar::get_sidra(\n    api = paste0(\n      \"/t/6407/n1/all/n3/all/v/606/p/2017/c2/all/c58/1144,1145,\",\n      \"1152,2793,3299,3300,3301,3302,6798\"\n    )\n  ) |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::select(\n    dplyr::all_of(\n      c(\n        \"Valor\", \"Brasil e Unidade da Federação\", \"Ano\", \"Sexo\",\n        \"Grupo de idade\"\n      )\n    )\n  ) |&gt;\n  dplyr::rename(\n    n = Valor,\n    state = `Brasil e Unidade da Federação`,\n    year = Ano,\n    sex = Sexo,\n    age_group = `Grupo de idade`\n  ) |&gt;\n  dplyr::filter(\n    state != \"Brasil\",\n    sex != \"Total\",\n    age_group != \"60 anos ou mais\"\n  ) |&gt;\n  dplyr::arrange(state, sex, age_group) |&gt;\n  dplyr::mutate(\n    year = as.integer(year),\n    country = \"Brazil\",\n    region = orbis::get_brazil_region(state),\n    sex = dplyr::case_when(\n      sex == \"Homens\" ~ \"Male\",\n      sex == \"Mulheres\" ~ \"Female\"\n    ),\n    sex = factor(sex, ordered = FALSE),\n    age_group = dplyr::case_when(\n      age_group == \"18 a 19 anos\" ~ \"18-19\",\n      age_group == \"20 a 24 anos\" ~ \"20-24\",\n      age_group == \"25 a 29 anos\" ~ \"25-29\",\n      age_group == \"30 a 39 anos\" ~ \"30-39\",\n      age_group == \"40 a 49 anos\" ~ \"40-49\",\n      age_group == \"50 a 59 anos\" ~ \"50-59\",\n      age_group == \"60 a 64 anos\" ~ \"60-64\",\n      age_group == \"65 anos ou mais\" ~ \"65+\"\n    ),\n    age_group = factor(age_group, ordered = TRUE),\n    n = as.integer(n * 1000)\n  ) |&gt;\n  dplyr::relocate(year, country, region, state, sex, age_group, n) |&gt;\n  dplyr::filter(state %in% orbis::get_brazil_state_by_utc(-3, \"state\")) |&gt;\n  dplyr::mutate(\n    n_rel = n / sum(n),\n    n_per = (n / sum(n)) * 100\n  )\n\nibge_6407_data\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt; dplyr::glimpse()\n#&gt; Rows: 336\n#&gt; Columns: 9\n#&gt; $ year      &lt;int&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 201…\n#&gt; $ country   &lt;chr&gt; \"Brazil\", \"Brazil\", \"Brazil\", \"Brazil\", \"Brazil\", \"Brazil…\n#&gt; $ region    &lt;chr&gt; \"Northeast\", \"Northeast\", \"Northeast\", \"Northeast\", \"Nort…\n#&gt; $ state     &lt;chr&gt; \"Alagoas\", \"Alagoas\", \"Alagoas\", \"Alagoas\", \"Alagoas\", \"A…\n#&gt; $ sex       &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Female, F…\n#&gt; $ age_group &lt;ord&gt; 18-19, 20-24, 25-29, 30-39, 40-49, 50-59, 60-64, 65+, 18-…\n#&gt; $ n         &lt;int&gt; 54000, 140000, 131000, 236000, 201000, 140000, 57000, 127…\n#&gt; $ n_rel     &lt;dbl&gt; 0.00037816714988, 0.00098043335154, 0.00091740549323, 0.0…\n#&gt; $ n_per     &lt;dbl&gt; 0.037816714988, 0.098043335154, 0.091740549323, 0.1652730…\n\n\n\nCodeibge_6407_data |&gt; rutils:::summarize_data(\"age_group\")\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt; rutils:::summarize_data(\"sex\")\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt; rutils:::summarize_data(\"state\")\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt; rutils:::summarize_data(\"region\")\n\n\n  \n\n\n\n\nCodeibge_6407_data |&gt; rutils:::summarize_data(\"country\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Sample Balance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-4.html#comparing-sample-data-with-the-population-data",
    "href": "qmd/supplementary-material-4.html#comparing-sample-data-with-the-population-data",
    "title": "Appendix D — Sample Balance",
    "section": "\nD.5 Comparing Sample Data With the Population Data",
    "text": "D.5 Comparing Sample Data With the Population Data\n\n\n\n\n\n\nPlease note that these values refer to the group of states in the Brazilian time zone UTC-3, not the whole country.\n\n\n\n\nCodedata |&gt; rutils:::compare_sample(ibge_6407_data, \"age_group\")\n\n\n  \n\n\n\n\nCodedata |&gt; rutils:::compare_sample(ibge_6407_data, \"sex\")\n\n\n  \n\n\n\n\nCodedata |&gt; rutils:::compare_sample(ibge_6407_data, \"state\")\n\n\n  \n\n\n\n\nCodedata |&gt; rutils:::compare_sample(ibge_6407_data, \"region\")\n\n\n  \n\n\n\n\nCodedata |&gt; rutils:::compare_sample(ibge_6407_data, \"country\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Sample Balance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-4.html#adjusting-sample-data-with-the-population-data",
    "href": "qmd/supplementary-material-4.html#adjusting-sample-data-with-the-population-data",
    "title": "Appendix D — Sample Balance",
    "section": "\nD.6 Adjusting Sample Data with the Population Data",
    "text": "D.6 Adjusting Sample Data with the Population Data\nThe tables above indicate an overrepresentation of states like São Paulo and Rio de Janeiro, while states such as Amapá and Tocantins are underrepresented. To achieve a balanced sample, a cell weighting procedure was use (see Kalton & Flores-Cervantes, 2003) based on the characteristics of state, sex and age group, using data from the 2017 PNAD as a reference.\n\\[\n\\text{Cell weighting}: \\cfrac{\\% \\ \\text{2017 PNAD}}{\\% \\ \\text{Survey sample}}\n\\]\n\nCodeibge_6407_data |&gt;\n  dplyr::select(state, sex, age_group, n) |&gt;\n  tidyr::pivot_wider(names_from = state, values_from = n) |&gt;\n  janitor::adorn_totals(c(\"row\", \"col\")) |&gt;\n  janitor::adorn_percentages(\"col\") |&gt;\n  janitor::adorn_pct_formatting(digits = 3) |&gt;\n  janitor::adorn_ns() |&gt;\n  dplyr::as_tibble()\n\n\n  \n\n\n\nClick here to learn more.\n\nCodeweights_data &lt;-\n  ibge_6407_data |&gt;\n  dplyr::left_join(\n    data |&gt;\n      dplyr::summarise(\n        n = dplyr::n(),\n        .by = c(\"country\", \"region\", \"state\", \"sex\", \"age_group\")\n      ) |&gt;\n      dplyr::mutate(\n        n_rel = n / sum(n),\n        n_per = (n / sum(n)) * 100\n      ),\n    by = c(\"country\", \"region\", \"state\", \"sex\", \"age_group\"),\n    suffix = c(\"_pnad\", \"_sample\")\n  ) |&gt;\n  dplyr::relocate(year, .before = country) |&gt;\n  dplyr::mutate(\n    cell_weight = parsnip::importance_weights(n_per_pnad / n_per_sample)\n    # inv_prob_weight = parsnip::importance_weights(1 / (n / sum(n)))\n  ) |&gt;\n  dplyr::select(\n    country, region, state, sex, age_group, cell_weight\n  ) |&gt;\n  dplyr::arrange(state, sex, age_group)\n\nweights_data\n\n\n  \n\n\n\n\nCodetest_data &lt;-\n  data |&gt;\n  dplyr::mutate(\n    msf_sc =\n      msf_sc |&gt;\n      lubritime:::link_to_timeline(\n        threshold = hms::parse_hms(\"12:00:00\")\n      ) |&gt;\n      as.numeric()\n  ) |&gt;\n  dplyr::left_join(\n    weights_data,\n    by = c(\"country\", \"region\", \"state\", \"sex\", \"age_group\")\n  )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Sample Balance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-4.html#testing-fiting-a-model-with-the-weights",
    "href": "qmd/supplementary-material-4.html#testing-fiting-a-model-with-the-weights",
    "title": "Appendix D — Sample Balance",
    "section": "\nD.7 Testing Fiting a Model with the Weights",
    "text": "D.7 Testing Fiting a Model with the Weights\n\nmodel &lt;-\n  parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\") |&gt;\n  parsnip::set_mode(\"regression\")\n\n\nworkflow &lt;-\n  workflows::workflow() |&gt;\n  workflows::add_case_weights(cell_weight) |&gt;\n  workflows::add_formula(msf_sc ~ sex + age) |&gt;\n  workflows::add_model(model)\n\nworkflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; msf_sc ~ sex + age\n#&gt; \n#&gt; ── Case Weights ─────────────────────────────────────────────────────────────\n#&gt; cell_weight\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nfit &lt;- workflow |&gt; parsnip::fit(test_data)\n\n\nCodefit |&gt;\n  broom::tidy() |&gt;\n  janitor::adorn_rounding(5)\n\n\n  \n\n\n\n\nCodefit |&gt;\n  broom::glance() |&gt;\n  tidyr::pivot_longer(cols = dplyr::everything()) |&gt;\n  janitor::adorn_rounding(10)\n\n\n  \n\n\n\n\nCodefit_engine &lt;- fit |&gt; parsnip::extract_fit_engine()\n\nfit_engine |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -48700.087  -2574.721   -181.577   2571.318  48926.513 \n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate   Std. Error   t value   Pr(&gt;|t|)    \n#&gt; (Intercept) 19828.531597    69.775678 284.17541 &lt; 2.22e-16 ***\n#&gt; sexMale       367.277867    39.336490   9.33682 &lt; 2.22e-16 ***\n#&gt; age          -123.263571     1.736254 -70.99398 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4568.365 on 65821 degrees of freedom\n#&gt; Multiple R-squared:  0.07261391, Adjusted R-squared:  0.07258573 \n#&gt; F-statistic: 2576.877 on 2 and 65821 DF,  p-value: &lt; 2.2204e-16\n\n\n\nfit_engine_2 &lt;-\n  lm(\n    formula = msf_sc ~ sex + age,\n    weights = cell_weight,\n    data = test_data\n  )\n\nfit_engine_2 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = msf_sc ~ sex + age, data = test_data, weights = cell_weight)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -48700.087  -2574.721   -181.577   2571.318  48926.513 \n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate   Std. Error   t value   Pr(&gt;|t|)    \n#&gt; (Intercept) 19828.531597    69.775678 284.17541 &lt; 2.22e-16 ***\n#&gt; sexMale       367.277867    39.336490   9.33682 &lt; 2.22e-16 ***\n#&gt; age          -123.263571     1.736254 -70.99398 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4568.365 on 65821 degrees of freedom\n#&gt; Multiple R-squared:  0.07261391, Adjusted R-squared:  0.07258573 \n#&gt; F-statistic: 2576.877 on 2 and 65821 DF,  p-value: &lt; 2.2204e-16\n\n\nCode\nreport::report(fit_engine_2)\n#&gt; We fitted a linear model (estimated using OLS) to predict msf_sc with sex\n#&gt; and age (formula: msf_sc ~ sex + age). The model explains a statistically\n#&gt; significant and weak proportion of variance (R2 = 0.07, F(2, 65821) =\n#&gt; 2576.88, p &lt; .001, adj. R2 = 0.07). The model's intercept, corresponding to\n#&gt; sex = Female and age = 0, is at 19828.53 (95% CI [19691.77, 19965.29],\n#&gt; t(65821) = 284.18, p &lt; .001). Within this model:\n#&gt; \n#&gt;   - The effect of sex [Male] is statistically significant and positive (beta =\n#&gt; 367.28, 95% CI [290.18, 444.38], t(65821) = 9.34, p &lt; .001; Std. beta =\n#&gt; 0.07, 95% CI [0.06, 0.08])\n#&gt;   - The effect of age is statistically significant and negative (beta =\n#&gt; -123.26, 95% CI [-126.67, -119.86], t(65821) = -70.99, p &lt; .001; Std. beta =\n#&gt; -0.27, 95% CI [-0.27, -0.26])\n#&gt; \n#&gt; Standardized parameters were obtained by fitting the model on a standardized\n#&gt; version of the dataset. 95% Confidence Intervals (CIs) and p-values were\n#&gt; computed using a Wald t-distribution approximation.\n\n\n\nlm(formula = msf_sc ~ sex + age, data = test_data) |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = msf_sc ~ sex + age, data = test_data)\n#&gt; \n#&gt; Residuals:\n#&gt;         Min          1Q      Median          3Q         Max \n#&gt; -15994.4448  -3559.2346   -366.0323   3310.4309  17395.6007 \n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate   Std. Error   t value   Pr(&gt;|t|)    \n#&gt; (Intercept) 20028.288311    71.784441 279.00598 &lt; 2.22e-16 ***\n#&gt; sexMale       512.589746    41.567515  12.33150 &lt; 2.22e-16 ***\n#&gt; age          -127.047687     2.120366 -59.91782 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5033.417 on 65821 degrees of freedom\n#&gt; Multiple R-squared:  0.05319379, Adjusted R-squared:  0.05316502 \n#&gt; F-statistic: 1848.989 on 2 and 65821 DF,  p-value: &lt; 2.2204e-16\n\n\n\n\n\nAllaire, J. J., Teague, C., Xie, Y., & Dervieux, C. (n.d.). Quarto [Computer software]. Zenodo. https://doi.org/10.5281/ZENODO.5960048\n\n\nInstituto Brasileiro de Geografia e Estatística. (n.d.). Tabela 6407: População residente, por sexo e grupos de idade [Table 6407: Resident population, by sex and age groups] [Data set]. SIDRA. Retrieved November 16, 2023, from https://sidra.ibge.gov.br/tabela/6407\n\n\nKalton, G., & Flores-Cervantes, I. (2003). Weighting methods. Journal of Official Statistics, 19(2), 81–97.\n\n\nR Core Team. (n.d.). R: A language and environment for statistical computing [Computer software]. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nRede Globo. (2017, October 15). Metade da população se sente mal no horário de verão, revela pesquisa (TV program Fantástico) [Half of the population feels unwell during daylight saving time, research reveals] [Video recording]. Rede Globo. https://g1.globo.com/fantastico/noticia/2017/10/metade-da-populacao-se-sente-mal-no-horario-de-verao-revela-pesquisa.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Sample Balance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html",
    "href": "qmd/supplementary-material-5.html",
    "title": "Appendix E — Hypothesis Test A",
    "section": "",
    "text": "E.1 Overview\nThis document focuses on testing the thesis hypothesis (Test A) using the methods described in the supplemental material titled Methods.\nThe assumptions addressed here are those relevant to general linear models, with further information available in the supplemental material titled Overview of General Linear Models.\nAs with all analyses in this thesis, the process is fully reproducible and was conducted using the R programming language (R Core Team, n.d.) along with the Quarto publishing system (Allaire et al., n.d.).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#setting-the-environment",
    "href": "qmd/supplementary-material-5.html#setting-the-environment",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.2 Setting the Environment",
    "text": "E.2 Setting the Environment\n\nCodelibrary(brandr)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(ggplotify)\nlibrary(here)\nlibrary(hms)\nlibrary(janitor)\nlibrary(latex2exp)\nlibrary(lmtest)\nlibrary(lubritime) # github.com/danielvartan/lubritime\nlibrary(magrittr)\nlibrary(methods)\nlibrary(olsrr)\nlibrary(parsnip)\nlibrary(patchwork)\nlibrary(performance)\nlibrary(plotr) # github.com/danielvartan/plotr\nlibrary(pwrss)\nlibrary(quartor) # github.com/danielvartan/quartor\nlibrary(report)\nlibrary(rlang)\nlibrary(rutils) # github.com/danielvartan/rutils\nlibrary(see)\nlibrary(skedastic)\nlibrary(stats)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(workflows)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#loading-and-processing-the-data",
    "href": "qmd/supplementary-material-5.html#loading-and-processing-the-data",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.3 Loading and Processing the Data",
    "text": "E.3 Loading and Processing the Data\n\n\n\n\n\n\n\nAssumption 1\n\nPredictor is known. Either the vectors \\(z_{1}, \\dots , z_{n}\\) are known ahead of time, or they are the observed values of random vectors \\(Z_{1}, \\dots , Z_{n}\\) on whose values we condition before computing the joint distribution of (\\(Y_{1}, \\dots , Y_{n}\\)) (DeGroot & Schervish, 2012, p. 736).\n\n\n\n\n\nAssumption 1 is satisfied, as the predictors are known.\n\nCodetargets::tar_make(script = here::here(\"_targets.R\"))\n\n\nThis data processing is performed solely for the purpose of the analysis. The variables undergo numerical transformations to streamline the modeling process and minimize potential errors.\n\nCodedata &lt;-\n  targets::tar_read(\"weighted_data\", store = here::here(\"_targets\")) |&gt;\n  dplyr::select(\n    msf_sc, age, sex, longitude, ghi_month, ghi_annual,\n    march_equinox_daylight, june_solstice_daylight,\n    december_solstice_daylight, cell_weight\n  ) |&gt;\n  dplyr::mutate(\n    dplyr::across(\n      .cols = dplyr::ends_with(\"_daylight\"),\n      .fns = ~ .x |&gt; as.numeric()\n    )\n  ) |&gt;\n  dplyr::mutate(\n    msf_sc =\n      msf_sc |&gt;\n      lubritime:::link_to_timeline(threshold = hms::parse_hms(\"12:00:00\")) |&gt;\n      as.numeric()\n  ) |&gt;\n  tidyr::drop_na()\n\n\n\ndata |&gt;\n  dplyr::select(-cell_weight) |&gt;\n  report::report()\n#&gt; The data contains 65823 observations of the following 9 variables:\n#&gt; \n#&gt;   - msf_sc: n = 65823, Mean = 16120.91, SD = 5172.84, Median = 15685.71, MAD =\n#&gt; 5210.28, range: [1542.86, 30578.57], Skewness = 0.27, Kurtosis = -0.25, 0%\n#&gt; missing\n#&gt;   - age: n = 65823, Mean = 32.11, SD = 9.26, Median = 30.70, MAD = 9.42,\n#&gt; range: [18, 58.95], Skewness = 0.66, Kurtosis = -0.19, 0% missing\n#&gt;   - sex: 2 levels, namely Female (n = 43728, 66.43%) and Male (n = 22095,\n#&gt; 33.57%)\n#&gt;   - longitude: n = 65823, Mean = -45.81, SD = 4.05, Median = -46.59, MAD =\n#&gt; 3.87, range: [-57.11, -34.81], Skewness = 0.65, Kurtosis = 0.47, 0% missing\n#&gt;   - ghi_month: n = 65823, Mean = 5103.14, SD = 545.25, Median = 5050.00, MAD =\n#&gt; 593.04, range: [3508, 6699], Skewness = 0.03, Kurtosis = -0.04, 0% missing\n#&gt;   - ghi_annual: n = 65823, Mean = 4766.14, SD = 412.12, Median = 4726.00, MAD\n#&gt; = 453.68, range: [3639, 6061], Skewness = 0.29, Kurtosis = -0.35, 0% missing\n#&gt;   - march_equinox_daylight: n = 65823, Mean = 43607.03, SD = 9.15, Median =\n#&gt; 43609.35, MAD = 5.11, range: [43588.57, 43642.80], Skewness = -0.02,\n#&gt; Kurtosis = 0.41, 0% missing\n#&gt;   - june_solstice_daylight: n = 65823, Mean = 39058.59, SD = 1384.27, Median =\n#&gt; 38622.60, MAD = 549.10, range: [35740.28, 43710.78], Skewness = 1.31,\n#&gt; Kurtosis = 1.65, 0% missing\n#&gt;   - december_solstice_daylight: n = 65823, Mean = 48318.08, SD = 1420.94,\n#&gt; Median = 48762.49, MAD = 568.08, range: [43582.40, 51774.49], Skewness =\n#&gt; -1.28, Kurtosis = 1.59, 0% missing",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#conducting-a-power-analysis",
    "href": "qmd/supplementary-material-5.html#conducting-a-power-analysis",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.4 Conducting a Power Analysis",
    "text": "E.4 Conducting a Power Analysis\nThe results indicate that at least \\(1,895\\) observations per variable are required to achieve a power of \\(0.99\\) (\\(1 - \\beta\\)) (the probability of not committing a type II error) and a significance level (\\(\\alpha\\)) of \\(0.01\\) (\\(0.99\\) probability of not committing a type I error). The dataset contains \\(65,823\\) observations, which exceeds this requirement.\n\npwr_analysis &lt;- pwrss::pwrss.f.reg(\n  f2 = 0.02, # Minimal Effect Size (MES).\n  k = length(data) - 2, # Number of predictors (-msf_sc, -cell_weight).\n  power = 0.99,\n  alpha = 0.01\n)\n#&gt;  Linear Regression (F test) \n#&gt;  R-squared Deviation from 0 (zero) \n#&gt;  H0: r2 = 0 \n#&gt;  HA: r2 &gt; 0 \n#&gt;  ------------------------------ \n#&gt;   Statistical power = 0.99 \n#&gt;   n = 1895 \n#&gt;  ------------------------------ \n#&gt;  Numerator degrees of freedom = 8 \n#&gt;  Denominator degrees of freedom = 1885.64 \n#&gt;  Non-centrality parameter = 37.893 \n#&gt;  Type I error rate = 0.01 \n#&gt;  Type II error rate = 0.01\n\nCodepwrss::power.f.test(\n  ncp = pwr_analysis$ncp,\n  df1 = pwr_analysis$df1,\n  df2 = pwr_analysis$df2,\n  alpha = pwr_analysis$parms$alpha,\n  plot = TRUE\n)\n\n\nFigure E.1: Power analysis for the hypothesis test.\n\n\n\n\n\n\n\n\n#&gt;        power ncp.alt ncp.null alpha df1         df2      f.crit\n#&gt;  0.989999999  37.893        0  0.01   8 1885.639848 2.520678643\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#examining-distributions",
    "href": "qmd/supplementary-material-5.html#examining-distributions",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.5 Examining Distributions",
    "text": "E.5 Examining Distributions\n\n\nMSF~sc~ (Chronotype proxy) (seconds)\nAge (years)\nLongitude (decimal degrees)\nMonthly average global horizontal irradiance (Wh/m²)\nAnnual average global horizontal irradiance (Wh/m²)\nDaylight on the March equinox (seconds)\nDaylight on the June solstice (seconds)\nDaylight on the December solstice (seconds)\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"msf_sc\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.1: Statistics for the msf_sc variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"msf_sc\",\n    x_label = \"MSF~sc~ (Chronotype proxy) (seconds)\"\n  )\n\n\nFigure E.2: Histogram of the msf_sc variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"msf_sc\")\n\n\nFigure E.3: Box plot of the msf_sc variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"age\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.2: Statistics for the age variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"age\",\n    x_label = \"Age (years)\"\n  )\n\n\nFigure E.4: Histogram of the age variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"age\")\n\n\nFigure E.5: Box plot of the age variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"longitude\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.3: Statistics for the longitude variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"longitude\",\n    x_label = \"Longitude (decimal degrees)\"\n  )\n\n\nFigure E.6: Histogram of the longitude variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"longitude\")\n\n\nFigure E.7: Box plot of the longitude variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"ghi_month\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.4: Statistics for the ghi_month variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"ghi_month\",\n    x_label = \"Monthly average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nFigure E.8: Histogram of the ghi_month variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"ghi_month\")\n\n\nFigure E.9: Box plot of the ghi_month variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"ghi_annual\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.5: Statistics for the ghi_annual variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"ghi_annual\",\n    x_label = \"Annual average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nFigure E.10: Histogram of the ghi_annual variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"ghi_annual\")\n\n\nFigure E.11: Box plot of the ghi_annual variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"march_equinox_daylight\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.6: Statistics for the march_equinox_daylight variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"march_equinox_daylight\",\n    x_label = \"Daylight on the March equinox (seconds)\"\n  )\n\n\nFigure E.12: Histogram of the march_equinox_daylight variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"march_equinox_daylight\")\n\n\nFigure E.13: Box plot of the march_equinox_daylight variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"june_solstice_daylight\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.7: Statistics for the june_solstice_daylight variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"june_solstice_daylight\",\n    x_label = \"Daylight on the June solstice (seconds)\"\n  )\n\n\nFigure E.14: Histogram of the june_solstice_daylight variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"june_solstice_daylight\")\n\n\nFigure E.15: Box plot of the june_solstice_daylight variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"december_solstice_daylight\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable E.8: Statistics for the december_solstice_daylight variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"december_solstice_daylight\",\n    x_label = \"Daylight on the December solstice (seconds)\"\n  )\n\n\nFigure E.16: Histogram of the december_solstice_daylight variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"december_solstice_daylight\")\n\n\nFigure E.17: Box plot of the december_solstice_daylight variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#examining-correlations",
    "href": "qmd/supplementary-material-5.html#examining-correlations",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.6 Examining Correlations",
    "text": "E.6 Examining Correlations\nCodedata |&gt;\n  plotr:::plot_ggally(\n    cols = c(\n      \"msf_sc\",\n      \"age\",\n      \"sex\",\n      \"longitude\",\n      \"ghi_month\",\n      \"ghi_annual\"\n    ),\n    mapping = ggplot2::aes(colour = sex)\n  ) |&gt;\n  rutils::shush()\n\n\nFigure E.18: Correlation matrix of the main predictors.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#building-the-restricted-model",
    "href": "qmd/supplementary-material-5.html#building-the-restricted-model",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.7 Building the Restricted Model",
    "text": "E.7 Building the Restricted Model\n\nE.7.1 Fitting the Model\n\nform &lt;- formula(msf_sc ~ age + sex + longitude + ghi_month)\n\n\nmodel &lt;-\n  parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\") |&gt;\n  parsnip::set_mode(\"regression\")\n\n\nworkflow &lt;-\n  workflows::workflow() |&gt;\n  workflows::add_case_weights(cell_weight) |&gt;\n  workflows::add_formula(form) |&gt;\n  workflows::add_model(model)\n\nworkflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; msf_sc ~ age + sex + longitude + ghi_month\n#&gt; \n#&gt; ── Case Weights ─────────────────────────────────────────────────────────────\n#&gt; cell_weight\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nfit &lt;- workflow |&gt; parsnip::fit(data)\nfit_restricted &lt;- fit\n\nCodefit_coefs &lt;- fit |&gt; broom::tidy()\n\nfit_coefs |&gt; janitor::adorn_rounding(5)\n\n\nTable E.9: Output from the model fitting process showing the estimated coefficients, standard errors, test statistics, and p-values for the terms in the linear regression model.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_stats &lt;- fit |&gt; broom::glance()\n\nfit_stats |&gt;\n  tidyr::pivot_longer(cols = dplyr::everything()) |&gt;\n  janitor::adorn_rounding(10)\n\n\nTable E.10: Summary of model fit statistics showing key metrics including R-squared, adjusted R-squared, sigma, statistic, p-value, degrees of freedom, log-likelihood, AIC, BIC, and deviance.\n\n\n\n\n  \n\n\n\n\n\n\n\nfit_engine &lt;- fit |&gt; parsnip::extract_fit_engine()\n\nfit_engine |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -44246.421  -2705.443   -315.904   2430.031  52016.723 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate     Std. Error   t value   Pr(&gt;|t|)    \n#&gt; (Intercept) 19427.76043870   389.03209392  49.93871 &lt; 2.22e-16 ***\n#&gt; age          -124.87537286     1.72710634 -72.30323 &lt; 2.22e-16 ***\n#&gt; sexMale       358.48561682    39.07149140   9.17512 &lt; 2.22e-16 ***\n#&gt; longitude     -71.86988120     4.86850721 -14.76220 &lt; 2.22e-16 ***\n#&gt; ghi_month      -0.52019020     0.03995143 -13.02056 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4537.445 on 65818 degrees of freedom\n#&gt; Multiple R-squared:  0.08516525, Adjusted R-squared:  0.08510965 \n#&gt; F-statistic: 1531.808 on 4 and 65818 DF,  p-value: &lt; 2.2204e-16\n\n\nCode# A jerry-rigged solution to fix issues related to modeling using the pipe.\n\nfit_engine_2 &lt;- lm(form, data = data, weights = cell_weight)\nfit_engine_restricted &lt;- fit_engine_2\n\n\n\nreport::report(fit_engine_2)\n#&gt; We fitted a linear model (estimated using OLS) to predict msf_sc with age,\n#&gt; sex, longitude and ghi_month (formula: msf_sc ~ age + sex + longitude +\n#&gt; ghi_month). The model explains a statistically significant and weak\n#&gt; proportion of variance (R2 = 0.09, F(4, 65818) = 1531.81, p &lt; .001, adj. R2\n#&gt; = 0.09). The model's intercept, corresponding to age = 0, sex = Female,\n#&gt; longitude = 0 and ghi_month = 0, is at 19427.76 (95% CI [18665.26,\n#&gt; 20190.26], t(65818) = 49.94, p &lt; .001). Within this model:\n#&gt; \n#&gt;   - The effect of age is statistically significant and negative (beta =\n#&gt; -124.88, 95% CI [-128.26, -121.49], t(65818) = -72.30, p &lt; .001; Std. beta =\n#&gt; -0.27, 95% CI [-0.28, -0.26])\n#&gt;   - The effect of sex [Male] is statistically significant and positive (beta =\n#&gt; 358.49, 95% CI [281.91, 435.07], t(65818) = 9.18, p &lt; .001; Std. beta =\n#&gt; 0.07, 95% CI [0.05, 0.08])\n#&gt;   - The effect of longitude is statistically significant and negative (beta =\n#&gt; -71.87, 95% CI [-81.41, -62.33], t(65818) = -14.76, p &lt; .001; Std. beta =\n#&gt; -0.07, 95% CI [-0.08, -0.06])\n#&gt;   - The effect of ghi month is statistically significant and negative (beta =\n#&gt; -0.52, 95% CI [-0.60, -0.44], t(65818) = -13.02, p &lt; .001; Std. beta =\n#&gt; -0.06, 95% CI [-0.07, -0.05])\n#&gt; \n#&gt; Standardized parameters were obtained by fitting the model on a standardized\n#&gt; version of the dataset. 95% Confidence Intervals (CIs) and p-values were\n#&gt; computed using a Wald t-distribution approximation.\n\n\nE.7.2 Evaluating the Model Fit\n\nE.7.2.1 Predictions\nCodelimits &lt;-\n  stats::predict(fit_engine, interval = \"prediction\") |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils::shush()\n\nfit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::bind_cols(limits) |&gt;\n  ggplot2::ggplot(ggplot2::aes(msf_sc, .pred)) +\n  # ggplot2::geom_ribbon(\n  #   mapping = ggplot2::aes(ymin = lwr, ymax = upr),\n  #   alpha = 0.2\n  # ) +\n  ggplot2::geom_ribbon(\n    mapping = ggplot2::aes(\n      ymin = stats::predict(stats::loess(lwr ~ msf_sc)),\n      ymax = stats::predict(stats::loess(upr ~ msf_sc)),\n    ),\n    alpha = 0.2\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = lwr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = upr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_point() +\n  ggplot2::geom_abline(\n    intercept = 0,\n    slope = 1,\n    color = brandr::get_brand_color(\"orange\")\n  ) +\n  ggplot2::labs(\n    x = \"Observed\",\n    y = \"Predicted\"\n  )\n\n\nFigure E.19: Relation between observed and predicted values. The orange line is a 45-degree line originating from the plane’s origin and represents a perfect fit. The shaded area depicts a smoothed version of the 95% confidence of the prediction interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.2.2 Posterior Predictive Checks\nPosterior predictive checks are a Bayesian technique used to assess model fit by comparing observed data to data simulated from the posterior predictive distribution (i.e., the distribution of potential unobserved values given the observed data). These checks help identify systematic discrepancies between the observed and simulated data, providing insight into whether the chosen model (or distributional family) is appropriate. Ideally, the model-predicted lines should closely match the observed data patterns.\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\n      brandr::get_brand_color(\"orange\"),\n      brandr::get_brand_color(\"black\"),\n      \"black\"\n    )\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$PP_CHECK +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"MSFsc (Chronotype proxy) (s)\",\n  ) +\n  ggplot2::theme_get()\n\n\nFigure E.20: Posterior predictive checks for the model. The orange line represents the observed data, while the black lines represent the model-predicted data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3 Conducting Model Diagnostics\n\n\n\n\n\n\nIt’s important to note that objective assumption tests (e.g., Anderson–Darling test) is not advisable for larger samples, since they can be overly sensitive to minor deviations. Additionally, they might overlook visual patterns that are not captured by a single metric (Kozak & Piepho, 2018; Schucany & Ng, 2006; Shatz, 2024).\nI included those tests here just for reference. However, for the reason above, all assumptions were diagnosed by visual assessment.\nFor a straightforward critique of normality tests specifically, refer to this article by Greener (2020).\n\n\n\n\nE.7.3.1 Normality\n\n\n\n\n\n\n\nAssumption 2\n\nNormality. For \\(i = 1, \\dots, n\\), the conditional distribution of \\(Y_{i}\\) given the vectors \\(z_{1}, \\dots , z_{n}\\) is a normal distribution (DeGroot & Schervish, 2012, p. 737).\n\n\n(Normality of the error term distribution (Hair, 2019, p. 287))\n\n\n\nAssumption 2 is satisfied, as the residuals shown a fairly normal distribution by visual inspection.\n\nE.7.3.1.1 Visual Inspection\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::test_normality(col = \"value\", name = \"Residuals\")\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\nFigure E.21: Histogram of the model residuals with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the residuals and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::select(.resid) |&gt;\n  tidyr::pivot_longer(.resid) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +\n  ggplot2::geom_boxplot(\n    outlier.colour = brandr::get_brand_color(\"orange\"),\n    outlier.shape = 1,\n    width = 0.5\n  ) +\n  ggplot2::labs(x = \"Variable\", y = \"Value\") +\n  ggplot2::coord_flip() +\n  ggplot2::theme(\n    axis.title.y = ggplot2::element_blank(),\n    axis.text.y = ggplot2::element_blank(),\n    axis.ticks.y = ggplot2::element_blank()\n  )\n\n\nFigure E.22: Boxplot of model residuals with outliers highlighted in orange.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::stats_summary(col = \"value\", name = \"Residuals\")\n\n\nTable E.11: Summary statistics of model residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.7.3.1.2 Tests\nIt’s important to note that the Kolmogorov-Smirnov and Pearson chi-square tests are included here just for reference, as many authors don’t recommend using them when testing for normality (D’Agostino & Belanger, 1990). Learn more about normality tests in Thode (2002).\nI also recommend checking the original papers for each test to understand their assumptions and limitations:\n\n\nAnderson-Darling test: Anderson & Darling (1952); Anderson & Darling (1954).\nBonett-Seier test: Bonett & Seier (2002).\n\nCramér-von Mises test: Cramér (1928); Anderson (1962).\n\nD’Agostino test: D’Agostino (1971); D’Agostino & Pearson (1973).\n\nJarque–Bera test: Jarque & Bera (1980); Bera & Jarque (1981); Jarque & Bera (1987).\n\nLilliefors (K-S) test: Smirnov (1948); Kolmogorov (1933); Massey (1951); Lilliefors (1967); Dallal & Wilkinson (1986).\n\nPearson chi-square test: Pearson (1900).\n\nShapiro-Francia test: Shapiro & Francia (1972).\n\nShapiro-Wilk test: Shapiro & Wilk (1965).\n\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The data is normally distributed} \\\\\n\\text{H}_{a}: \\text{The data is not normally distributed}\n\\end{cases}\n\\]\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::normality_summary(col = \"value\")\n\n\nTable E.12: Summary of statistical tests conducted to assess the normality of the residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.7.3.2 Linearity\n\n\n\n\n\n\n\nAssumption 3\n\nLinear mean. There is a vector of parameters \\(\\beta = (\\beta_{0}, \\dots, \\beta_{p - 1})\\) such that the conditional mean of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) has the form\n\n\n\\[\nz_{i0} \\beta_{0} + z_{i1} \\beta_{1} + \\cdots + z_{ip - 1} \\beta_{p - 1}\n\\]\nfor \\(i = 1, \\dots, n\\) (DeGroot & Schervish, 2012, p. 737).\n(Linearity of the phenomenon measured (Hair, 2019, p. 287))\n\n\n\nAssumption 3 is satisfied, as the relationship between the variables is fairly linear. As shown in Chapter 4, the hypothesis implies linearity and the distribution of the residuals also support this.\nCodeplot &lt;-\n  fit |&gt;\n  broom::augment(data) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_hline(\n    yintercept = 0,\n    color = \"black\",\n    linewidth = 0.5,\n    linetype = \"dashed\"\n  ) +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(x = \"Fitted values\", y = \"Residuals\")\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure E.23: Residual plot showing the relationship between fitted values and residuals. The dashed black line represent zero residuals, indicating an ideal model fit. The orange line indicate the conditional mean of residuals.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots &lt;- fit_engine |&gt; olsrr::ols_plot_resid_fit_spread(print_plot = FALSE)\n\nfor (i in seq_along(plots)) {\n  q &lt;- plots[[i]] + ggplot2::labs(title = ggplot2::element_blank())\n\n  q &lt;- q |&gt; ggplot2::ggplot_build()\n  q$data[[1]]$colour &lt;- brandr::get_brand_color(\"orange\")\n  q$plot$layers[[1]]$constructor$color &lt;- brandr::get_brand_color(\"orange\")\n\n  plots[[i]] &lt;- q |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n}\n\npatchwork::wrap_plots(plots$fm_plot, plots$rsd_plot, ncol = 2)\n\n\nFigure E.24: Residual fit spread plots to detect non-linearity, influential observations, and outliers. The side-by-side plots show the centered fit and residuals, illustrating the variation explained by the model and what remains in the residuals. Inappropriately specified models often exhibit greater spread in the residuals than in the centered fit. “Proportion Less” indicates the cumulative distribution function, representing the proportion of observations below a specific value, facilitating an assessment of model performance.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ramsey’s RESET test indicates that the model has no omitted variables. This test examines whether non-linear combinations of the fitted values can explain the response variable.\nLearn more about the Ramsey’s RESET test in: Ramsey (1969).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The model has no omitted variables} \\\\\n\\text{H}_{a}: \\text{The model has omitted variables}\n\\end{cases}\n\\]\n\nfit_engine |&gt; lmtest::resettest(power = 2:3)\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 154.57861, df1 = 2, df2 = 65816, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::resettest(type = \"regressor\")\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 48.097756, df1 = 10, df2 = 65808, p-value &lt; 2.2204e-16\n\n\nE.7.3.3 Homoscedasticity (Common Variance)\n\n\n\n\n\n\n\nAssumption 4\n\nCommon variance (homoscedasticity). There is as parameter \\(\\sigma^{2}\\) such the conditional variance of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) is \\(\\sigma^{2}\\) for \\(i = 1, \\dots, n\\).\n\n\n(Constant variance of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 4 is satisfied. When comparing the standardized residuals (\\(\\sqrt{|\\text{Standardized Residuals}|}\\)) spread to the fitted values, we can observe that the residuals are fairly constant across the range of values. This suggests that the residuals have a constant variance.\n\nE.7.3.3.1 Visual Inspection\nCodeplot &lt;-\n  fit |&gt;\n  stats::predict(data) |&gt;\n  dplyr::mutate(\n    .sd_resid =\n      fit_engine |&gt;\n      stats::rstandard() |&gt;\n      abs() |&gt;\n      sqrt()\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .sd_resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(\n    x = \"Fitted values\",\n    y = latex2exp::TeX(\"$\\\\sqrt{|Standardized \\\\ Residuals|}$\")\n  )\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure E.25: Relation between the fitted values of the model and its standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSFsc (Chronotype proxy) (seconds)\nAge (years)\nLongitude (decimal degrees)\nMonthly average global horizontal irradiance (Wh/m²)\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"msf_sc\",\n    x_label = \"MSF~sc~  (Chronotype proxy) (seconds)\"\n  )\n\n\nTable E.13: Relation between msf_sc and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"age\",\n    x_label = \"Age (years)\"\n  )\n\n\nTable E.14: Relation between age and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"longitude\",\n    x_label = \"Longitude (decimal degrees)\"\n  )\n\n\nTable E.15: Relation between longitude and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"ghi_month\",\n    x_label = \"Monthly average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nTable E.16: Relation between ghi_month and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.3.2 Breusch-Pagan Test\nThe Breusch-Pagan test test indicates that the residuals exhibit constant variance.\nLearn more about the Breusch-Pagan test in: Breusch & Pagan (1979) and Koenker (1981).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The variance is constant} \\\\\n\\text{H}_{a}: \\text{The variance is not constant}\n\\end{cases}\n\\]\n\n# With studentising modification of Koenker\nfit_engine |&gt; lmtest::bptest(studentize = TRUE)\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1123.2579, df = 4, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::bptest(studentize = FALSE)\n#&gt; \n#&gt;  Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1545.0951, df = 4, p-value &lt; 2.2204e-16\n\n\nCode# Using the studentized modification of Koenker.\nfit_engine |&gt; skedastic::breusch_pagan(koenker = TRUE)\n\n\n  \n\n\n\n\nCodefit_engine |&gt; skedastic::breusch_pagan(koenker = FALSE)\n\n\n  \n\n\n\n\nfit_engine |&gt; car::ncvTest()\n#&gt; Non-constant Variance Score Test \n#&gt; Variance formula: ~ fitted.values \n#&gt; Chisquare = 8204.68981, Df = 1, p = &lt; 2.22045e-16\n\n\nfit_engine_2 |&gt; olsrr::ols_test_breusch_pagan()\n#&gt; \n#&gt;  Breusch Pagan Test for Heteroskedasticity\n#&gt;  -----------------------------------------\n#&gt;  Ho: the variance is constant            \n#&gt;  Ha: the variance is not constant        \n#&gt; \n#&gt;                Data                \n#&gt;  ----------------------------------\n#&gt;  Response : msf_sc \n#&gt;  Variables: fitted values of msf_sc \n#&gt; \n#&gt;           Test Summary            \n#&gt;  ---------------------------------\n#&gt;  DF            =    1 \n#&gt;  Chi2          =    78.53154173 \n#&gt;  Prob &gt; Chi2   =    7.87310582e-19\n\n\nE.7.3.4 Independence\n\n\n\n\n\n\n\nAssumption 5\n\nIndependence. The random variables \\(Y_{1}, \\dots , Y_{n}\\) are independent given the observed \\(z_{1}, \\dots , z_{n}\\) (DeGroot & Schervish, 2012, p. 737).\n\n\n(Independence of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 5 is satisfied. Although the residuals show some autocorrelation, they fall within the acceptable range of the Durbin–Watson statistic (\\(1.5\\) to \\(2.5\\)). It’s also important to note that the observations for each predicted value are not related to any other prediction; in other words, they are not grouped or sequenced by any variable (by design) (see Hair (2019, p. 291) for more information).\nMany authors don’t consider autocorrelation tests for linear regression models, as they are more relevant for time series data. They were include here just for reference.\n\nE.7.3.4.1 Visual Inspection\nCodefit_engine |&gt;\n  residuals() |&gt;\n  forecast::ggtsdisplay(\n    lag.max = 30,\n    theme = ggplot2::theme_get()\n  )\n\n\nFigure E.26: Time series plot of the residuals along with its AutoCorrelation Function (ACF) and Partial AutoCorrelation Function (PACF).\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.4.2 Correlations\nTable E.17 shows the relative importance of independent variables in determining the response variable. It highlights how much each variable uniquely contributes to the R-squared value, beyond what is explained by the other predictors.\nCodefit_engine |&gt; olsrr::ols_correlations()\n\n\nTable E.17: Correlations between the dependent variable and the independent variables, along with the zero-order, part, and partial correlations. The zero-order correlation represents the Pearson correlation coefficient between the dependent and independent variables. Part correlations indicate how much the R-squared would decrease if a specific variable were removed from the model, while partial correlations reflect the portion of variance in the response variable that is explained by a specific independent variable, beyond the influence of other predictors in the model.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.7.3.4.3 Newey-West Estimator\nThe Newey-West estimator is a method used to estimate the covariance matrix of the coefficients in a regression model when the residuals are autocorrelated.\nLearn more about the Newey-West estimator in: Newey & West (1987) and Newey & West (1994).\n\nfit_engine |&gt; sandwich::NeweyWest()\n#&gt;                 (Intercept)              age          sexMale\n#&gt; (Intercept) 386230.75410310 -627.66593784297 -3949.0613848769\n#&gt; age           -627.66593784   11.37622243485    10.7984441977\n#&gt; sexMale      -3949.06138488   10.79844419774  3917.6442174967\n#&gt; longitude     4052.02814916    1.93511690642    -3.1560992922\n#&gt; ghi_month      -34.32905542    0.06796834477     0.4120212059\n#&gt;                   longitude       ghi_month\n#&gt; (Intercept) 4052.0281491632 -34.32905541605\n#&gt; age            1.9351169064   0.06796834477\n#&gt; sexMale       -3.1560992922   0.41202120594\n#&gt; longitude     64.4731661575  -0.22379900720\n#&gt; ghi_month     -0.2237990072   0.00420877298\n\n\nE.7.3.4.4 Durbin-Watson Test\nThe Durbin-Watson test is a statistical test used to detect the presence of autocorrelation at lag \\(1\\) in the residuals from a regression analysis. The test statistic ranges from \\(0\\) to \\(4\\), with a value of \\(2\\) indicating no autocorrelation. Values less than \\(2\\) indicate positive autocorrelation, while values greater than \\(2\\) indicate negative autocorrelation (Fox, 2016).\nA common rule of thumb in the statistical community is that a Durbin-Watson statistic between \\(1.5\\) and \\(2.5\\) suggests little to no autocorrelation.\nLearn more about the Durbin-Watson test in: Durbin & Watson (1950); Durbin & Watson (1951); and Durbin & Watson (1971).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Autocorrelation of the disturbances is 0} \\\\\n\\text{H}_{a}: \\text{Autocorrelation of the disturbances is not equal to 0}\n\\end{cases}\n\\]\n\ncar::durbinWatsonTest(fit_engine)\n#&gt;  lag Autocorrelation D-W Statistic p-value\n#&gt;    1   0.03604242224   1.927892131       0\n#&gt;  Alternative hypothesis: rho != 0\n\n\nE.7.3.4.5 Ljung-Box Test\nThe Ljung–Box test is a statistical test used to determine whether any autocorrelations within a time series are significantly different from zero. Rather than testing randomness at individual lags, it assesses the “overall” randomness across multiple lags.\nLearn more about the Ljung-Box test in: Box & Pierce (1970) and Ljung & Box (1978).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Residuals are independently distributed} \\\\\n\\text{H}_{a}: \\text{Residuals are not independently distributed}\n\\end{cases}\n\\]\n\nfit_engine |&gt;\n  stats::residuals() |&gt;\n  stats::Box.test(type = \"Ljung-Box\", lag = 10)\n#&gt; \n#&gt;  Box-Ljung test\n#&gt; \n#&gt; data:  stats::residuals(fit_engine)\n#&gt; X-squared = 1078.3505, df = 10, p-value &lt; 2.2204e-16\n\n\nE.7.3.5 Colinearity/Multicollinearity\nNo high degree of colinearity was observed among the independent variables.\n\nE.7.3.5.1 Variance Inflation Factor (VIF)\nThe Variance Inflation Factor (VIF) indicates the effect of other independent variables on the standard error of a regression coefficient. The VIF is directly related to the tolerance value (\\(\\text{VIF}_{i} = 1/\\text{TO}L\\)). High VIF values (larger than ~5 (Struck, 2024)) suggest significant collinearity or multicollinearity among the independent variables (Hair, 2019, p. 265).\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(panel = FALSE) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$VIF +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank()\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    axis.text = ggplot2::element_text(colour = \"black\"),\n    axis.text.y = ggplot2::element_text(size = 9),\n    legend.text = ggplot2::element_text(colour = \"black\")\n  )\n\n\nFigure E.27: Variance Inflation Factors (VIF) for each predictor variable. VIFs below 5 are considered acceptable. Between 5 and 10, the variable should be examined. Above 10, the variable must considered highly collinear.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt; olsrr::ols_vif_tol()\n\n\nTable E.18: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_engine_2 |&gt; performance::check_collinearity()\n\n\nTable E.19: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.7.3.5.2 Condition Index\nThe condition index is a measure of multicollinearity in a regression model. It is based on the eigenvalues of the correlation matrix of the predictors. A condition index of 30 or higher is generally considered indicative of significant collinearity (Belsley et al., 2004, pp. 112–114).\nCodefit_engine |&gt; olsrr::ols_eigen_cindex()\n\n\nTable E.20: Condition indexes and eigenvalues for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.7.3.6 Measures of Influence\nIn this section, I check several measures of influence that can be used to assess the impact of individual observations on the model estimates.\n\n\n\n\n\n\n\nLeverage points\n\nLeverage is a measure of the distance between individual values of a predictor and other values of the predictor. In other words, a point with high leverage has an x-value far away from the other x-values. Points with high leverage have the potential to influence the model estimates (Hair, 2019, p. 262; Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\n\n\n\n\n\n\nInfluence points\n\nInfluence is a measure of how much an observation affects the model estimates. If an observation with large influence were removed from the dataset, we would expect a large change in the predictive equation (Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\nE.7.3.6.1 Standardized Residuals\nStandardized residuals are a rescaling of the residual to a common basis by dividing each residual by the standard deviation of the residuals (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstandard() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.16375203728  1.90741700801 -0.05283727038  0.15325458085  0.89675594708 \n#&gt;              6 \n#&gt;  0.31486218404\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstandard(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Standardized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure E.28: Standardized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.6.2 Studentized Residuals\nStudentized residuals are a commonly used variant of the standardized residual. It differs from other methods in how it calculates the standard deviation used in standardization. To minimize the effect of any observation on the standardization process, the standard deviation of the residual for observation \\(i\\) is computed from regression estimates omitting the \\(i\\)th observation in the calculation of the regression estimates (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstudent() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.16375516976  1.90745523807 -0.05283687011  0.15325344396  0.89675461300 \n#&gt;              6 \n#&gt;  0.31486002925\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstudent(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Studentized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure E.29: Studentized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::mutate(\n    std = stats::rstudent(fit_engine)\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, std)) +\n  ggplot2::geom_point(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Predicted value\",\n    y = \"Studentized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure E.30: Relation between studentized residuals and fitted values.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_lev(threshold = 2, print_plot = FALSE)\n\nplot$plot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Studentized residual\"\n  )\n\n\nFigure E.31: Relation between studentized residuals and their leverage points.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.6.3 Hat Values\nThe hat value indicates how distinct an observation’s predictor values are from those of other observations. Observations with high hat values have high leverage and may be, though not necessarily, influential. There is no fixed threshold for what constitutes a “large” hat value; instead, the focus must be on observations with hat values significantly higher than the rest (Hair, 2019, p. 261; Nahhas, 2024).\n\nfit_engine |&gt; stats::hatvalues() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt; 0.00010146471316 0.00005268015355 0.00001872310039 0.00028115925295 \n#&gt;                5                6 \n#&gt; 0.00009202990156 0.00009341669996\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  hat = stats::hatvalues(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = hat, ymin = 0, ymax = hat)\n  ) +\n  ggplot2::geom_linerange(color = brandr::get_brand_color(\"black\")) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Hat value\"\n  )\n\n\nFigure E.32: Hat values for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.6.4 Cook’s Distance\nThe Cook’s D measures each observation’s influence on the model’s fitted values. It is considered one of the most representative metrics for assessing overall influence (Hair, 2019).\nA common practice is to flag observations with a Cook’s distance of 1.0 or greater. However, a threshold of \\(4 / (n - k - 1)\\), where \\(n\\) is the sample size and \\(k\\) is the number of independent variables, is suggested as a more conservative measure in small samples or for use with larger datasets (Hair, 2019).\nLearn more about Cook’s D in: Cook (1977); Cook (1979).\n\nfit_engine |&gt; stats::cooks.distance() |&gt; head()\n#&gt;                   1                   2                   3 \n#&gt; 0.00002748590265018 0.00003833462407715 0.00000001045434047 \n#&gt;                   4                   5                   6 \n#&gt; 0.00000132108702976 0.00001480292211365 0.00000185240564778\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_cooksd_bar(type = 2, print_plot = FALSE)\n\n# The following procedure changes the plot aesthetics.\nq &lt;- plot$plot + ggplot2::labs(title = ggplot2::element_blank())\nq &lt;- q |&gt; ggplot2::ggplot_build()\nq$data[[5]]$label &lt;- \"\"\n\nq |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure E.33: Cook’s distance for each observation along with a threshold line at \\(4 / (n - k - 1)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\"blue\", \"black\", \"black\")\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\nplot &lt;-\n  diag_sum_plots$OUTLIERS +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"Leverage\",\n    y = \"Studentized residuals\"\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    # axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    # axis.text = ggplot2::element_text(colour = \"gray25\"),\n    axis.text.y = ggplot2::element_text(size = 9)\n  )\n\nplot &lt;- plot |&gt; ggplot2::ggplot_build()\n\n# The following procedure changes the plot aesthetics.\nfor (i in c(1:9)) {\n  # \"#1b6ca8\" \"#3aaf85\"\n  plot$data[[i]]$colour &lt;- dplyr::case_when(\n    plot$data[[i]]$colour == \"blue\" ~\n      ifelse(i == 4, brandr::get_brand_color(\"grey\"), brandr::get_brand_color(\"orange\")),\n    plot$data[[i]]$colour == \"#1b6ca8\" ~ \"black\",\n    plot$data[[i]]$colour == \"darkgray\" ~ \"black\",\n    TRUE ~ plot$data[[i]]$colour\n  )\n}\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\nplot |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure E.34: Relation between studentized residuals and their leverage points. The orange line represents the Cook’s distance. Any points outside the contour lines are influential observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.6.5 Influence on Prediction (DFFITS)\nDFFITS (difference in fits) is a standardized measure of how much the prediction for a given observation would change if it were deleted from the model. Each observation’s DFFITS is standardized by the standard deviation of fit at that point (Struck, 2024).\nThe best rule of thumb is to classify as influential any standardized values that exceed \\(2 \\sqrt{(p / n)}\\), where \\(p\\) is the number of independent variables + 1 and \\(n\\) is the sample size (Hair, 2019, p. 261).\nLearn more about DDFITS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dffits() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt;  0.0117230650042  0.0138448836690 -0.0002286283237  0.0025700850466 \n#&gt;                5                6 \n#&gt;  0.0086031616484  0.0030433372255\n\nCodeplot &lt;- fit_engine |&gt;\n  olsrr::ols_plot_dffits(print_plot = FALSE)\n\nplot$plot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure E.35: Standardized DFFITS (difference in fits) for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.6.6 Influence on Parameter Estimates (DFBETAS)\nDFBETAS are a measure of the change in a regression coefficient when an observation is omitted from the regression analysis. The value of the DFBETA is in terms of the coefficient itself (Hair, 2019, p. 261). A cutoff for what is considered a large DFBETAS value is \\(2 / \\sqrt{n}\\), where \\(n\\) is the number of observations. (Struck, 2024).\nLearn more about DFBETAS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dfbeta() |&gt; head()\n#&gt;      (Intercept)              age       sexMale         longitude\n#&gt; 1 -2.03640720959 -0.0103549562328 0.16604079671 -0.03782400905068\n#&gt; 2  1.78097597074 -0.0086526416297 0.24348797412 -0.00271444025137\n#&gt; 3 -0.03682022608  0.0002125757538 0.00456010503 -0.00008384067498\n#&gt; 4  0.43147642077 -0.0023850543494 0.03450072846  0.00725970026893\n#&gt; 5 -0.31752143263 -0.0083607753914 0.14169036321 -0.01730146267547\n#&gt; 6 -0.60088074723 -0.0016766287238 0.04956283032 -0.01062135319232\n#&gt;            ghi_month\n#&gt; 1  0.000138033471483\n#&gt; 2 -0.000299437545344\n#&gt; 3  0.000003930534343\n#&gt; 4 -0.000003628288665\n#&gt; 5 -0.000027700852353\n#&gt; 6  0.000035653094081\n\n\n\nIntercept\nage\nsexMale\nlongitude\nghi_month\n\n\n\nCodeplots$plots[[1]] +\nggplot2::labs(title = \"Intercept coefficient\")\n\n\nTable E.21: Standardized DFBETAS values for each observation concerning the Intercept coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[2]] +\nggplot2::labs(title = \"age coefficient\")\n\n\nTable E.22: Standardized DFBETAS values for each observation concerning the age coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[3]] +\nggplot2::labs(title = \"sexMale coefficient\")\n\n\nTable E.23: Standardized DFBETAS values for each observation concerning the sexMale coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[4]] +\nggplot2::labs(title = \"longitude coefficient\")\n\n\nTable E.24: Standardized DFBETAS values for each observation concerning the longitude coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[5]] +\nggplot2::labs(title = \"ghi_month coefficient\")\n\n\nTable E.25: Standardized DFBETAS values for each observation concerning the ghi_month coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.7.3.6.7 Hadi’s Measure\nHadi’s measure of influence is based on the idea that influential observations can occur in either the response variable, the predictors, or both.\nLearn more about Hadi’s measure in: Chatterjee & Hadi (2012).\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_hadi(print_plot = FALSE)\n\nplot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Hadi's measure\"\n  )\n\n\nFigure E.36: Hadi’s influence measure for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_pot(print_plot = FALSE)\n\nplot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure E.37: Potential-residual plot classifying unusual observations as high-leverage points, outliers, or a combination of both.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#building-the-full-model",
    "href": "qmd/supplementary-material-5.html#building-the-full-model",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.8 Building the Full Model",
    "text": "E.8 Building the Full Model\n\nE.8.1 Fitting the Model\n\n\nform &lt;- as.formula(\n    paste0(\n      \"msf_sc ~ \",\n      paste0(\n        c(\n          \"age\", \"sex\", \"longitude\", \"ghi_month\",\n          \"ghi_annual\", \"march_equinox_daylight\", \"june_solstice_daylight\",\n          \"december_solstice_daylight\"\n        ),\n        collapse = \" + \"\n      )\n    )\n)\n\n\nlm(form, data = data, weights = cell_weight)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = form, data = data, weights = cell_weight)\n#&gt; \n#&gt; Coefficients:\n#&gt;                (Intercept)                         age  \n#&gt;           -9295025.7140650                -125.9917660  \n#&gt;                    sexMale                   longitude  \n#&gt;                358.6047941                 -86.3597975  \n#&gt;                  ghi_month                  ghi_annual  \n#&gt;                 -0.3505484                   0.6116185  \n#&gt;     march_equinox_daylight      june_solstice_daylight  \n#&gt;                471.3219671                -129.0403016  \n#&gt; december_solstice_daylight  \n#&gt;               -128.3733647\n\n\nmodel &lt;-\n  parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\") |&gt;\n  parsnip::set_mode(\"regression\")\n\n\nworkflow &lt;-\n  workflows::workflow() |&gt;\n  workflows::add_case_weights(cell_weight) |&gt;\n  workflows::add_formula(form) |&gt;\n  workflows::add_model(model)\n\nworkflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; msf_sc ~ age + sex + longitude + ghi_month + ghi_annual + march_equinox_daylight + \n#&gt;     june_solstice_daylight + december_solstice_daylight\n#&gt; \n#&gt; ── Case Weights ─────────────────────────────────────────────────────────────\n#&gt; cell_weight\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nfit &lt;- workflow |&gt; parsnip::fit(data)\nfit_full &lt;- fit\n\nCodefit_coefs &lt;- fit |&gt; broom::tidy()\n\nfit_coefs |&gt; janitor::adorn_rounding(5)\n\n\nTable E.26: Output from the model fitting process showing the estimated coefficients, standard errors, test statistics, and p-values for the terms in the linear regression model.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_stats &lt;- fit |&gt; broom::glance()\n\nfit_stats |&gt;\n  tidyr::pivot_longer(cols = dplyr::everything()) |&gt;\n  janitor::adorn_rounding(10)\n\n\nTable E.27: Summary of model fit statistics showing key metrics including R-squared, adjusted R-squared, sigma, statistic, p-value, degrees of freedom, log-likelihood, AIC, BIC, and deviance.\n\n\n\n\n  \n\n\n\n\n\n\n\nfit_engine &lt;- fit |&gt; parsnip::extract_fit_engine()\n\nfit_engine |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -45208.614  -2714.196   -330.379   2412.763  52268.869 \n#&gt; \n#&gt; Coefficients:\n#&gt;                                    Estimate       Std. Error   t value\n#&gt; (Intercept)                -9295025.7140650  1046596.2288908  -8.88120\n#&gt; age                            -125.9917660        1.7267070 -72.96650\n#&gt; sexMale                         358.6047941       39.0128010   9.19198\n#&gt; longitude                       -86.3597975        8.5831193 -10.06159\n#&gt; ghi_month                        -0.3505484        0.1759076  -1.99280\n#&gt; ghi_annual                        0.6116185        0.2734072   2.23702\n#&gt; march_equinox_daylight          471.3219671       64.7780034   7.27596\n#&gt; june_solstice_daylight         -129.0403016       20.8131938  -6.19993\n#&gt; december_solstice_daylight     -128.3733647       20.6684973  -6.21106\n#&gt;                                       Pr(&gt;|t|)    \n#&gt; (Intercept)                         &lt; 2.22e-16 ***\n#&gt; age                                 &lt; 2.22e-16 ***\n#&gt; sexMale                             &lt; 2.22e-16 ***\n#&gt; longitude                           &lt; 2.22e-16 ***\n#&gt; ghi_month                             0.046288 *  \n#&gt; ghi_annual                            0.025288 *  \n#&gt; march_equinox_daylight     0.00000000000034779 ***\n#&gt; june_solstice_daylight     0.00000000056823247 ***\n#&gt; december_solstice_daylight 0.00000000052940524 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4530.47 on 65814 degrees of freedom\n#&gt; Multiple R-squared:  0.08803137, Adjusted R-squared:  0.08792052 \n#&gt; F-statistic: 794.1195 on 8 and 65814 DF,  p-value: &lt; 2.2204e-16\n\n\nCode# A jerry-rigged solution to fix issues related to modeling using the pipe.\n\nfit_engine_2 &lt;- lm(form, data = data, weights = cell_weight)\nfit_engine_full &lt;- fit_engine_2\n\n\n\nreport::report(fit_engine_2)\n#&gt; We fitted a linear model (estimated using OLS) to predict msf_sc with age,\n#&gt; sex, longitude, ghi_month, ghi_annual, march_equinox_daylight,\n#&gt; june_solstice_daylight and december_solstice_daylight (formula: msf_sc ~ age\n#&gt; + sex + longitude + ghi_month + ghi_annual + march_equinox_daylight +\n#&gt; june_solstice_daylight + december_solstice_daylight). The model explains a\n#&gt; statistically significant and weak proportion of variance (R2 = 0.09, F(8,\n#&gt; 65814) = 794.12, p &lt; .001, adj. R2 = 0.09). The model's intercept,\n#&gt; corresponding to age = 0, sex = Female, longitude = 0, ghi_month = 0,\n#&gt; ghi_annual = 0, march_equinox_daylight = 0, june_solstice_daylight = 0 and\n#&gt; december_solstice_daylight = 0, is at -9.30e+06 (95% CI [-1.13e+07,\n#&gt; -7.24e+06], t(65814) = -8.88, p &lt; .001). Within this model:\n#&gt; \n#&gt;   - The effect of age is statistically significant and negative (beta =\n#&gt; -125.99, 95% CI [-129.38, -122.61], t(65814) = -72.97, p &lt; .001; Std. beta =\n#&gt; -0.27, 95% CI [-0.28, -0.27])\n#&gt;   - The effect of sex [Male] is statistically significant and positive (beta =\n#&gt; 358.60, 95% CI [282.14, 435.07], t(65814) = 9.19, p &lt; .001; Std. beta =\n#&gt; 0.07, 95% CI [0.05, 0.08])\n#&gt;   - The effect of longitude is statistically significant and negative (beta =\n#&gt; -86.36, 95% CI [-103.18, -69.54], t(65814) = -10.06, p &lt; .001; Std. beta =\n#&gt; -0.08, 95% CI [-0.10, -0.06])\n#&gt;   - The effect of ghi month is statistically significant and negative (beta =\n#&gt; -0.35, 95% CI [-0.70, -5.77e-03], t(65814) = -1.99, p = 0.046; Std. beta =\n#&gt; -0.04, 95% CI [-0.08, -6.57e-04])\n#&gt;   - The effect of ghi annual is statistically significant and positive (beta =\n#&gt; 0.61, 95% CI [0.08, 1.15], t(65814) = 2.24, p = 0.025; Std. beta = 0.05, 95%\n#&gt; CI [6.57e-03, 0.10])\n#&gt;   - The effect of march equinox daylight is statistically significant and\n#&gt; positive (beta = 471.32, 95% CI [344.36, 598.29], t(65814) = 7.28, p &lt; .001;\n#&gt; Std. beta = 1.00, 95% CI [0.73, 1.26])\n#&gt;   - The effect of june solstice daylight is statistically significant and\n#&gt; negative (beta = -129.04, 95% CI [-169.83, -88.25], t(65814) = -6.20, p &lt;\n#&gt; .001; Std. beta = -45.89, 95% CI [-60.40, -31.39])\n#&gt;   - The effect of december solstice daylight is statistically significant and\n#&gt; negative (beta = -128.37, 95% CI [-168.88, -87.86], t(65814) = -6.21, p &lt;\n#&gt; .001; Std. beta = -46.78, 95% CI [-61.54, -32.02])\n#&gt; \n#&gt; Standardized parameters were obtained by fitting the model on a standardized\n#&gt; version of the dataset. 95% Confidence Intervals (CIs) and p-values were\n#&gt; computed using a Wald t-distribution approximation.\n\n\nE.8.2 Evaluating the Model Fit\n\nE.8.2.1 Predictions\nCodelimits &lt;-\n  stats::predict(fit_engine, interval = \"prediction\") |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils::shush()\n\nfit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::bind_cols(limits) |&gt;\n  ggplot2::ggplot(ggplot2::aes(msf_sc, .pred)) +\n  # ggplot2::geom_ribbon(\n  #   mapping = ggplot2::aes(ymin = lwr, ymax = upr),\n  #   alpha = 0.2\n  # ) +\n  ggplot2::geom_ribbon(\n    mapping = ggplot2::aes(\n      ymin = stats::predict(stats::loess(lwr ~ msf_sc)),\n      ymax = stats::predict(stats::loess(upr ~ msf_sc)),\n    ),\n    alpha = 0.2\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = lwr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = upr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_point() +\n  ggplot2::geom_abline(\n    intercept = 0,\n    slope = 1,\n    color = brandr::get_brand_color(\"orange\")\n  ) +\n  ggplot2::labs(\n    x = \"Observed\",\n    y = \"Predicted\"\n  )\n\n\nFigure E.38: Relation between observed and predicted values. The orange line is a 45-degree line originating from the plane’s origin and represents a perfect fit. The shaded area depicts a smoothed version of the 95% confidence of the prediction interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.2.2 Posterior Predictive Checks\nPosterior predictive checks are a Bayesian technique used to assess model fit by comparing observed data to data simulated from the posterior predictive distribution (i.e., the distribution of potential unobserved values given the observed data). These checks help identify systematic discrepancies between the observed and simulated data, providing insight into whether the chosen model (or distributional family) is appropriate. Ideally, the model-predicted lines should closely match the observed data patterns.\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\n      brandr::get_brand_color(\"orange\"),\n      brandr::get_brand_color(\"black\"),\n      \"black\"\n    )\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$PP_CHECK +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"MSFsc (Chronotype proxy) (s)\",\n  ) +\n  ggplot2::theme_get()\n\n\nFigure E.39: Posterior predictive checks for the model. The orange line represents the observed data, while the black lines represent the model-predicted data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3 Conducting Model Diagnostics\n\n\n\n\n\n\nIt’s important to note that objective assumption tests (e.g., Anderson–Darling test) is not advisable for larger samples, since they can be overly sensitive to minor deviations. Additionally, they might overlook visual patterns that are not captured by a single metric (Kozak & Piepho, 2018; Schucany & Ng, 2006; Shatz, 2024).\nI included those tests here just for reference. However, for the reason above, all assumptions were diagnosed by visual assessment.\nFor a straightforward critique of normality tests specifically, refer to this article by Greener (2020).\n\n\n\n\nE.8.3.1 Normality\n\n\n\n\n\n\n\nAssumption 2\n\nNormality. For \\(i = 1, \\dots, n\\), the conditional distribution of \\(Y_{i}\\) given the vectors \\(z_{1}, \\dots , z_{n}\\) is a normal distribution (DeGroot & Schervish, 2012, p. 737).\n\n\n(Normality of the error term distribution (Hair, 2019, p. 287))\n\n\n\nAssumption 2 is satisfied, as the residuals shown a fairly normal distribution by visual inspection.\n\nE.8.3.1.1 Visual Inspection\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::test_normality(col = \"value\", name = \"Residuals\")\n\n\nFigure E.40: Histogram of the model residuals with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the residuals and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::select(.resid) |&gt;\n  tidyr::pivot_longer(.resid) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +\n  ggplot2::geom_boxplot(\n    outlier.colour = brandr::get_brand_color(\"orange\"),\n    outlier.shape = 1,\n    width = 0.5\n  ) +\n  ggplot2::labs(x = \"Variable\", y = \"Value\") +\n  ggplot2::coord_flip() +\n  ggplot2::theme(\n    axis.title.y = ggplot2::element_blank(),\n    axis.text.y = ggplot2::element_blank(),\n    axis.ticks.y = ggplot2::element_blank()\n  )\n\n\nFigure E.41: Boxplot of model residuals with outliers highlighted in orange.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::stats_summary(col = \"value\", name = \"Residuals\")\n\n\nTable E.28: Summary statistics of model residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.8.3.1.2 Tests\nIt’s important to note that the Kolmogorov-Smirnov and Pearson chi-square tests are included here just for reference, as many authors don’t recommend using them when testing for normality (D’Agostino & Belanger, 1990). Learn more about normality tests in Thode (2002).\nI also recommend checking the original papers for each test to understand their assumptions and limitations:\n\n\nAnderson-Darling test: Anderson & Darling (1952); Anderson & Darling (1954).\nBonett-Seier test: Bonett & Seier (2002).\n\nCramér-von Mises test: Cramér (1928); Anderson (1962).\n\nD’Agostino test: D’Agostino (1971); D’Agostino & Pearson (1973).\n\nJarque–Bera test: Jarque & Bera (1980); Bera & Jarque (1981); Jarque & Bera (1987).\n\nLilliefors (K-S) test: Smirnov (1948); Kolmogorov (1933); Massey (1951); Lilliefors (1967); Dallal & Wilkinson (1986).\n\nPearson chi-square test: Pearson (1900).\n\nShapiro-Francia test: Shapiro & Francia (1972).\n\nShapiro-Wilk test: Shapiro & Wilk (1965).\n\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The data is normally distributed} \\\\\n\\text{H}_{a}: \\text{The data is not normally distributed}\n\\end{cases}\n\\]\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::normality_summary(col = \"value\")\n\n\nTable E.29: Summary of statistical tests conducted to assess the normality of the residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.8.3.2 Linearity\n\n\n\n\n\n\n\nAssumption 3\n\nLinear mean. There is a vector of parameters \\(\\beta = (\\beta_{0}, \\dots, \\beta_{p - 1})\\) such that the conditional mean of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) has the form\n\n\n\\[\nz_{i0} \\beta_{0} + z_{i1} \\beta_{1} + \\cdots + z_{ip - 1} \\beta_{p - 1}\n\\]\nfor \\(i = 1, \\dots, n\\) (DeGroot & Schervish, 2012, p. 737).\n(Linearity of the phenomenon measured (Hair, 2019, p. 287))\n\n\n\nAssumption 3 is satisfied, as the relationship between the variables is fairly linear. As shown in Chapter 4, the hypothesis implies linearity and the distribution of the residuals also support this.\nCodeplot &lt;-\n  fit |&gt;\n  broom::augment(data) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_hline(\n    yintercept = 0,\n    color = \"black\",\n    linewidth = 0.5,\n    linetype = \"dashed\"\n  ) +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(x = \"Fitted values\", y = \"Residuals\")\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure E.42: Residual plot showing the relationship between fitted values and residuals. The dashed black line represent zero residuals, indicating an ideal model fit. The orange line indicate the conditional mean of residuals.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots &lt;- fit_engine |&gt; olsrr::ols_plot_resid_fit_spread(print_plot = FALSE)\n\nfor (i in seq_along(plots)) {\n  q &lt;- plots[[i]] + ggplot2::labs(title = ggplot2::element_blank())\n\n  q &lt;- q |&gt; ggplot2::ggplot_build()\n  q$data[[1]]$colour &lt;- brandr::get_brand_color(\"orange\")\n  q$plot$layers[[1]]$constructor$color &lt;- brandr::get_brand_color(\"orange\")\n\n  plots[[i]] &lt;- q |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n}\n\npatchwork::wrap_plots(plots$fm_plot, plots$rsd_plot, ncol = 2)\n\n\nFigure E.43: Residual fit spread plots to detect non-linearity, influential observations, and outliers. The side-by-side plots show the centered fit and residuals, illustrating the variation explained by the model and what remains in the residuals. Inappropriately specified models often exhibit greater spread in the residuals than in the centered fit. “Proportion Less” indicates the cumulative distribution function, representing the proportion of observations below a specific value, facilitating an assessment of model performance.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ramsey’s RESET test indicates that the model has no omitted variables. This test examines whether non-linear combinations of the fitted values can explain the response variable.\nLearn more about the Ramsey’s RESET test in: Ramsey (1969).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The model has no omitted variables} \\\\\n\\text{H}_{a}: \\text{The model has omitted variables}\n\\end{cases}\n\\]\n\nfit_engine |&gt; lmtest::resettest(power = 2:3)\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 147.17255, df1 = 2, df2 = 65812, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::resettest(type = \"regressor\")\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 30.252017, df1 = 18, df2 = 65796, p-value &lt; 2.2204e-16\n\n\nE.8.3.3 Homoscedasticity (Common Variance)\n\n\n\n\n\n\n\nAssumption 4\n\nCommon variance (homoscedasticity). There is as parameter \\(\\sigma^{2}\\) such the conditional variance of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) is \\(\\sigma^{2}\\) for \\(i = 1, \\dots, n\\).\n\n\n(Constant variance of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 4 is satisfied. When comparing the standardized residuals (\\(\\sqrt{|\\text{Standardized Residuals}|}\\)) spread to the fitted values, we can observe that the residuals are fairly constant across the range of values. This suggests that the residuals have a constant variance.\n\nE.8.3.3.1 Visual Inspection\nCodeplot &lt;-\n  fit |&gt;\n  stats::predict(data) |&gt;\n  dplyr::mutate(\n    .sd_resid =\n      fit_engine |&gt;\n      stats::rstandard() |&gt;\n      abs() |&gt;\n      sqrt()\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .sd_resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(\n    x = \"Fitted values\",\n    y = latex2exp::TeX(\"$\\\\sqrt{|Standardized \\\\ Residuals|}$\")\n  )\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure E.44: Relation between the fitted values of the model and its standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSFsc (Chronotype proxy) (seconds)\nAge (years)\nLongitude (decimal degrees)\nMonthly average global horizontal irradiance (Wh/m²)\nAnnual average global horizontal irradiance (Wh/m²)\nDaylight on the March equinox (seconds)\nDaylight on the June solstice (seconds)\nDaylight on the December solstice (seconds)\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"msf_sc\",\n    x_label = \"MSF~sc~ (Chronotype proxy) (seconds)\"\n  )\n\n\nTable E.30: Relation between msf_sc and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"age\",\n    x_label = \"Age (years)\"\n  )\n\n\nTable E.31: Relation between age and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"longitude\",\n    x_label = \"Longitude (decimal degrees)\"\n  )\n\n\nTable E.32: Relation between longitude and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"ghi_month\",\n    x_label = \"Monthly average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nTable E.33: Relation between ghi_month and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"ghi_annual\",\n    x_label = \"Annual average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nTable E.34: Relation between ghi_annual and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"march_equinox_daylight\",\n    x_label = \"Daylight on the March equinox (seconds)\"\n  )\n\n\nTable E.35: Relation between march_equinox_daylight and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"june_solstice_daylight\",\n    x_label = \"Daylight on the June solstice (seconds)\"\n  )\n\n\nTable E.36: Relation between june_solstice_daylight and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"december_solstice_daylight\",\n    x_label = \"Daylight on the December solstice (seconds)\"\n  )\n\n\nTable E.37: Relation between december_solstice_daylight and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.3.2 Breusch-Pagan Test\nThe Breusch-Pagan test test indicates that the residuals exhibit constant variance.\nLearn more about the Breusch-Pagan test in: Breusch & Pagan (1979) and Koenker (1981).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The variance is constant} \\\\\n\\text{H}_{a}: \\text{The variance is not constant}\n\\end{cases}\n\\]\n\n# With studentising modification of Koenker\nfit_engine |&gt; lmtest::bptest(studentize = TRUE)\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1142.3758, df = 8, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::bptest(studentize = FALSE)\n#&gt; \n#&gt;  Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1576.0339, df = 8, p-value &lt; 2.2204e-16\n\n\nCode# Using the studentized modification of Koenker.\nfit_engine |&gt; skedastic::breusch_pagan(koenker = TRUE)\n\n\n  \n\n\n\n\nCodefit_engine |&gt; skedastic::breusch_pagan(koenker = FALSE)\n\n\n  \n\n\n\n\nfit_engine |&gt; car::ncvTest()\n#&gt; Non-constant Variance Score Test \n#&gt; Variance formula: ~ fitted.values \n#&gt; Chisquare = 8392.973319, Df = 1, p = &lt; 2.22045e-16\n\n\nfit_engine_2 |&gt; olsrr::ols_test_breusch_pagan()\n#&gt; \n#&gt;  Breusch Pagan Test for Heteroskedasticity\n#&gt;  -----------------------------------------\n#&gt;  Ho: the variance is constant            \n#&gt;  Ha: the variance is not constant        \n#&gt; \n#&gt;                Data                \n#&gt;  ----------------------------------\n#&gt;  Response : msf_sc \n#&gt;  Variables: fitted values of msf_sc \n#&gt; \n#&gt;            Test Summary            \n#&gt;  ----------------------------------\n#&gt;  DF            =    1 \n#&gt;  Chi2          =    72.49499784 \n#&gt;  Prob &gt; Chi2   =    1.674558642e-17\n\n\nE.8.3.4 Independence\n\n\n\n\n\n\n\nAssumption 5\n\nIndependence. The random variables \\(Y_{1}, \\dots , Y_{n}\\) are independent given the observed \\(z_{1}, \\dots , z_{n}\\) (DeGroot & Schervish, 2012, p. 737).\n\n\n(Independence of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 5 is satisfied. Although the residuals show some autocorrelation, they fall within the acceptable range of the Durbin–Watson statistic (\\(1.5\\) to \\(2.5\\)). It’s also important to note that the observations for each predicted value are not related to any other prediction; in other words, they are not grouped or sequenced by any variable (by design) (see Hair (2019, p. 291) for more information).\nMany authors don’t consider autocorrelation tests for linear regression models, as they are more relevant for time series data. However, I include them here just for reference.\n\nE.8.3.4.1 Visual Inspection\nCodefit_engine |&gt;\n  residuals() |&gt;\n  forecast::ggtsdisplay(\n    lag.max = 30,\n    theme = ggplot2::theme_get()\n  )\n\n\nFigure E.45: Time series plot of the residuals along with its AutoCorrelation Function (ACF) and Partial AutoCorrelation Function (PACF).\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.4.2 Correlations\nTable E.38 shows the relative importance of independent variables in determining the response variable. It highlights how much each variable uniquely contributes to the R-squared value, beyond what is explained by the other predictors.\nCodefit_engine |&gt; olsrr::ols_correlations()\n\n\nTable E.38: Correlations between the dependent variable and the independent variables, along with the zero-order, part, and partial correlations. The zero-order correlation represents the Pearson correlation coefficient between the dependent and independent variables. Part correlations indicate how much the R-squared would decrease if a specific variable were removed from the model, while partial correlations reflect the portion of variance in the response variable that is explained by a specific independent variable, beyond the influence of other predictors in the model.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.8.3.4.3 Newey-West Estimator\nThe Newey-West estimator is a method used to estimate the covariance matrix of the coefficients in a regression model when the residuals are autocorrelated.\nLearn more about the Newey-West estimator in: Newey & West (1987) and Newey & West (1994).\n\nfit_engine |&gt; sandwich::NeweyWest()\n\n\nE.8.3.4.4 Durbin-Watson Test\nThe Durbin-Watson test is a statistical test used to detect the presence of autocorrelation at lag \\(1\\) in the residuals from a regression analysis. The test statistic ranges from \\(0\\) to \\(4\\), with a value of \\(2\\) indicating no autocorrelation. Values less than \\(2\\) indicate positive autocorrelation, while values greater than \\(2\\) indicate negative autocorrelation (Fox, 2016).\nA common rule of thumb in the statistical community is that a Durbin-Watson statistic between \\(1.5\\) and \\(2.5\\) suggests little to no autocorrelation.\nLearn more about the Durbin-Watson test in: Durbin & Watson (1950); Durbin & Watson (1951); and Durbin & Watson (1971).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Autocorrelation of the disturbances is 0} \\\\\n\\text{H}_{a}: \\text{Autocorrelation of the disturbances is not equal to 0}\n\\end{cases}\n\\]\n\ncar::durbinWatsonTest(fit_engine)\n#&gt;  lag Autocorrelation D-W Statistic p-value\n#&gt;    1   0.03636534742   1.927246859       0\n#&gt;  Alternative hypothesis: rho != 0\n\n\nE.8.3.4.5 Ljung-Box Test\nThe Ljung–Box test is a statistical test used to determine whether any autocorrelations within a time series are significantly different from zero. Rather than testing randomness at individual lags, it assesses the “overall” randomness across multiple lags.\nLearn more about the Ljung-Box test in: Box & Pierce (1970) and Ljung & Box (1978).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Residuals are independently distributed} \\\\\n\\text{H}_{a}: \\text{Residuals are not independently distributed}\n\\end{cases}\n\\]\n\nfit_engine |&gt;\n  stats::residuals() |&gt;\n  stats::Box.test(type = \"Ljung-Box\", lag = 10)\n#&gt; \n#&gt;  Box-Ljung test\n#&gt; \n#&gt; data:  stats::residuals(fit_engine)\n#&gt; X-squared = 1077.904, df = 10, p-value &lt; 2.2204e-16\n\n\nE.8.3.5 Colinearity/Multicollinearity\nDaylight duration for the March equinox, June solstice, and December solstice exhibited high multicollinearity, with a variance inflation factor (VIF) greater than \\(1000\\). However, since these variables are grouped as latitude proxies, this does not significantly impact the analysis. The primary focus is on the combined effect of the group, rather than the individual contributions of each variable.\n\nE.8.3.5.1 Variance Inflation Factor (VIF)\nThe Variance Inflation Factor (VIF) indicates the effect of other independent variables on the standard error of a regression coefficient. The VIF is directly related to the tolerance value (\\(\\text{VIF}_{i} = 1/\\text{TO}L\\)). High VIF values (larger than ~5 (Struck, 2024)) suggest significant collinearity or multicollinearity among the independent variables (Hair, 2019, p. 265).\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(panel = FALSE) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$VIF +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank()\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    axis.text = ggplot2::element_text(colour = \"black\"),\n    axis.text.y = ggplot2::element_text(size = 9),\n    legend.text = ggplot2::element_text(colour = \"black\")\n  )\n\n\nFigure E.46: Variance Inflation Factors (VIF) for each predictor variable. VIFs below 5 are considered acceptable. Between 5 and 10, the variable should be examined. Above 10, the variable must considered highly collinear.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt; olsrr::ols_vif_tol()\n\n\nTable E.39: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_engine_2 |&gt; performance::check_collinearity()\n\n\nTable E.40: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.8.3.5.2 Condition Index\nThe condition index is a measure of multicollinearity in a regression model. It is based on the eigenvalues of the correlation matrix of the predictors. A condition index of 30 or higher is generally considered indicative of significant collinearity (Belsley et al., 2004, pp. 112–114).\nCodefit_engine |&gt; olsrr::ols_eigen_cindex()\n\n\nTable E.41: Condition indexes and eigenvalues for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nE.8.3.6 Measures of Influence\nIn this section, I check several measures of influence that can be used to assess the impact of individual observations on the model estimates.\n\n\n\n\n\n\n\nLeverage points\n\nLeverage is a measure of the distance between individual values of a predictor and other values of the predictor. In other words, a point with high leverage has an x-value far away from the other x-values. Points with high leverage have the potential to influence the model estimates (Hair, 2019, p. 262; Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\n\n\n\n\n\n\nInfluence points\n\nInfluence is a measure of how much an observation affects the model estimates. If an observation with large influence were removed from the dataset, we would expect a large change in the predictive equation (Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\nE.8.3.6.1 Standardized Residuals\nStandardized residuals are a rescaling of the residual to a common basis by dividing each residual by the standard deviation of the residuals (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstandard() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.15712964050  1.93979500176 -0.04657102599  0.16847775936  0.74057620873 \n#&gt;              6 \n#&gt;  0.26545357021\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstandard(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Standardized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure E.47: Standardized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.6.2 Studentized Residuals\nStudentized residuals are a commonly used variant of the standardized residual. It differs from other methods in how it calculates the standard deviation used in standardization. To minimize the effect of any observation on the standardization process, the standard deviation of the residual for observation \\(i\\) is computed from regression estimates omitting the \\(i\\)th observation in the calculation of the regression estimates (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstudent() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.15713262023  1.93983571897 -0.04657067295  0.16847651573  0.74057366818 \n#&gt;              6 \n#&gt;  0.26545169561\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstudent(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Studentized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure E.48: Studentized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::mutate(\n    std = stats::rstudent(fit_engine)\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, std)) +\n  ggplot2::geom_point(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Predicted value\",\n    y = \"Studentized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure E.49: Relation between studentized residuals and fitted values.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_lev(threshold = 2, print_plot = FALSE)\n\nplot$plot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Studentized residual\"\n  )\n\n\nFigure E.50: Relation between studentized residuals and their leverage points.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.6.3 Hat Values\nThe hat value indicates how distinct an observation’s predictor values are from those of other observations. Observations with high hat values have high leverage and may be, though not necessarily, influential. There is no fixed threshold for what constitutes a “large” hat value; instead, the focus must be on observations with hat values significantly higher than the rest (Hair, 2019, p. 261; Nahhas, 2024).\n\nfit_engine |&gt; stats::hatvalues() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt; 0.00013898048936 0.00007224409278 0.00002461350726 0.00033667660191 \n#&gt;                5                6 \n#&gt; 0.00027050566497 0.00016155706267\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  hat = stats::hatvalues(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = hat, ymin = 0, ymax = hat)\n  ) +\n  ggplot2::geom_linerange(color = brandr::get_brand_color(\"black\")) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Hat value\"\n  )\n\n\nFigure E.51: Hat values for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.6.4 Cook’s Distance\nThe Cook’s D measures each observation’s influence on the model’s fitted values. It is considered one of the most representative metrics for assessing overall influence (Hair, 2019).\nA common practice is to flag observations with a Cook’s distance of 1.0 or greater. However, a threshold of \\(4 / (n - k - 1)\\), where \\(n\\) is the sample size and \\(k\\) is the number of independent variables, is suggested as a more conservative measure in small samples or for use with larger datasets (Hair, 2019).\nLearn more about Cook’s D in: Cook (1977); Cook (1979).\n\nfit_engine |&gt; stats::cooks.distance() |&gt; head()\n#&gt;                    1                    2                    3 \n#&gt; 0.000020679294899836 0.000030206672047819 0.000000005931619634 \n#&gt;                    4                    5                    6 \n#&gt; 0.000001062189057920 0.000016488868797202 0.000001265117168830\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_cooksd_bar(type = 2, print_plot = FALSE)\n\n# The following procedure changes the plot aesthetics.\nq &lt;- plot$plot + ggplot2::labs(title = ggplot2::element_blank())\nq &lt;- q |&gt; ggplot2::ggplot_build()\nq$data[[5]]$label &lt;- \"\"\n\nq |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure E.52: Cook’s distance for each observation along with a threshold line at \\(4 / (n - k - 1)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\"blue\", \"black\", \"black\")\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\nplot &lt;-\n  diag_sum_plots$OUTLIERS +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"Leverage\",\n    y = \"Studentized residuals\"\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    axis.text = ggplot2::element_text(colour = \"gray25\"),\n    axis.text.y = ggplot2::element_text(size = 9)\n  )\n\nplot &lt;- plot |&gt; ggplot2::ggplot_build()\n\n# The following procedure changes the plot aesthetics.\nfor (i in c(1:9)) {\n  # \"#1b6ca8\" \"#3aaf85\"\n  plot$data[[i]]$colour &lt;- dplyr::case_when(\n    plot$data[[i]]$colour == \"blue\" ~\n      ifelse(i == 4, brandr::get_brand_color(\"orange\"), \"blue\"),\n    plot$data[[i]]$colour == \"#1b6ca8\" ~ \"black\",\n    plot$data[[i]]$colour == \"darkgray\" ~ \"black\",\n    TRUE ~ plot$data[[i]]$colour\n  )\n}\n\nplot |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure E.53: Relation between studentized residuals and their leverage points. The blue line represents the Cook’s distance. Any points outside the contour lines are influential observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.6.5 Influence on Prediction (DFFITS)\nDFFITS (difference in fits) is a standardized measure of how much the prediction for a given observation would change if it were deleted from the model. Each observation’s DFFITS is standardized by the standard deviation of fit at that point (Struck, 2024).\nThe best rule of thumb is to classify as influential any standardized values that exceed \\(2 \\sqrt{(p / n)}\\), where \\(p\\) is the number of independent variables + 1 and \\(n\\) is the sample size (Hair, 2019, p. 261).\nLearn more about DDFITS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dffits() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt;  0.0136423829525  0.0164885251460 -0.0002310492747  0.0030918538761 \n#&gt;                5                6 \n#&gt;  0.0121819046541  0.0033742989948\n\nCodeplot &lt;- fit_engine |&gt;\n  olsrr::ols_plot_dffits(print_plot = FALSE)\n\nplot$plot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure E.54: Standardized DFFITS (difference in fits) for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.6.6 Influence on Parameter Estimates (DFBETAS)\nDFBETAS are a measure of the change in a regression coefficient when an observation is omitted from the regression analysis. The value of the DFBETA is in terms of the coefficient itself (Hair, 2019, p. 261). A cutoff for what is considered a large DFBETAS value is \\(2 / \\sqrt{n}\\), where \\(n\\) is the number of observations. (Struck, 2024).\nLearn more about DFBETAS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dfbeta() |&gt; head()\n#&gt;      (Intercept)              age        sexMale        longitude\n#&gt; 1  5272.81733566 -0.0103297498879 0.166626704794  0.0090802202511\n#&gt; 2  8197.57444066 -0.0083164496952 0.249335185503  0.0491262892645\n#&gt; 3   -36.15816716  0.0001849916294 0.003982737968 -0.0002199641745\n#&gt; 4  1064.61765809 -0.0025851382486 0.038075211806  0.0165727972950\n#&gt; 5 -1481.54121906 -0.0074473743746 0.117363954744  0.0121523116983\n#&gt; 6  1032.32650474 -0.0014681568274 0.042242644129  0.0038165160269\n#&gt;           ghi_month        ghi_annual march_equinox_daylight\n#&gt; 1  0.00061959362173 -0.00083507255247        -0.350540890345\n#&gt; 2  0.00045631163342 -0.00163940556034        -0.500784543805\n#&gt; 3 -0.00001487680831  0.00002897145287         0.001255023842\n#&gt; 4  0.00001927510088 -0.00008800967868        -0.073695333960\n#&gt; 5 -0.00008278741261  0.00039734105832        -0.019067781476\n#&gt; 6  0.00018707231159 -0.00023394209932        -0.077533426721\n#&gt;   june_solstice_daylight december_solstice_daylight\n#&gt; 1        0.1150434119642            0.1142725937461\n#&gt; 2        0.1567806083698            0.1557306315524\n#&gt; 3       -0.0002128704799           -0.0002139771606\n#&gt; 4        0.0247096323240            0.0245261762800\n#&gt; 5        0.0266921188409            0.0262809990294\n#&gt; 6        0.0269945937590            0.0267953331454\n\n\n\nIntercept\nage\nsexMale\nlongitude\nghi_month\nghi_annual\nmarch_equinox_daylight\njune_solstice_daylight\ndecember_solstice_daylight\n\n\n\nCodeplots$plots[[1]] +\nggplot2::labs(title = \"Intercept coefficient\")\n\n\nTable E.42: Standardized DFBETAS values for each observation concerning the Intercept coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[2]] +\nggplot2::labs(title = \"age coefficient\")\n\n\nTable E.43: Standardized DFBETAS values for each observation concerning the age coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[3]] +\nggplot2::labs(title = \"sexMale coefficient\")\n\n\nTable E.44: Standardized DFBETAS values for each observation concerning the sexMale coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[4]] +\nggplot2::labs(title = \"longitude coefficient\")\n\n\nTable E.45: Standardized DFBETAS values for each observation concerning the longitude coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[5]] +\nggplot2::labs(title = \"ghi_month coefficient\")\n\n\nTable E.46: Standardized DFBETAS values for each observation concerning the ghi_month coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[6]] +\nggplot2::labs(title = \"ghi_annual coefficient\")\n\n\nTable E.47: Standardized DFBETAS values for each observation concerning the ghi_annual coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[7]] +\nggplot2::labs(title = \"march_equinox_daylight coefficient\")\n\n\nTable E.48: Standardized DFBETAS values for each observation concerning the march_equinox_daylight coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[8]] +\nggplot2::labs(title = \"june_solstice_daylight coefficient\")\n\n\nTable E.49: Standardized DFBETAS values for each observation concerning the june_solstice_daylight coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[9]] +\nggplot2::labs(title = \"december_solstice_daylight coefficient\")\n\n\nTable E.50: Standardized DFBETAS values for each observation concerning the december_solstice_daylight coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE.8.3.6.7 Hadi’s Measure\nHadi’s measure of influence is based on the idea that influential observations can occur in either the response variable, the predictors, or both.\nLearn more about Hadi’s measure in: Chatterjee & Hadi (2012).\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_hadi(print_plot = FALSE)\n\nplot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Hadi's measure\"\n  )\n\n\nFigure E.55: Hadi’s influence measure for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_pot(print_plot = FALSE)\n\nplot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure E.56: Potential-residual plot classifying unusual observations as high-leverage points, outliers, or a combination of both.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#hypothesis-testing",
    "href": "qmd/supplementary-material-5.html#hypothesis-testing",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.9 Hypothesis Testing",
    "text": "E.9 Hypothesis Testing\nFollowing the criteria outlined in the methodology supplementary material, we now address the hypothesis for this test:\n\nHypothesis\n\nLatitude is associated with chronotype distributions, with populations closer to the equator exhibiting, on average, a shorter or more morning-oriented circadian phenotype compared to those residing near the poles.\n\n\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} \\leq \\text{MES} \\quad \\text{or} \\quad \\text{F-test is not significant} \\ (\\alpha \\geq 0.05) \\\\\n\\text{H}_{a}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} &gt; \\text{MES} \\quad \\text{and} \\quad \\text{F-test is significant} \\ (\\alpha &lt; 0.05)\n\\end{cases}\n\\]\n\nE.9.1 F-Test\nThe results indicate that the F-test is significant (\\(\\alpha &lt; 0.05\\)), meaning that the model including the latitude variables differs from the model without them.\n\\[\n\\text{F} = \\cfrac{\\text{R}^{2}_{f} - \\text{R}^{2}_{r} / (k_{f} - k_{R})}{(1 - \\text{R}^{2}_{f}) / (\\text{N} - k_{f} - 1)}\n\\]\n\nf_test &lt;- stats::anova(fit_engine_restricted, fit_engine_full)\n\nprint(f_test)\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: msf_sc ~ age + sex + longitude + ghi_month\n#&gt; Model 2: msf_sc ~ age + sex + longitude + ghi_month + ghi_annual + march_equinox_daylight + \n#&gt;     june_solstice_daylight + december_solstice_daylight\n#&gt;   Res.Df           RSS Df  Sum of Sq        F     Pr(&gt;F)    \n#&gt; 1  65818 1355087927709                                      \n#&gt; 2  65814 1350842514863  4 4245412847 51.70988 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nn &lt;- nrow(data)\nk_res &lt;- length(stats::coefficients(fit_engine_restricted)) - 1\nk_full &lt;- length(stats::coefficients(fit_engine_full)) - 1\n\nr_squared_restricted &lt;- summary(fit_engine_restricted)$r.squared\nr_squared_full &lt;- summary(fit_engine_full)$r.squared\n\n((r_squared_full - r_squared_restricted) /\n    (k_full - k_res)) / ((1 - r_squared_full) / (n  - k_full - 1))\n#&gt; [1] 51.70987699\n\n\nE.9.2 Effect Size\nThe results show that the \\(\\Delta \\ \\text{Adjusted} \\ \\text{R}^{2}\\) value is below the Minimal Effect Size (MES) threshold (\\(\\text{R}^{2} \\approx 0.01960784\\)), indicating that adding latitude does not meaningfully improve the model’s fit.\n\\[\n\\text{Cohen's } f^2 = \\cfrac{\\text{R}^{2}_{f} - \\text{R}^{2}_{r}}{1 - \\text{R}^{2}_{f}} = \\cfrac{\\Delta \\text{R}^{2}}{1 - \\text{R}^{2}_{f}}\n\\]\n\\[\n\\text{MES} = \\text{Cohen's } f^2 \\text{small threshold} = 0.02 \\\\\n\\]\n\\[\n0.02 = \\cfrac{\\text{R}^{2}}{1 - \\text{R}^{2}} \\quad \\text{or} \\quad \\text{R}^{2} = \\cfrac{0.02}{1.02} \\eqsim 0.01960784\n\\]\nCodeadj_r_squared_restricted &lt;-\n  fit_engine_restricted |&gt;\n  rutils:::summarize_r2(\n    n = nrow(data),\n    k = length(fit_engine_restricted$coefficients) - 1,\n    ci_level = 0.95\n  )\n\nadj_r_squared_restricted\n\n\nTable E.51: Confidence interval for the adjusted R-squared of the restricted model. LCL correspond to the lower limit, and UCL to the upper limit.\n\n\n\n\n  \n\n\n\n\n\n\nCodeadj_r_squared_full &lt;-\n  fit_engine_full |&gt;\n  rutils:::summarize_r2(\n    n = nrow(data),\n    k = length(fit_engine_full$coefficients) - 1,\n    ci_level = 0.95\n  )\n\nadj_r_squared_full\n\n\nTable E.52: Confidence interval for the adjusted R-squared of the full model. LCL correspond to the lower limit, and UCL to the upper limit.\n\n\n\n\n  \n\n\n\n\n\n\nCodedplyr::tibble(\n  name = c(\n    \"adj_r_squared_res\",\n    \"adj_r_squared_full\",\n    \"diff\"\n  ),\n  value = c(\n    adj_r_squared_restricted$value[1],\n    adj_r_squared_full$value[1],\n    adj_r_squared_full$value[1] - adj_r_squared_restricted$value[1]\n  )\n)\n\n\nTable E.53: Comparison between the coefficients of determination (\\(\\text{R}^2\\)) of the restricted and full models.\n\n\n\n\n  \n\n\n\n\n\n\nCodeeffect_size &lt;- rutils:::cohens_f_squared_summary(\n  base_r_squared = adj_r_squared_restricted,\n  new_r_squared = adj_r_squared_full\n)\n\neffect_size |&gt; rutils:::list_as_tibble()\n\n\nTable E.54: Effect size between the restricted and full models based on Cohen’s \\(f^2\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-5.html#conclusion",
    "href": "qmd/supplementary-material-5.html#conclusion",
    "title": "Appendix E — Hypothesis Test A",
    "section": "\nE.10 Conclusion",
    "text": "E.10 Conclusion\nBased on the hypothesis test results, we must reject the alternative hypothesis in favor of the null hypothesis. Latitude does not meaningful contribute to explaining the variance in chronotype.\n\nFor questions regarding these computations, please contact the author at danvartan@gmail.com.\n\n\n\n\nAllaire, J. J., Teague, C., Xie, Y., & Dervieux, C. (n.d.). Quarto [Computer software]. Zenodo. https://doi.org/10.5281/ZENODO.5960048\n\n\nAnderson, T. W. (1962). On the distribution of the two-sample Cramér-von Mises criterion. The Annals of Mathematical Statistics, 33(3), 1148–1159. https://doi.org/10.1214/aoms/1177704477\n\n\nAnderson, T. W., & Darling, D. A. (1952). Asymptotic theory of certain \"goodness of fit\" criteria based on stochastic processes. The Annals of Mathematical Statistics, 23(2), 193–212. https://www.jstor.org/stable/2236446\n\n\nAnderson, T. W., & Darling, D. A. (1954). A test of goodness of fit. Journal of the American Statistical Association, 49(268), 765–769. https://doi.org/10.1080/01621459.1954.10501232\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (2004). Regression diagnostics: Identifying influential data and sources of collinearity (Print ISBN: 9780471058564\nOnline ISBN: 9780471725152). John Wiley & Sons. https://doi.org/10.1002/0471725153\n\n\nBera, A. K., & Jarque, C. M. (1981). Efficient tests for normality, homoscedasticity and serial independence of regression residuals: Monte Carlo evidence. Economics Letters, 7(4), 313–318. https://doi.org/10.1016/0165-1765(81)90035-5\n\n\nBonett, D. G., & Seier, E. (2002). A test of normality with high uniform power. Computational Statistics & Data Analysis, 40(3), 435–445. https://doi.org/10.1016/S0167-9473(02)00074-9\n\n\nBox, G. E. P., & Pierce, D. A. (1970). Distribution of residual autocorrelations in autoregressive-integrated moving average time series models. Journal of the American Statistical Association, 65(332), 1509–1526. https://doi.org/10.1080/01621459.1970.10481180\n\n\nBreusch, T. S., & Pagan, A. R. (1979). A simple test for heteroscedasticity and random coefficient variation. Econometrica, 47(5), 1287–1294. https://doi.org/10.2307/1911963\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example (5th ed.). Wiley.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18. https://doi.org/10.1080/00401706.1977.10489493\n\n\nCook, R. D. (1979). Influential observations in linear regression. Journal of the American Statistical Association, 74(365), 169–174. https://doi.org/10.1080/01621459.1979.10481634\n\n\nCramér, H. (1928). On the composition of elementary errors: First paper: Mathematical deductions. Scandinavian Actuarial Journal, 1928(1), 13–74. https://doi.org/10.1080/03461238.1928.10416862\n\n\nD’Agostino, R. B. (1971). An omnibus test of normality for moderate and large size samples. Biometrika, 58(2), 341–348. https://doi.org/10.1093/biomet/58.2.341\n\n\nD’Agostino, R. B., & Belanger, A. (1990). A suggestion for using powerful and informative tests of normality. The American Statistician, 44(4), 316–321. https://doi.org/10.2307/2684359\n\n\nD’Agostino, R. B., & Pearson, E. S. (1973). Tests for departure from normality. Empirical results for the distributions of b2 and √b1. Biometrika, 60(3), 613–622. https://doi.org/10.1093/biomet/60.3.613\n\n\nDallal, G. E., & Wilkinson, L. (1986). An analytic approximation to the distribution of Lilliefors’s test statistic for normality. The American Statistician, 40(4), 294–296. https://doi.org/10.1080/00031305.1986.10475419\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and statistics (OCLC: ocn502674206) (4th ed.). Addison-Wesley.\n\n\nDurbin, J., & Watson, G. S. (1950). Testing for serial correlation in least squares regression. I. Biometrika, 37(3–4), 409–428. https://doi.org/10.1093/biomet/37.3-4.409\n\n\nDurbin, J., & Watson, G. S. (1951). Testing for serial correlation in least squares regression. II. Biometrika, 38(1–2), 159–178. https://doi.org/10.1093/biomet/38.1-2.159\n\n\nDurbin, J., & Watson, G. S. (1971). Testing for serial correlation in least squares regression. III. Biometrika, 58(1), 1–19. https://doi.org/10.1093/biomet/58.1.1\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (3rd ed.). Sage.\n\n\nGreener, R. (2020, August 4). Stop testing for normality. Medium. https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90\n\n\nHair, J. F. (2019). Multivariate data analysis (8th ed.). Cengage.\n\n\nJarque, C. M., & Bera, A. K. (1980). Efficient tests for normality, homoscedasticity and serial independence of regression residuals. Economics Letters, 6(3), 255–259. https://doi.org/10.1016/0165-1765(80)90024-5\n\n\nJarque, C. M., & Bera, A. K. (1987). A test for normality of observations and regression residuals. International Statistical Review, 55(2), 163–172. https://doi.org/10.2307/1403192\n\n\nKoenker, R. (1981). A note on studentizing a test for heteroscedasticity. Journal of Econometrics, 17(1), 107–112. https://doi.org/10.1016/0304-4076(81)90062-2\n\n\nKolmogorov, A. (1933). Sulla determinazione empirica di una legge di distribuzione. Giornale dell’Istituto Italiano degli Attuari, 4.\n\n\nKozak, M., & Piepho, H.-P. (2018). What’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions. Journal of Agronomy and Crop Science, 204(1), 86–98. https://doi.org/10.1111/jac.12220\n\n\nLilliefors, H. W. (1967). On the Kolmogorov-Smirnov test for normality with mean and variance unknown. Journal of the American Statistical Association, 62(318), 399–402. https://doi.org/10.1080/01621459.1967.10482916\n\n\nLjung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. Biometrika, 65(2), 297–303. https://doi.org/10.1093/biomet/65.2.297\n\n\nMassey, F. J. (1951). The Kolmogorov-Smirnov test for goodness of fit. Journal of the American Statistical Association, 46(253), 68–78. https://doi.org/10.1080/01621459.1951.10500769\n\n\nNahhas, R. W. (2024). Introduction to regression methods for public health using R. https://www.bookdown.org/rwnahhas/RMPH/\n\n\nNewey, W. K., & West, K. D. (1987). A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3), 703–708. https://doi.org/10.2307/1913610\n\n\nNewey, W. K., & West, K. D. (1994). Automatic lag selection in covariance matrix estimation. The Review of Economic Studies, 61(4), 631–653. https://doi.org/10.2307/2297912\n\n\nPearson, K. (1900). X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 50(302), 157–175. https://doi.org/10.1080/14786440009463897\n\n\nR Core Team. (n.d.). R: A language and environment for statistical computing [Computer software]. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nRamsey, J. B. (1969). Tests for specification errors in classical linear least-squares regression analysis. Journal of the Royal Statistical Society. Series B (Methodological), 31(2), 350–371. https://doi.org/10.1111/j.2517-6161.1969.tb00796.x\n\n\nSchucany, W. R., & Ng, H. K. T. (2006). Preliminary goodness-of-fit tests for normality do not validate the one-sample Student t. Communications in Statistics - Theory and Methods, 35(12), 2275–2286. https://doi.org/10.1080/03610920600853308\n\n\nShapiro, S. S., & Francia, R. S. (1972). An approximate analysis of variance test for normality. Journal of the American Statistical Association, 67(337), 215–216. https://doi.org/10.1080/01621459.1972.10481232\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples)†. Biometrika, 52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nShatz, I. (2024). Assumption-checking rather than (just) testing: The importance of visualization and effect size in statistical diagnostics. Behavior Research Methods, 56(2), 826–845. https://doi.org/10.3758/s13428-023-02072-x\n\n\nSmirnov, N. (1948). Table for estimating the goodness of fit of empirical distributions. Annals of Mathematical Statistics, 19, 279–281.\n\n\nStruck, J. (2024). Regression Diagnostics with R. University of Wisconsin-Madison. https://sscc.wisc.edu/sscc/pubs/RegDiag-R/\n\n\nThode, H. C. (2002). Testing for normality. Marcel Dekker.\n\n\nWelsch, R., & Kuh, E. (1977). Linear regression diagnostics (Working Paper 0173; p. 44). National Bureau of Economic Research. https://doi.org/10.3386/w0173",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hypothesis Test A</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html",
    "href": "qmd/supplementary-material-6.html",
    "title": "Appendix F — Hypothesis Test B",
    "section": "",
    "text": "F.1 Overview\nThis document focuses on testing the thesis hypothesis (Test B) using the methods described in the supplemental material titled Methods.\nThe assumptions addressed here are those relevant to general linear models, with further information available in the supplemental material titled Overview of General Linear Models.\nAs with all analyses in this thesis, the process is fully reproducible and was conducted using the R programming language (R Core Team, n.d.) along with the Quarto publishing system (Allaire et al., n.d.).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#setting-the-enviroment",
    "href": "qmd/supplementary-material-6.html#setting-the-enviroment",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.2 Setting the Enviroment",
    "text": "F.2 Setting the Enviroment\n\nCodelibrary(brandr)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(forecast)\nlibrary(ggplot2)\nlibrary(ggplotify)\nlibrary(here)\nlibrary(hms)\nlibrary(janitor)\nlibrary(latex2exp)\nlibrary(lmtest)\nlibrary(lubritime) # github.com/danielvartan/lubritime\nlibrary(magrittr)\nlibrary(methods)\nlibrary(olsrr)\nlibrary(parsnip)\nlibrary(patchwork)\nlibrary(performance)\nlibrary(pwrss)\nlibrary(quartor) # github.com/danielvartan/quartor\nlibrary(report)\nlibrary(rlang)\nlibrary(rutils) # github.com/danielvartan/rutils\nlibrary(see)\nlibrary(skedastic)\nlibrary(stats)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(workflows)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#loading-and-processing-the-data",
    "href": "qmd/supplementary-material-6.html#loading-and-processing-the-data",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.3 Loading and Processing the Data",
    "text": "F.3 Loading and Processing the Data\n\n\n\n\n\n\n\nAssumption 1\n\nPredictor is known. Either the vectors \\(z_{1}, \\dots , z_{n}\\) are known ahead of time, or they are the observed values of random vectors \\(Z_{1}, \\dots , Z_{n}\\) on whose values we condition before computing the joint distribution of (\\(Y_{1}, \\dots , Y_{n}\\)) (DeGroot & Schervish, 2012, p. 736).\n\n\n\n\n\nAssumption 1 is satisfied, as the predictors are known.\n\nCodetargets::tar_make(script = here::here(\"_targets.R\"))\n\n\nThis data processing is performed solely for the purpose of the analysis. The variables undergo numerical transformations to streamline the modeling process and minimize potential errors.\n\nCodedata &lt;-\n  targets::tar_read(\"weighted_data\", store = here::here(\"_targets\")) |&gt;\n  dplyr::select(\n    msf_sc, age, sex, latitude, longitude, ghi_month, cell_weight\n  ) |&gt;\n  dplyr::mutate(\n    msf_sc =\n      msf_sc |&gt;\n      lubritime:::link_to_timeline(threshold = hms::parse_hms(\"12:00:00\")) |&gt;\n      as.numeric()\n  ) |&gt;\n  tidyr::drop_na()\n\n\n\ndata |&gt;\n  dplyr::select(-cell_weight) |&gt;\n  report::report()\n#&gt; The data contains 65823 observations of the following 6 variables:\n#&gt; \n#&gt;   - msf_sc: n = 65823, Mean = 16120.91, SD = 5172.84, Median = 15685.71, MAD =\n#&gt; 5210.28, range: [1542.86, 30578.57], Skewness = 0.27, Kurtosis = -0.25, 0%\n#&gt; missing\n#&gt;   - age: n = 65823, Mean = 32.11, SD = 9.26, Median = 30.70, MAD = 9.42,\n#&gt; range: [18, 58.95], Skewness = 0.66, Kurtosis = -0.19, 0% missing\n#&gt;   - sex: 2 levels, namely Female (n = 43728, 66.43%) and Male (n = 22095,\n#&gt; 33.57%)\n#&gt;   - latitude: n = 65823, Mean = -20.88, SD = 6.04, Median = -22.90, MAD =\n#&gt; 2.26, range: [-33.52, 0.33], Skewness = 1.50, Kurtosis = 2.02, 0% missing\n#&gt;   - longitude: n = 65823, Mean = -45.81, SD = 4.05, Median = -46.59, MAD =\n#&gt; 3.87, range: [-57.11, -34.81], Skewness = 0.65, Kurtosis = 0.47, 0% missing\n#&gt;   - ghi_month: n = 65823, Mean = 5103.14, SD = 545.25, Median = 5050.00, MAD =\n#&gt; 593.04, range: [3508, 6699], Skewness = 0.03, Kurtosis = -0.04, 0% missing",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#conducting-a-power-analysis",
    "href": "qmd/supplementary-material-6.html#conducting-a-power-analysis",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.4 Conducting a Power Analysis",
    "text": "F.4 Conducting a Power Analysis\nThe results indicate that at least \\(1,683\\) observations per variable are required to achieve a power of \\(0.99\\) (\\(1 - \\beta\\)) (the probability of not committing a type II error) and a significance level (\\(\\alpha\\)) of \\(0.01\\) (\\(0.99\\) probability of not committing a type I error). The dataset contains \\(65,823\\) observations, which exceeds this requirement.\n\npwr_analysis &lt;- pwrss::pwrss.f.reg(\n  f2 = 0.02, # Minimal Effect Size (MES).\n  k = length(data) - 2, # Number of predictors (-msf_sc, -cell_weight).\n  power = 0.99,\n  alpha = 0.01\n)\n#&gt;  Linear Regression (F test) \n#&gt;  R-squared Deviation from 0 (zero) \n#&gt;  H0: r2 = 0 \n#&gt;  HA: r2 &gt; 0 \n#&gt;  ------------------------------ \n#&gt;   Statistical power = 0.99 \n#&gt;   n = 1683 \n#&gt;  ------------------------------ \n#&gt;  Numerator degrees of freedom = 5 \n#&gt;  Denominator degrees of freedom = 1676.459 \n#&gt;  Non-centrality parameter = 33.649 \n#&gt;  Type I error rate = 0.01 \n#&gt;  Type II error rate = 0.01\n\nCodepwrss::power.f.test(\n  ncp = pwr_analysis$ncp,\n  df1 = pwr_analysis$df1,\n  df2 = pwr_analysis$df2,\n  alpha = pwr_analysis$parms$alpha,\n  plot = TRUE\n)\n\n\nFigure F.1: Power analysis for the hypothesis test.\n\n\n\n\n\n\n\n\n#&gt;         power ncp.alt ncp.null alpha df1         df2      f.crit\n#&gt;  0.9900000002  33.649        0  0.01   5 1676.459387 3.028152636\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#examining-distributions",
    "href": "qmd/supplementary-material-6.html#examining-distributions",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.5 Examining Distributions",
    "text": "F.5 Examining Distributions\n\n\nMSF~sc~ (Chronotype proxy) (seconds)\nAge (years)\nLatitude (decimal degrees)\nLongitude (decimal degrees)\nMonthly average global horizontal irradiance (Wh/m²)\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"msf_sc\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable F.1: Statistics for the msf_sc variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"msf_sc\",\n    x_label = \"MSF~sc~ (Chronotype proxy) (seconds)\"\n  )\n\n\nFigure F.2: Histogram of the msf_sc variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"msf_sc\")\n\n\nFigure F.3: Box plot of the msf_sc variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"age\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable F.2: Statistics for the age variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"age\",\n    x_label = \"Age (years)\"\n  )\n\n\nFigure F.4: Histogram of the age variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"age\")\n\n\nFigure F.5: Box plot of the age variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"latitude\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable F.3: Statistics for the latitude variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"latitude\",\n    x_label = \"Latitude (decimal degrees)\"\n  )\n\n\nFigure F.6: Histogram of the latitude variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"latitude\")\n\n\nFigure F.7: Box plot of the latitude variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"longitude\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable F.4: Statistics for the longitude variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"longitude\",\n    x_label = \"Longitude (decimal degrees)\"\n  )\n\n\nFigure F.8: Histogram of the longitude variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"longitude\")\n\n\nFigure F.9: Box plot of the longitude variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\n\n\nCodedata |&gt;\n  rutils:::stats_summary(\n    col = \"ghi_month\",\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    hms_format = TRUE,\n    threshold = hms::parse_hms(\"12:00:00\"),\n    as_list = FALSE\n  )\n\n\nTable F.5: Statistics for the ghi_month variable.\n\n\n  \n\nSource: Created by the authors.\n\n\n\nCodedata |&gt;\n  plotr:::plot_dist(\n    col = \"ghi_month\",\n    x_label = \"Monthly average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nFigure F.10: Histogram of the ghi_month variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.\n\n\n\nCodedata |&gt; plotr:::plot_box_plot(col = \"ghi_month\")\n\n\nFigure F.11: Box plot of the ghi_month variable with jittered data points in black.\n\n\n\n\n\n\n\n\n\nSource: Created by the authors.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#examining-correlations",
    "href": "qmd/supplementary-material-6.html#examining-correlations",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.6 Examining Correlations",
    "text": "F.6 Examining Correlations\nCodedata |&gt;\n  plotr:::plot_ggally(\n    cols = c(\n      \"msf_sc\",\n      \"age\",\n      \"sex\",\n      \"latitude\",\n      \"longitude\",\n      \"ghi_month\"\n    ),\n    mapping = ggplot2::aes(colour = sex)\n  ) |&gt;\n  rutils::shush()\n\n\nFigure F.12: Correlation matrix of the main predictors.\n\n\n\n\n\n\n\n\n\nSource: Created by the author.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#building-the-restricted-model",
    "href": "qmd/supplementary-material-6.html#building-the-restricted-model",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.7 Building the Restricted Model",
    "text": "F.7 Building the Restricted Model\n\nF.7.1 Fitting the Model\n\nform &lt;- formula(msf_sc ~ age + sex + longitude + ghi_month)\n\n\nmodel &lt;-\n  parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\") |&gt;\n  parsnip::set_mode(\"regression\")\n\n\nworkflow &lt;-\n  workflows::workflow() |&gt;\n  workflows::add_case_weights(cell_weight) |&gt;\n  workflows::add_formula(form) |&gt;\n  workflows::add_model(model)\n\nworkflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; msf_sc ~ age + sex + longitude + ghi_month\n#&gt; \n#&gt; ── Case Weights ─────────────────────────────────────────────────────────────\n#&gt; cell_weight\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nfit &lt;- workflow |&gt; parsnip::fit(data)\nfit_restricted &lt;- fit\n\nCodefit_coefs &lt;- fit |&gt; broom::tidy()\n\nfit_coefs |&gt; janitor::adorn_rounding(5)\n\n\nTable F.6: Output from the model fitting process showing the estimated coefficients, standard errors, test statistics, and p-values for the terms in the linear regression model.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_stats &lt;- fit |&gt; broom::glance()\n\nfit_stats |&gt;\n  tidyr::pivot_longer(cols = dplyr::everything()) |&gt;\n  janitor::adorn_rounding(10)\n\n\nTable F.7: Summary of model fit statistics showing key metrics including R-squared, adjusted R-squared, sigma, statistic, p-value, degrees of freedom, log-likelihood, AIC, BIC, and deviance.\n\n\n\n\n  \n\n\n\n\n\n\n\nfit_engine &lt;- fit |&gt; parsnip::extract_fit_engine()\n\nfit_engine |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -44246.421  -2705.443   -315.904   2430.031  52016.723 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate     Std. Error   t value   Pr(&gt;|t|)    \n#&gt; (Intercept) 19427.76043870   389.03209392  49.93871 &lt; 2.22e-16 ***\n#&gt; age          -124.87537286     1.72710634 -72.30323 &lt; 2.22e-16 ***\n#&gt; sexMale       358.48561682    39.07149140   9.17512 &lt; 2.22e-16 ***\n#&gt; longitude     -71.86988120     4.86850721 -14.76220 &lt; 2.22e-16 ***\n#&gt; ghi_month      -0.52019020     0.03995143 -13.02056 &lt; 2.22e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4537.445 on 65818 degrees of freedom\n#&gt; Multiple R-squared:  0.08516525, Adjusted R-squared:  0.08510965 \n#&gt; F-statistic: 1531.808 on 4 and 65818 DF,  p-value: &lt; 2.2204e-16\n\n\nCode# A jerry-rigged solution to fix issues related to modeling using the pipe.\n\nfit_engine_2 &lt;- lm(form, data = data, weights = cell_weight)\nfit_engine_restricted &lt;- fit_engine_2\n\n\n\nreport::report(fit_engine_2)\n#&gt; We fitted a linear model (estimated using OLS) to predict msf_sc with age,\n#&gt; sex, longitude and ghi_month (formula: msf_sc ~ age + sex + longitude +\n#&gt; ghi_month). The model explains a statistically significant and weak\n#&gt; proportion of variance (R2 = 0.09, F(4, 65818) = 1531.81, p &lt; .001, adj. R2\n#&gt; = 0.09). The model's intercept, corresponding to age = 0, sex = Female,\n#&gt; longitude = 0 and ghi_month = 0, is at 19427.76 (95% CI [18665.26,\n#&gt; 20190.26], t(65818) = 49.94, p &lt; .001). Within this model:\n#&gt; \n#&gt;   - The effect of age is statistically significant and negative (beta =\n#&gt; -124.88, 95% CI [-128.26, -121.49], t(65818) = -72.30, p &lt; .001; Std. beta =\n#&gt; -0.27, 95% CI [-0.28, -0.26])\n#&gt;   - The effect of sex [Male] is statistically significant and positive (beta =\n#&gt; 358.49, 95% CI [281.91, 435.07], t(65818) = 9.18, p &lt; .001; Std. beta =\n#&gt; 0.07, 95% CI [0.05, 0.08])\n#&gt;   - The effect of longitude is statistically significant and negative (beta =\n#&gt; -71.87, 95% CI [-81.41, -62.33], t(65818) = -14.76, p &lt; .001; Std. beta =\n#&gt; -0.07, 95% CI [-0.08, -0.06])\n#&gt;   - The effect of ghi month is statistically significant and negative (beta =\n#&gt; -0.52, 95% CI [-0.60, -0.44], t(65818) = -13.02, p &lt; .001; Std. beta =\n#&gt; -0.06, 95% CI [-0.07, -0.05])\n#&gt; \n#&gt; Standardized parameters were obtained by fitting the model on a standardized\n#&gt; version of the dataset. 95% Confidence Intervals (CIs) and p-values were\n#&gt; computed using a Wald t-distribution approximation.\n\n\nF.7.2 Evaluating the Model Fit\n\nF.7.2.1 Predictions\nCodelimits &lt;-\n  stats::predict(fit_engine, interval = \"prediction\") |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils::shush()\n\nfit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::bind_cols(limits) |&gt;\n  ggplot2::ggplot(ggplot2::aes(msf_sc, .pred)) +\n  # ggplot2::geom_ribbon(\n  #   mapping = ggplot2::aes(ymin = lwr, ymax = upr),\n  #   alpha = 0.2\n  # ) +\n  ggplot2::geom_ribbon(\n    mapping = ggplot2::aes(\n      ymin = stats::predict(stats::loess(lwr ~ msf_sc)),\n      ymax = stats::predict(stats::loess(upr ~ msf_sc)),\n    ),\n    alpha = 0.2\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = lwr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = upr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_point() +\n  ggplot2::geom_abline(\n    intercept = 0,\n    slope = 1,\n    color = brandr::get_brand_color(\"orange\")\n  ) +\n  ggplot2::labs(\n    x = \"Observed\",\n    y = \"Predicted\"\n  )\n\n\nFigure F.13: Relation between observed and predicted values. The orange line is a 45-degree line originating from the plane’s origin and represents a perfect fit. The shaded area depicts a smoothed version of the 95% confidence of the prediction interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.2.2 Posterior Predictive Checks\nPosterior predictive checks are a Bayesian technique used to assess model fit by comparing observed data to data simulated from the posterior predictive distribution (i.e., the distribution of potential unobserved values given the observed data). These checks help identify systematic discrepancies between the observed and simulated data, providing insight into whether the chosen model (or distributional family) is appropriate. Ideally, the model-predicted lines should closely match the observed data patterns.\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\n      brandr::get_brand_color(\"orange\"),\n      brandr::get_brand_color(\"black\"),\n      \"black\"\n    )\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$PP_CHECK +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"MSFsc (Chronotype proxy) (s)\",\n  ) +\n  ggplot2::theme_get()\n\n\nFigure F.14: Posterior predictive checks for the model. The orange line represents the observed data, while the black lines represent the model-predicted data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3 Conducting Model Diagnostics\n\n\n\n\n\n\nIt’s important to note that objective assumption tests (e.g., Anderson–Darling test) is not advisable for larger samples, since they can be overly sensitive to minor deviations. Additionally, they might overlook visual patterns that are not captured by a single metric (Kozak & Piepho, 2018; Schucany & Ng, 2006; Shatz, 2024).\nI included those tests here just for reference. However, for the reason above, all assumptions were diagnosed by visual assessment.\nFor a straightforward critique of normality tests specifically, refer to this article by Greener (2020).\n\n\n\n\nF.7.3.1 Normality\n\n\n\n\n\n\n\nAssumption 2\n\nNormality. For \\(i = 1, \\dots, n\\), the conditional distribution of \\(Y_{i}\\) given the vectors \\(z_{1}, \\dots , z_{n}\\) is a normal distribution (DeGroot & Schervish, 2012, p. 737).\n\n\n(Normality of the error term distribution (Hair, 2019, p. 287))\n\n\n\nAssumption 2 is satisfied, as the residuals shown a fairly normal distribution by visual inspection.\n\nF.7.3.1.1 Visual Inspection\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::test_normality(col = \"value\", name = \"Residuals\")\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\nFigure F.15: Histogram of the model residuals with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the residuals and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::select(.resid) |&gt;\n  tidyr::pivot_longer(.resid) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +\n  ggplot2::geom_boxplot(\n    outlier.colour = brandr::get_brand_color(\"orange\"),\n    outlier.shape = 1,\n    width = 0.5\n  ) +\n  ggplot2::labs(x = \"Variable\", y = \"Value\") +\n  ggplot2::coord_flip() +\n  ggplot2::theme(\n    axis.title.y = ggplot2::element_blank(),\n    axis.text.y = ggplot2::element_blank(),\n    axis.ticks.y = ggplot2::element_blank()\n  )\n\n\nFigure F.16: Boxplot of model residuals with outliers highlighted in orange.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::stats_summary(col = \"value\", name = \"Residuals\")\n\n\nTable F.8: Summary statistics of model residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.7.3.1.2 Tests\nIt’s important to note that the Kolmogorov-Smirnov and Pearson chi-square tests are included here just for reference, as many authors don’t recommend using them when testing for normality (D’Agostino & Belanger, 1990). Learn more about normality tests in Thode (2002).\nI also recommend checking the original papers for each test to understand their assumptions and limitations:\n\n\nAnderson-Darling test: Anderson & Darling (1952); Anderson & Darling (1954).\nBonett-Seier test: Bonett & Seier (2002).\n\nCramér-von Mises test: Cramér (1928); Anderson (1962).\n\nD’Agostino test: D’Agostino (1971); D’Agostino & Pearson (1973).\n\nJarque–Bera test: Jarque & Bera (1980); Bera & Jarque (1981); Jarque & Bera (1987).\n\nLilliefors (K-S) test: Smirnov (1948); Kolmogorov (1933); Massey (1951); Lilliefors (1967); Dallal & Wilkinson (1986).\n\nPearson chi-square test: Pearson (1900).\n\nShapiro-Francia test: Shapiro & Francia (1972).\n\nShapiro-Wilk test: Shapiro & Wilk (1965).\n\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The data is normally distributed} \\\\\n\\text{H}_{a}: \\text{The data is not normally distributed}\n\\end{cases}\n\\]\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::normality_summary(col = \"value\")\n\n\nTable F.9: Summary of statistical tests conducted to assess the normality of the residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.7.3.2 Linearity\n\n\n\n\n\n\n\nAssumption 3\n\nLinear mean. There is a vector of parameters \\(\\beta = (\\beta_{0}, \\dots, \\beta_{p - 1})\\) such that the conditional mean of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) has the form\n\n\n\\[\nz_{i0} \\beta_{0} + z_{i1} \\beta_{1} + \\cdots + z_{ip - 1} \\beta_{p - 1}\n\\]\nfor \\(i = 1, \\dots, n\\) (DeGroot & Schervish, 2012, p. 737).\n(Linearity of the phenomenon measured (Hair, 2019, p. 287))\n\n\n\nAssumption 3 is satisfied, as the relationship between the variables is fairly linear. As shown in Chapter 4, the hypothesis implies linearity and the distribution of the residuals also support this.\nCodeplot &lt;-\n  fit |&gt;\n  broom::augment(data) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_hline(\n    yintercept = 0,\n    color = \"black\",\n    linewidth = 0.5,\n    linetype = \"dashed\"\n  ) +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(x = \"Fitted values\", y = \"Residuals\")\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure F.17: Residual plot showing the relationship between fitted values and residuals. The dashed black line represent zero residuals, indicating an ideal model fit. The orange line indicate the conditional mean of residuals.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots &lt;- fit_engine |&gt; olsrr::ols_plot_resid_fit_spread(print_plot = FALSE)\n\nfor (i in seq_along(plots)) {\n  q &lt;- plots[[i]] + ggplot2::labs(title = ggplot2::element_blank())\n\n  q &lt;- q |&gt; ggplot2::ggplot_build()\n  q$data[[1]]$colour &lt;- brandr::get_brand_color(\"orange\")\n  q$plot$layers[[1]]$constructor$color &lt;- brandr::get_brand_color(\"orange\")\n\n  plots[[i]] &lt;- q |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n}\n\npatchwork::wrap_plots(plots$fm_plot, plots$rsd_plot, ncol = 2)\n\n\nFigure F.18: Residual fit spread plots to detect non-linearity, influential observations, and outliers. The side-by-side plots show the centered fit and residuals, illustrating the variation explained by the model and what remains in the residuals. Inappropriately specified models often exhibit greater spread in the residuals than in the centered fit. “Proportion Less” indicates the cumulative distribution function, representing the proportion of observations below a specific value, facilitating an assessment of model performance.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ramsey’s RESET test indicates that the model has no omitted variables. This test examines whether non-linear combinations of the fitted values can explain the response variable.\nLearn more about the Ramsey’s RESET test in: Ramsey (1969).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The model has no omitted variables} \\\\\n\\text{H}_{a}: \\text{The model has omitted variables}\n\\end{cases}\n\\]\n\nfit_engine |&gt; lmtest::resettest(power = 2:3)\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 154.57861, df1 = 2, df2 = 65816, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::resettest(type = \"regressor\")\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 48.097756, df1 = 10, df2 = 65808, p-value &lt; 2.2204e-16\n\n\nF.7.3.3 Homoscedasticity (Common Variance)\n\n\n\n\n\n\n\nAssumption 4\n\nCommon variance (homoscedasticity). There is as parameter \\(\\sigma^{2}\\) such the conditional variance of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) is \\(\\sigma^{2}\\) for \\(i = 1, \\dots, n\\).\n\n\n(Constant variance of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 4 is satisfied. When comparing the standardized residuals (\\(\\sqrt{|\\text{Standardized Residuals}|}\\)) spread to the fitted values, we can observe that the residuals are fairly constant across the range of values. This suggests that the residuals have a constant variance.\n\nF.7.3.3.1 Visual Inspection\nCodeplot &lt;-\n  fit |&gt;\n  stats::predict(data) |&gt;\n  dplyr::mutate(\n    .sd_resid =\n      fit_engine |&gt;\n      stats::rstandard() |&gt;\n      abs() |&gt;\n      sqrt()\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .sd_resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(\n    x = \"Fitted values\",\n    y = latex2exp::TeX(\"$\\\\sqrt{|Standardized \\\\ Residuals|}$\")\n  )\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure F.19: Relation between the fitted values of the model and its standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSFsc (Chronotype proxy) (seconds)\nAge (years)\nLongitude (decimal degrees)\nMonthly average global horizontal irradiance (Wh/m²)\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"msf_sc\",\n    x_label = \"MSF~sc~  (Chronotype proxy) (seconds)\"\n  )\n\n\nTable F.10: Relation between msf_sc and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"age\",\n    x_label = \"Age (years)\"\n  )\n\n\nTable F.11: Relation between age and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"longitude\",\n    x_label = \"Longitude (decimal degrees)\"\n  )\n\n\nTable F.12: Relation between longitude and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"ghi_month\",\n    x_label = \"Monthly average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nTable F.13: Relation between ghi_month and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.3.2 Breusch-Pagan Test\nThe Breusch-Pagan test test indicates that the residuals exhibit constant variance.\nLearn more about the Breusch-Pagan test in: Breusch & Pagan (1979) and Koenker (1981).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The variance is constant} \\\\\n\\text{H}_{a}: \\text{The variance is not constant}\n\\end{cases}\n\\]\n\n# With studentising modification of Koenker\nfit_engine |&gt; lmtest::bptest(studentize = TRUE)\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1123.2579, df = 4, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::bptest(studentize = FALSE)\n#&gt; \n#&gt;  Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1545.0951, df = 4, p-value &lt; 2.2204e-16\n\n\nCode# Using the studentized modification of Koenker.\nfit_engine |&gt; skedastic::breusch_pagan(koenker = TRUE)\n\n\n  \n\n\n\n\nCodefit_engine |&gt; skedastic::breusch_pagan(koenker = FALSE)\n\n\n  \n\n\n\n\nfit_engine |&gt; car::ncvTest()\n#&gt; Non-constant Variance Score Test \n#&gt; Variance formula: ~ fitted.values \n#&gt; Chisquare = 8204.68981, Df = 1, p = &lt; 2.22045e-16\n\n\nfit_engine_2 |&gt; olsrr::ols_test_breusch_pagan()\n#&gt; \n#&gt;  Breusch Pagan Test for Heteroskedasticity\n#&gt;  -----------------------------------------\n#&gt;  Ho: the variance is constant            \n#&gt;  Ha: the variance is not constant        \n#&gt; \n#&gt;                Data                \n#&gt;  ----------------------------------\n#&gt;  Response : msf_sc \n#&gt;  Variables: fitted values of msf_sc \n#&gt; \n#&gt;           Test Summary            \n#&gt;  ---------------------------------\n#&gt;  DF            =    1 \n#&gt;  Chi2          =    78.53154173 \n#&gt;  Prob &gt; Chi2   =    7.87310582e-19\n\n\nF.7.3.4 Independence\n\n\n\n\n\n\n\nAssumption 5\n\nIndependence. The random variables \\(Y_{1}, \\dots , Y_{n}\\) are independent given the observed \\(z_{1}, \\dots , z_{n}\\) (DeGroot & Schervish, 2012, p. 737).\n\n\n(Independence of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 5 is satisfied. Although the residuals show some autocorrelation, they fall within the acceptable range of the Durbin–Watson statistic (\\(1.5\\) to \\(2.5\\)). It’s also important to note that the observations for each predicted value are not related to any other prediction; in other words, they are not grouped or sequenced by any variable (by design) (see Hair (2019, p. 291) for more information).\nMany authors don’t consider autocorrelation tests for linear regression models, as they are more relevant for time series data. They were include here just for reference.\n\nF.7.3.4.1 Visual Inspection\nCodefit_engine |&gt;\n  residuals() |&gt;\n  forecast::ggtsdisplay(lag.max=30)\n\n\nFigure F.20: Time series plot of the residuals along with its AutoCorrelation Function (ACF) and Partial AutoCorrelation Function (PACF).\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.4.2 Correlations\nTable F.14 shows the relative importance of independent variables in determining the response variable. It highlights how much each variable uniquely contributes to the R-squared value, beyond what is explained by the other predictors.\nCodefit_engine |&gt; olsrr::ols_correlations()\n\n\nTable F.14: Correlations between the dependent variable and the independent variables, along with the zero-order, part, and partial correlations. The zero-order correlation represents the Pearson correlation coefficient between the dependent and independent variables. Part correlations indicate how much the R-squared would decrease if a specific variable were removed from the model, while partial correlations reflect the portion of variance in the response variable that is explained by a specific independent variable, beyond the influence of other predictors in the model.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.7.3.4.3 Newey-West Estimator\nThe Newey-West estimator is a method used to estimate the covariance matrix of the coefficients in a regression model when the residuals are autocorrelated.\nLearn more about the Newey-West estimator in: Newey & West (1987) and Newey & West (1994).\n\nfit_engine |&gt; sandwich::NeweyWest()\n#&gt;                 (Intercept)              age          sexMale\n#&gt; (Intercept) 386230.75410310 -627.66593784297 -3949.0613848769\n#&gt; age           -627.66593784   11.37622243485    10.7984441977\n#&gt; sexMale      -3949.06138488   10.79844419774  3917.6442174967\n#&gt; longitude     4052.02814916    1.93511690642    -3.1560992922\n#&gt; ghi_month      -34.32905542    0.06796834477     0.4120212059\n#&gt;                   longitude       ghi_month\n#&gt; (Intercept) 4052.0281491632 -34.32905541605\n#&gt; age            1.9351169064   0.06796834477\n#&gt; sexMale       -3.1560992922   0.41202120594\n#&gt; longitude     64.4731661575  -0.22379900720\n#&gt; ghi_month     -0.2237990072   0.00420877298\n\n\nF.7.3.4.4 Durbin-Watson Test\nThe Durbin-Watson test is a statistical test used to detect the presence of autocorrelation at lag \\(1\\) in the residuals from a regression analysis. The test statistic ranges from \\(0\\) to \\(4\\), with a value of \\(2\\) indicating no autocorrelation. Values less than \\(2\\) indicate positive autocorrelation, while values greater than \\(2\\) indicate negative autocorrelation (Fox, 2016).\nA common rule of thumb in the statistical community is that a Durbin-Watson statistic between \\(1.5\\) and \\(2.5\\) suggests little to no autocorrelation.\nLearn more about the Durbin-Watson test in: Durbin & Watson (1950); Durbin & Watson (1951); and Durbin & Watson (1971).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Autocorrelation of the disturbances is 0} \\\\\n\\text{H}_{a}: \\text{Autocorrelation of the disturbances is not equal to 0}\n\\end{cases}\n\\]\n\ncar::durbinWatsonTest(fit_engine)\n#&gt;  lag Autocorrelation D-W Statistic p-value\n#&gt;    1   0.03604242224   1.927892131       0\n#&gt;  Alternative hypothesis: rho != 0\n\n\nF.7.3.4.5 Ljung-Box Test\nThe Ljung–Box test is a statistical test used to determine whether any autocorrelations within a time series are significantly different from zero. Rather than testing randomness at individual lags, it assesses the “overall” randomness across multiple lags.\nLearn more about the Ljung-Box test in: Box & Pierce (1970) and Ljung & Box (1978).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Residuals are independently distributed} \\\\\n\\text{H}_{a}: \\text{Residuals are not independently distributed}\n\\end{cases}\n\\]\n\nfit_engine |&gt;\n  stats::residuals() |&gt;\n  stats::Box.test(type = \"Ljung-Box\", lag = 10)\n#&gt; \n#&gt;  Box-Ljung test\n#&gt; \n#&gt; data:  stats::residuals(fit_engine)\n#&gt; X-squared = 1078.3505, df = 10, p-value &lt; 2.2204e-16\n\n\nF.7.3.5 Colinearity/Multicollinearity\nNo high degree of colinearity was observed among the independent variables.\n\nF.7.3.5.1 Variance Inflation Factor (VIF)\nThe Variance Inflation Factor (VIF) indicates the effect of other independent variables on the standard error of a regression coefficient. The VIF is directly related to the tolerance value (\\(\\text{VIF}_{i} = 1/\\text{TO}L\\)). High VIF values (larger than ~5 (Struck, 2024)) suggest significant collinearity or multicollinearity among the independent variables (Hair, 2019, p. 265).\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(panel = FALSE) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$VIF +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank()\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    axis.text = ggplot2::element_text(colour = \"black\"),\n    axis.text.y = ggplot2::element_text(size = 9),\n    legend.text = ggplot2::element_text(colour = \"black\")\n  )\n\n\nFigure F.21: Variance Inflation Factors (VIF) for each predictor variable. VIFs below 5 are considered acceptable. Between 5 and 10, the variable should be examined. Above 10, the variable must considered highly collinear.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt; olsrr::ols_vif_tol()\n\n\nTable F.15: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_engine_2 |&gt; performance::check_collinearity()\n\n\nTable F.16: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.7.3.5.2 Condition Index\nThe condition index is a measure of multicollinearity in a regression model. It is based on the eigenvalues of the correlation matrix of the predictors. A condition index of 30 or higher is generally considered indicative of significant collinearity (Belsley et al., 2004, pp. 112–114).\nCodefit_engine |&gt; olsrr::ols_eigen_cindex()\n\n\nTable F.17: Condition indexes and eigenvalues for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.7.3.6 Measures of Influence\nIn this section, I check several measures of influence that can be used to assess the impact of individual observations on the model estimates.\n\n\n\n\n\n\n\nLeverage points\n\nLeverage is a measure of the distance between individual values of a predictor and other values of the predictor. In other words, a point with high leverage has an x-value far away from the other x-values. Points with high leverage have the potential to influence the model estimates (Hair, 2019, p. 262; Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\n\n\n\n\n\n\nInfluence points\n\nInfluence is a measure of how much an observation affects the model estimates. If an observation with large influence were removed from the dataset, we would expect a large change in the predictive equation (Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\nF.7.3.6.1 Standardized Residuals\nStandardized residuals are a rescaling of the residual to a common basis by dividing each residual by the standard deviation of the residuals (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstandard() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.16375203728  1.90741700801 -0.05283727038  0.15325458085  0.89675594708 \n#&gt;              6 \n#&gt;  0.31486218404\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstandard(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = \"blue\") +\n  ggplot2::geom_hline(yintercept = -2, color = \"blue\") +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Standardized residual\"\n  )\n\n\nFigure F.22: Standardized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.6.2 Studentized Residuals\nStudentized residuals are a commonly used variant of the standardized residual. It differs from other methods in how it calculates the standard deviation used in standardization. To minimize the effect of any observation on the standardization process, the standard deviation of the residual for observation \\(i\\) is computed from regression estimates omitting the \\(i\\)th observation in the calculation of the regression estimates (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstudent() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.16375516976  1.90745523807 -0.05283687011  0.15325344396  0.89675461300 \n#&gt;              6 \n#&gt;  0.31486002925\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstudent(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = \"blue\") +\n  ggplot2::geom_hline(yintercept = -2, color = \"blue\") +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Studentized residual\"\n  )\n\n\nFigure F.23: Studentized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::mutate(\n    std = stats::rstudent(fit_engine)\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, std)) +\n  ggplot2::geom_point(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = \"blue\") +\n  ggplot2::geom_hline(yintercept = -2, color = \"blue\") +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Predicted value\",\n    y = \"Studentized residual\"\n  )\n\n\nFigure F.24: Relation between studentized residuals and fitted values.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_lev(threshold = 2, print_plot = FALSE)\n\nplot$plot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Studentized residual\"\n  )\n\n\nFigure F.25: Relation between studentized residuals and their leverage points.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.6.3 Hat Values\nThe hat value indicates how distinct an observation’s predictor values are from those of other observations. Observations with high hat values have high leverage and may be, though not necessarily, influential. There is no fixed threshold for what constitutes a “large” hat value; instead, the focus must be on observations with hat values significantly higher than the rest (Hair, 2019, p. 261; Nahhas, 2024).\n\nfit_engine |&gt; stats::hatvalues() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt; 0.00010146471316 0.00005268015355 0.00001872310039 0.00028115925295 \n#&gt;                5                6 \n#&gt; 0.00009202990156 0.00009341669996\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  hat = stats::hatvalues(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = hat, ymin = 0, ymax = hat)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Hat value\"\n  )\n\n\nFigure F.26: Hat values for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.6.4 Cook’s Distance\nThe Cook’s D measures each observation’s influence on the model’s fitted values. It is considered one of the most representative metrics for assessing overall influence (Hair, 2019).\nA common practice is to flag observations with a Cook’s distance of 1.0 or greater. However, a threshold of \\(4 / (n - k - 1)\\), where \\(n\\) is the sample size and \\(k\\) is the number of independent variables, is suggested as a more conservative measure in small samples or for use with larger datasets (Hair, 2019).\nLearn more about Cook’s D in: Cook (1977); Cook (1979).\n\nfit_engine |&gt; stats::cooks.distance() |&gt; head()\n#&gt;                   1                   2                   3 \n#&gt; 0.00002748590265018 0.00003833462407715 0.00000001045434047 \n#&gt;                   4                   5                   6 \n#&gt; 0.00000132108702976 0.00001480292211365 0.00000185240564778\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_cooksd_bar(type = 2, print_plot = FALSE)\n\n# The following procedure changes the plot aesthetics.\nq &lt;- plot$plot + ggplot2::labs(title = ggplot2::element_blank())\nq &lt;- q |&gt; ggplot2::ggplot_build()\nq$data[[5]]$label &lt;- \"\"\n\nq |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure F.27: Cook’s distance for each observation along with a threshold line at \\(4 / (n - k - 1)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\"blue\", \"black\", \"black\")\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\nplot &lt;-\n  diag_sum_plots$OUTLIERS +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"Leverage\",\n    y = \"Studentized residuals\"\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    axis.text = ggplot2::element_text(colour = \"gray25\"),\n    axis.text.y = ggplot2::element_text(size = 9)\n  )\n\nplot &lt;- plot |&gt; ggplot2::ggplot_build()\n\n# The following procedure changes the plot aesthetics.\nfor (i in c(1:9)) {\n  # \"#1b6ca8\" \"#3aaf85\"\n  plot$data[[i]]$colour &lt;- dplyr::case_when(\n    plot$data[[i]]$colour == \"blue\" ~\n      ifelse(i == 4, brandr::get_brand_color(\"grey\"), brandr::get_brand_color(\"orange\")),\n    plot$data[[i]]$colour == \"#1b6ca8\" ~ \"black\",\n    plot$data[[i]]$colour == \"darkgray\" ~ \"black\",\n    TRUE ~ plot$data[[i]]$colour\n  )\n}\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\nplot |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure F.28: Relation between studentized residuals and their leverage points. The blue line represents the Cook’s distance. Any points outside the contour lines are influential observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.6.5 Influence on Prediction (DFFITS)\nDFFITS (difference in fits) is a standardized measure of how much the prediction for a given observation would change if it were deleted from the model. Each observation’s DFFITS is standardized by the standard deviation of fit at that point (Struck, 2024).\nThe best rule of thumb is to classify as influential any standardized values that exceed \\(2 \\sqrt{(p / n)}\\), where \\(p\\) is the number of independent variables + 1 and \\(n\\) is the sample size (Hair, 2019, p. 261).\nLearn more about DDFITS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dffits() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt;  0.0117230650042  0.0138448836690 -0.0002286283237  0.0025700850466 \n#&gt;                5                6 \n#&gt;  0.0086031616484  0.0030433372255\n\nCodeplot &lt;- fit_engine |&gt;\n  olsrr::ols_plot_dffits(print_plot = FALSE)\n\nplot$plot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure F.29: Standardized DFFITS (difference in fits) for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.6.6 Influence on Parameter Estimates (DFBETAS)\nDFBETAS are a measure of the change in a regression coefficient when an observation is omitted from the regression analysis. The value of the DFBETA is in terms of the coefficient itself (Hair, 2019, p. 261). A cutoff for what is considered a large DFBETAS value is \\(2 / \\sqrt{n}\\), where \\(n\\) is the number of observations. (Struck, 2024).\nLearn more about DFBETAS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dfbeta() |&gt; head()\n#&gt;      (Intercept)              age       sexMale         longitude\n#&gt; 1 -2.03640720959 -0.0103549562328 0.16604079671 -0.03782400905068\n#&gt; 2  1.78097597074 -0.0086526416297 0.24348797412 -0.00271444025137\n#&gt; 3 -0.03682022608  0.0002125757538 0.00456010503 -0.00008384067498\n#&gt; 4  0.43147642077 -0.0023850543494 0.03450072846  0.00725970026893\n#&gt; 5 -0.31752143263 -0.0083607753914 0.14169036321 -0.01730146267547\n#&gt; 6 -0.60088074723 -0.0016766287238 0.04956283032 -0.01062135319232\n#&gt;            ghi_month\n#&gt; 1  0.000138033471483\n#&gt; 2 -0.000299437545344\n#&gt; 3  0.000003930534343\n#&gt; 4 -0.000003628288665\n#&gt; 5 -0.000027700852353\n#&gt; 6  0.000035653094081\n\n\n\nIntercept\nage\nsexMale\nlongitude\nghi_month\n\n\n\nCodeplots$plots[[1]] +\nggplot2::labs(title = \"Intercept coefficient\")\n\n\nTable F.18: Standardized DFBETAS values for each observation concerning the Intercept coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[2]] +\nggplot2::labs(title = \"age coefficient\")\n\n\nTable F.19: Standardized DFBETAS values for each observation concerning the age coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[3]] +\nggplot2::labs(title = \"sexMale coefficient\")\n\n\nTable F.20: Standardized DFBETAS values for each observation concerning the sexMale coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[4]] +\nggplot2::labs(title = \"longitude coefficient\")\n\n\nTable F.21: Standardized DFBETAS values for each observation concerning the longitude coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[5]] +\nggplot2::labs(title = \"ghi_month coefficient\")\n\n\nTable F.22: Standardized DFBETAS values for each observation concerning the ghi_month coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.7.3.6.7 Hadi’s Measure\nHadi’s measure of influence is based on the idea that influential observations can occur in either the response variable, the predictors, or both.\nLearn more about Hadi’s measure in: Chatterjee & Hadi (2012).\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_hadi(print_plot = FALSE)\n\nplot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Hadi's measure\"\n  )\n\n\nFigure F.30: Hadi’s influence measure for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_pot(print_plot = FALSE)\n\nplot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure F.31: Potential-residual plot classifying unusual observations as high-leverage points, outliers, or a combination of both.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#building-the-full-model",
    "href": "qmd/supplementary-material-6.html#building-the-full-model",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.8 Building the Full Model",
    "text": "F.8 Building the Full Model\n\nF.8.1 Fitting the Model\n\nform &lt;- as.formula(\n    paste0(\n      \"msf_sc ~ \",\n      paste0(\n        c(\"age\", \"sex\", \"longitude\", \"ghi_month\",  \"latitude\"),\n        collapse = \" + \"\n      )\n    )\n)\n\n\nlm(form, data = data, weights = cell_weight)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = form, data = data, weights = cell_weight)\n#&gt; \n#&gt; Coefficients:\n#&gt;   (Intercept)            age        sexMale      longitude      ghi_month  \n#&gt; 18515.5920070   -125.2676948    359.1762870    -63.3197170     -0.3451739  \n#&gt;      latitude  \n#&gt;   -21.7328983\n\n\nmodel &lt;-\n  parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\") |&gt;\n  parsnip::set_mode(\"regression\")\n\n\nworkflow &lt;-\n  workflows::workflow() |&gt;\n  workflows::add_case_weights(cell_weight) |&gt;\n  workflows::add_formula(form) |&gt;\n  workflows::add_model(model)\n\nworkflow\n#&gt; ══ Workflow ═════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; msf_sc ~ age + sex + longitude + ghi_month + latitude\n#&gt; \n#&gt; ── Case Weights ─────────────────────────────────────────────────────────────\n#&gt; cell_weight\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nfit &lt;- workflow |&gt; parsnip::fit(data)\nfit_full &lt;- fit\n\nCodefit_coefs &lt;- fit |&gt; broom::tidy()\n\nfit_coefs |&gt; janitor::adorn_rounding(5)\n\n\nTable F.23: Output from the model fitting process showing the estimated coefficients, standard errors, test statistics, and p-values for the terms in the linear regression model.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_stats &lt;- fit |&gt; broom::glance()\n\nfit_stats |&gt;\n  tidyr::pivot_longer(cols = dplyr::everything()) |&gt;\n  janitor::adorn_rounding(10)\n\n\nTable F.24: Summary of model fit statistics showing key metrics including R-squared, adjusted R-squared, sigma, statistic, p-value, degrees of freedom, log-likelihood, AIC, BIC, and deviance.\n\n\n\n\n  \n\n\n\n\n\n\n\nfit_engine &lt;- fit |&gt; parsnip::extract_fit_engine()\n\nfit_engine |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;        Min         1Q     Median         3Q        Max \n#&gt; -44379.621  -2728.127   -339.733   2408.158  52050.165 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate     Std. Error   t value           Pr(&gt;|t|)    \n#&gt; (Intercept) 18515.59200698   416.59887820  44.44465         &lt; 2.22e-16 ***\n#&gt; age          -125.26769483     1.72782367 -72.50028         &lt; 2.22e-16 ***\n#&gt; sexMale       359.17628699    39.06087770   9.19529         &lt; 2.22e-16 ***\n#&gt; longitude     -63.31971702     5.06437656 -12.50296         &lt; 2.22e-16 ***\n#&gt; ghi_month      -0.34517390     0.04915168  -7.02263 0.0000000000021984 ***\n#&gt; latitude      -21.73289833     3.55728335  -6.10941 0.0000000010055943 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4536.194 on 65817 degrees of freedom\n#&gt; Multiple R-squared:  0.08568376, Adjusted R-squared:  0.0856143 \n#&gt; F-statistic: 1233.588 on 5 and 65817 DF,  p-value: &lt; 2.2204e-16\n\n\nCode# A jerry-rigged solution to fix issues related to modeling using the pipe.\n\nfit_engine_2 &lt;- lm(form, data = data, weights = cell_weight)\nfit_engine_full &lt;- fit_engine_2\n\n\n\nreport::report(fit_engine_2)\n#&gt; We fitted a linear model (estimated using OLS) to predict msf_sc with age,\n#&gt; sex, longitude, ghi_month and latitude (formula: msf_sc ~ age + sex +\n#&gt; longitude + ghi_month + latitude). The model explains a statistically\n#&gt; significant and weak proportion of variance (R2 = 0.09, F(5, 65817) =\n#&gt; 1233.59, p &lt; .001, adj. R2 = 0.09). The model's intercept, corresponding to\n#&gt; age = 0, sex = Female, longitude = 0, ghi_month = 0 and latitude = 0, is at\n#&gt; 18515.59 (95% CI [17699.06, 19332.13], t(65817) = 44.44, p &lt; .001). Within\n#&gt; this model:\n#&gt; \n#&gt;   - The effect of age is statistically significant and negative (beta =\n#&gt; -125.27, 95% CI [-128.65, -121.88], t(65817) = -72.50, p &lt; .001; Std. beta =\n#&gt; -0.27, 95% CI [-0.28, -0.26])\n#&gt;   - The effect of sex [Male] is statistically significant and positive (beta =\n#&gt; 359.18, 95% CI [282.62, 435.74], t(65817) = 9.20, p &lt; .001; Std. beta =\n#&gt; 0.07, 95% CI [0.05, 0.08])\n#&gt;   - The effect of longitude is statistically significant and negative (beta =\n#&gt; -63.32, 95% CI [-73.25, -53.39], t(65817) = -12.50, p &lt; .001; Std. beta =\n#&gt; -0.06, 95% CI [-0.07, -0.05])\n#&gt;   - The effect of ghi month is statistically significant and negative (beta =\n#&gt; -0.35, 95% CI [-0.44, -0.25], t(65817) = -7.02, p &lt; .001; Std. beta = -0.04,\n#&gt; 95% CI [-0.05, -0.03])\n#&gt;   - The effect of latitude is statistically significant and negative (beta =\n#&gt; -21.73, 95% CI [-28.71, -14.76], t(65817) = -6.11, p &lt; .001; Std. beta =\n#&gt; -0.03, 95% CI [-0.05, -0.02])\n#&gt; \n#&gt; Standardized parameters were obtained by fitting the model on a standardized\n#&gt; version of the dataset. 95% Confidence Intervals (CIs) and p-values were\n#&gt; computed using a Wald t-distribution approximation.\n\n\nF.8.2 Evaluating the Model Fit\n\nF.8.2.1 Predictions\nCodelimits &lt;-\n  stats::predict(fit_engine, interval = \"prediction\") |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils::shush()\n\nfit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::bind_cols(limits) |&gt;\n  ggplot2::ggplot(ggplot2::aes(msf_sc, .pred)) +\n  # ggplot2::geom_ribbon(\n  #   mapping = ggplot2::aes(ymin = lwr, ymax = upr),\n  #   alpha = 0.2\n  # ) +\n  ggplot2::geom_ribbon(\n    mapping = ggplot2::aes(\n      ymin = stats::predict(stats::loess(lwr ~ msf_sc)),\n      ymax = stats::predict(stats::loess(upr ~ msf_sc)),\n    ),\n    alpha = 0.2\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = lwr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_smooth(\n    mapping = ggplot2::aes(y = upr),\n    se = FALSE,\n    method = \"loess\",\n    formula = y ~ x,\n    linetype = \"dashed\",\n    linewidth = 0.2,\n    color = \"black\"\n  ) +\n  ggplot2::geom_point() +\n  ggplot2::geom_abline(\n    intercept = 0,\n    slope = 1,\n    color = brandr::get_brand_color(\"orange\")\n  ) +\n  ggplot2::labs(\n    x = \"Observed\",\n    y = \"Predicted\"\n  )\n\n\nFigure F.32: Relation between observed and predicted values. The orange line is a 45-degree line originating from the plane’s origin and represents a perfect fit. The shaded area depicts a smoothed version of the 95% confidence of the prediction interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.2.2 Posterior Predictive Checks\nPosterior predictive checks are a Bayesian technique used to assess model fit by comparing observed data to data simulated from the posterior predictive distribution (i.e., the distribution of potential unobserved values given the observed data). These checks help identify systematic discrepancies between the observed and simulated data, providing insight into whether the chosen model (or distributional family) is appropriate. Ideally, the model-predicted lines should closely match the observed data patterns.\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\n      brandr::get_brand_color(\"orange\"),\n      brandr::get_brand_color(\"black\"),\n      \"black\"\n    )\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$PP_CHECK +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"MSFsc (Chronotype proxy) (s)\",\n  ) +\n  ggplot2::theme_get()\n\n\nFigure F.33: Posterior predictive checks for the model. The orange line represents the observed data, while the black lines represent the model-predicted data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3 Conducting Model Diagnostics\n\n\n\n\n\n\nIt’s important to note that objective assumption tests (e.g., Anderson–Darling test) is not advisable for larger samples, since they can be overly sensitive to minor deviations. Additionally, they might overlook visual patterns that are not captured by a single metric (Kozak & Piepho, 2018; Schucany & Ng, 2006; Shatz, 2024).\nI included those tests here just for reference. However, for the reason above, all assumptions were diagnosed by visual assessment.\nFor a straightforward critique of normality tests specifically, refer to this article by Greener (2020).\n\n\n\n\nF.8.3.1 Normality\n\n\n\n\n\n\n\nAssumption 2\n\nNormality. For \\(i = 1, \\dots, n\\), the conditional distribution of \\(Y_{i}\\) given the vectors \\(z_{1}, \\dots , z_{n}\\) is a normal distribution (DeGroot & Schervish, 2012, p. 737).\n\n\n(Normality of the error term distribution (Hair, 2019, p. 287))\n\n\n\nAssumption 2 is satisfied, as the residuals shown a fairly normal distribution by visual inspection.\n\nF.8.3.1.1 Visual Inspection\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::test_normality(col = \"value\", name = \"Residuals\")\n\n\nFigure F.34: Histogram of the model residuals with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the residuals and the theoretical quantiles of the normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::select(.resid) |&gt;\n  tidyr::pivot_longer(.resid) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +\n  ggplot2::geom_boxplot(\n    outlier.colour = brandr::get_brand_color(\"orange\"),\n    outlier.shape = 1,\n    width = 0.5\n  ) +\n  ggplot2::labs(x = \"Variable\", y = \"Value\") +\n  ggplot2::coord_flip() +\n  ggplot2::theme(\n    axis.title.y = ggplot2::element_blank(),\n    axis.text.y = ggplot2::element_blank(),\n    axis.ticks.y = ggplot2::element_blank()\n  )\n\n\nFigure F.35: Boxplot of model residuals with outliers highlighted in orange.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::stats_summary(col = \"value\", name = \"Residuals\")\n\n\nTable F.25: Summary statistics of model residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.8.3.1.2 Tests\nIt’s important to note that the Kolmogorov-Smirnov and Pearson chi-square tests are included here just for reference, as many authors don’t recommend using them when testing for normality (D’Agostino & Belanger, 1990). Learn more about normality tests in Thode (2002).\nI also recommend checking the original papers for each test to understand their assumptions and limitations:\n\n\nAnderson-Darling test: Anderson & Darling (1952); Anderson & Darling (1954).\nBonett-Seier test: Bonett & Seier (2002).\n\nCramér-von Mises test: Cramér (1928); Anderson (1962).\n\nD’Agostino test: D’Agostino (1971); D’Agostino & Pearson (1973).\n\nJarque–Bera test: Jarque & Bera (1980); Bera & Jarque (1981); Jarque & Bera (1987).\n\nLilliefors (K-S) test: Smirnov (1948); Kolmogorov (1933); Massey (1951); Lilliefors (1967); Dallal & Wilkinson (1986).\n\nPearson chi-square test: Pearson (1900).\n\nShapiro-Francia test: Shapiro & Francia (1972).\n\nShapiro-Wilk test: Shapiro & Wilk (1965).\n\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The data is normally distributed} \\\\\n\\text{H}_{a}: \\text{The data is not normally distributed}\n\\end{cases}\n\\]\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  dplyr::as_tibble() |&gt;\n  rutils:::normality_summary(col = \"value\")\n\n\nTable F.26: Summary of statistical tests conducted to assess the normality of the residuals.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.8.3.2 Linearity\n\n\n\n\n\n\n\nAssumption 3\n\nLinear mean. There is a vector of parameters \\(\\beta = (\\beta_{0}, \\dots, \\beta_{p - 1})\\) such that the conditional mean of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) has the form\n\n\n\\[\nz_{i0} \\beta_{0} + z_{i1} \\beta_{1} + \\cdots + z_{ip - 1} \\beta_{p - 1}\n\\]\nfor \\(i = 1, \\dots, n\\) (DeGroot & Schervish, 2012, p. 737).\n(Linearity of the phenomenon measured (Hair, 2019, p. 287))\n\n\n\nAssumption 3 is satisfied, as the relationship between the variables is fairly linear. As shown in Chapter 4, the hypothesis implies linearity and the distribution of the residuals also support this.\nCodeplot &lt;-\n  fit |&gt;\n  broom::augment(data) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_hline(\n    yintercept = 0,\n    color = \"black\",\n    linewidth = 0.5,\n    linetype = \"dashed\"\n  ) +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(x = \"Fitted values\", y = \"Residuals\")\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure F.36: Residual plot showing the relationship between fitted values and residuals. The dashed black line represent zero residuals, indicating an ideal model fit. The orange line indicate the conditional mean of residuals.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots &lt;- fit_engine |&gt; olsrr::ols_plot_resid_fit_spread(print_plot = FALSE)\n\nfor (i in seq_along(plots)) {\n  q &lt;- plots[[i]] + ggplot2::labs(title = ggplot2::element_blank())\n\n  q &lt;- q |&gt; ggplot2::ggplot_build()\n  q$data[[1]]$colour &lt;- brandr::get_brand_color(\"orange\")\n  q$plot$layers[[1]]$constructor$color &lt;- brandr::get_brand_color(\"orange\")\n\n  plots[[i]] &lt;- q |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n}\n\npatchwork::wrap_plots(plots$fm_plot, plots$rsd_plot, ncol = 2)\n\n\nFigure F.37: Residual fit spread plots to detect non-linearity, influential observations, and outliers. The side-by-side plots show the centered fit and residuals, illustrating the variation explained by the model and what remains in the residuals. Inappropriately specified models often exhibit greater spread in the residuals than in the centered fit. “Proportion Less” indicates the cumulative distribution function, representing the proportion of observations below a specific value, facilitating an assessment of model performance.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ramsey’s RESET test indicates that the model has no omitted variables. This test examines whether non-linear combinations of the fitted values can explain the response variable.\nLearn more about the Ramsey’s RESET test in: Ramsey (1969).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The model has no omitted variables} \\\\\n\\text{H}_{a}: \\text{The model has omitted variables}\n\\end{cases}\n\\]\n\nfit_engine |&gt; lmtest::resettest(power = 2:3)\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 157.24246, df1 = 2, df2 = 65815, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::resettest(type = \"regressor\")\n#&gt; \n#&gt;  RESET test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; RESET = 42.477319, df1 = 12, df2 = 65805, p-value &lt; 2.2204e-16\n\n\nF.8.3.3 Homoscedasticity (Common Variance)\n\n\n\n\n\n\n\nAssumption 4\n\nCommon variance (homoscedasticity). There is as parameter \\(\\sigma^{2}\\) such the conditional variance of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) is \\(\\sigma^{2}\\) for \\(i = 1, \\dots, n\\).\n\n\n(Constant variance of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 4 is satisfied. When comparing the standardized residuals (\\(\\sqrt{|\\text{Standardized Residuals}|}\\)) spread to the fitted values, we can observe that the residuals are fairly constant across the range of values. This suggests that the residuals have a constant variance.\n\nF.8.3.3.1 Visual Inspection\nCodeplot &lt;-\n  fit |&gt;\n  stats::predict(data) |&gt;\n  dplyr::mutate(\n    .sd_resid =\n      fit_engine |&gt;\n      stats::rstandard() |&gt;\n      abs() |&gt;\n      sqrt()\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .sd_resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_smooth(color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::labs(\n    x = \"Fitted values\",\n    y = latex2exp::TeX(\"$\\\\sqrt{|Standardized \\\\ Residuals|}$\")\n  )\n\nplot |&gt; print() |&gt; rutils::shush()\n\n\nFigure F.38: Relation between the fitted values of the model and its standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSFsc (Chronotype proxy) (seconds)\nAge (years)\nLatitude (decimal degrees)\nLongitude (decimal degrees)\nMonthly average global horizontal irradiance (Wh/m²)\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"msf_sc\",\n    x_label = \"MSF~sc~ (Chronotype proxy) (seconds)\"\n  )\n\n\nTable F.27: Relation between msf_sc and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"age\",\n    x_label = \"Age (years)\"\n  )\n\n\nTable F.28: Relation between age and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"longitude\",\n    x_label = \"Latitude (decimal degrees)\"\n  )\n\n\nTable F.29: Relation between longitude and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"ghi_month\",\n    x_label = \"Longitude (decimal degrees)\"\n  )\n\n\nTable F.30: Relation between ghi_month and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  plotr:::plot_homoscedasticity(\n    data = data,\n    col = \"latitude\",\n    x_label = \"Monthly average global horizontal irradiance (Wh/m²)\"\n  )\n\n\nTable F.31: Relation between latitude and the model standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.3.2 Breusch-Pagan Test\nThe Breusch-Pagan test test indicates that the residuals exhibit constant variance.\nLearn more about the Breusch-Pagan test in: Breusch & Pagan (1979) and Koenker (1981).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{The variance is constant} \\\\\n\\text{H}_{a}: \\text{The variance is not constant}\n\\end{cases}\n\\]\n\n# With studentising modification of Koenker\nfit_engine |&gt; lmtest::bptest(studentize = TRUE)\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1131.7268, df = 5, p-value &lt; 2.2204e-16\n\n\nfit_engine |&gt; lmtest::bptest(studentize = FALSE)\n#&gt; \n#&gt;  Breusch-Pagan test\n#&gt; \n#&gt; data:  fit_engine\n#&gt; BP = 1552.1555, df = 5, p-value &lt; 2.2204e-16\n\n\nCode# Using the studentized modification of Koenker.\nfit_engine |&gt; skedastic::breusch_pagan(koenker = TRUE)\n\n\n  \n\n\n\n\nCodefit_engine |&gt; skedastic::breusch_pagan(koenker = FALSE)\n\n\n  \n\n\n\n\nfit_engine |&gt; car::ncvTest()\n#&gt; Non-constant Variance Score Test \n#&gt; Variance formula: ~ fitted.values \n#&gt; Chisquare = 9083.72986, Df = 1, p = &lt; 2.22045e-16\n\n\nfit_engine_2 |&gt; olsrr::ols_test_breusch_pagan()\n#&gt; \n#&gt;  Breusch Pagan Test for Heteroskedasticity\n#&gt;  -----------------------------------------\n#&gt;  Ho: the variance is constant            \n#&gt;  Ha: the variance is not constant        \n#&gt; \n#&gt;                Data                \n#&gt;  ----------------------------------\n#&gt;  Response : msf_sc \n#&gt;  Variables: fitted values of msf_sc \n#&gt; \n#&gt;            Test Summary            \n#&gt;  ----------------------------------\n#&gt;  DF            =    1 \n#&gt;  Chi2          =    72.17177366 \n#&gt;  Prob &gt; Chi2   =    1.972571651e-17\n\n\nF.8.3.4 Independence\n\n\n\n\n\n\n\nAssumption 5\n\nIndependence. The random variables \\(Y_{1}, \\dots , Y_{n}\\) are independent given the observed \\(z_{1}, \\dots , z_{n}\\) (DeGroot & Schervish, 2012, p. 737).\n\n\n(Independence of the error terms (Hair, 2019, p. 287))\n\n\n\nAssumption 5 is satisfied. Although the residuals show some autocorrelation, they fall within the acceptable range of the Durbin–Watson statistic (\\(1.5\\) to \\(2.5\\)). It’s also important to note that the observations for each predicted value are not related to any other prediction; in other words, they are not grouped or sequenced by any variable (by design) (see Hair (2019, p. 291) for more information).\nMany authors don’t consider autocorrelation tests for linear regression models, as they are more relevant for time series data. However, I include them here just for reference.\n\nF.8.3.4.1 Visual Inspection\nCodefit_engine |&gt;\n  residuals() |&gt;\n  forecast::ggtsdisplay(\n    lag.max = 30,\n    theme = ggplot2::theme_get()\n  )\n\n\nFigure F.39: Time series plot of the residuals along with its AutoCorrelation Function (ACF) and Partial AutoCorrelation Function (PACF).\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.4.2 Correlations\nTable F.32 shows the relative importance of independent variables in determining the response variable. It highlights how much each variable uniquely contributes to the R-squared value, beyond what is explained by the other predictors.\nCodefit_engine |&gt; olsrr::ols_correlations()\n\n\nTable F.32: Correlations between the dependent variable and the independent variables, along with the zero-order, part, and partial correlations. The zero-order correlation represents the Pearson correlation coefficient between the dependent and independent variables. Part correlations indicate how much the R-squared would decrease if a specific variable were removed from the model, while partial correlations reflect the portion of variance in the response variable that is explained by a specific independent variable, beyond the influence of other predictors in the model.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.8.3.4.3 Newey-West Estimator\nThe Newey-West estimator is a method used to estimate the covariance matrix of the coefficients in a regression model when the residuals are autocorrelated.\nLearn more about the Newey-West estimator in: Newey & West (1987) and Newey & West (1994).\n\nfit_engine |&gt; sandwich::NeweyWest()\n#&gt;                 (Intercept)              age          sexMale\n#&gt; (Intercept) 372182.06412627 -425.61745456166 -3424.2186418696\n#&gt; age           -425.61745456   11.64048090150    12.6072183707\n#&gt; sexMale      -3424.21864187   12.60721837068  3925.6828919070\n#&gt; longitude     3147.13754183   -0.78822771680   -12.4555482544\n#&gt; ghi_month      -37.01256722    0.02467722347     0.2877321473\n#&gt; latitude       943.81149308    5.87118029535    19.1989134139\n#&gt;                    longitude        ghi_month       latitude\n#&gt; (Intercept) 3147.13754183324 -37.012567216563 943.8114930774\n#&gt; age           -0.78822771680   0.024677223471   5.8711802954\n#&gt; sexMale      -12.45554825444   0.287732147283  19.1989134139\n#&gt; longitude     84.40290200333   0.010127180744 -35.6321522497\n#&gt; ghi_month      0.01012718074   0.005812386318  -0.3212730449\n#&gt; latitude     -35.63215224968  -0.321273044908  55.5679774342\n\n\nF.8.3.4.4 Durbin-Watson Test\nThe Durbin-Watson test is a statistical test used to detect the presence of autocorrelation at lag \\(1\\) in the residuals from a regression analysis. The test statistic ranges from \\(0\\) to \\(4\\), with a value of \\(2\\) indicating no autocorrelation. Values less than \\(2\\) indicate positive autocorrelation, while values greater than \\(2\\) indicate negative autocorrelation (Fox, 2016).\nA common rule of thumb in the statistical community is that a Durbin-Watson statistic between \\(1.5\\) and \\(2.5\\) suggests little to no autocorrelation.\nLearn more about the Durbin-Watson test in: Durbin & Watson (1950); Durbin & Watson (1951); and Durbin & Watson (1971).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Autocorrelation of the disturbances is 0} \\\\\n\\text{H}_{a}: \\text{Autocorrelation of the disturbances is not equal to 0}\n\\end{cases}\n\\]\n\ncar::durbinWatsonTest(fit_engine)\n#&gt;  lag Autocorrelation D-W Statistic p-value\n#&gt;    1   0.03647412376   1.927029552       0\n#&gt;  Alternative hypothesis: rho != 0\n\n\nF.8.3.4.5 Ljung-Box Test\nThe Ljung–Box test is a statistical test used to determine whether any autocorrelations within a time series are significantly different from zero. Rather than testing randomness at individual lags, it assesses the “overall” randomness across multiple lags.\nLearn more about the Ljung-Box test in: Box & Pierce (1970) and Ljung & Box (1978).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Residuals are independently distributed} \\\\\n\\text{H}_{a}: \\text{Residuals are not independently distributed}\n\\end{cases}\n\\]\n\nfit_engine |&gt;\n  stats::residuals() |&gt;\n  stats::Box.test(type = \"Ljung-Box\", lag = 10)\n#&gt; \n#&gt;  Box-Ljung test\n#&gt; \n#&gt; data:  stats::residuals(fit_engine)\n#&gt; X-squared = 1088.895, df = 10, p-value &lt; 2.2204e-16\n\n\nF.8.3.5 Colinearity/Multicollinearity\nNo high degree of colinearity was observed among the independent variables.\n\nF.8.3.5.1 Variance Inflation Factor (VIF)\nThe Variance Inflation Factor (VIF) indicates the effect of other independent variables on the standard error of a regression coefficient. The VIF is directly related to the tolerance value (\\(\\text{VIF}_{i} = 1/\\text{TO}L\\)). High VIF values (larger than ~5 (Struck, 2024)) suggest significant collinearity or multicollinearity among the independent variables (Hair, 2019, p. 265).\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(panel = FALSE) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\ndiag_sum_plots$VIF +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank()\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    axis.text = ggplot2::element_text(colour = \"black\"),\n    axis.text.y = ggplot2::element_text(size = 9),\n    legend.text = ggplot2::element_text(colour = \"black\")\n  )\n\n\nFigure F.40: Variance Inflation Factors (VIF) for each predictor variable. VIFs below 5 are considered acceptable. Between 5 and 10, the variable should be examined. Above 10, the variable must considered highly collinear.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit_engine |&gt; olsrr::ols_vif_tol()\n\n\nTable F.33: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\nCodefit_engine_2 |&gt; performance::check_collinearity()\n\n\nTable F.34: Variance Inflation Factors (VIF) and tolerance values for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.8.3.5.2 Condition Index\nThe condition index is a measure of multicollinearity in a regression model. It is based on the eigenvalues of the correlation matrix of the predictors. A condition index of 30 or higher is generally considered indicative of significant collinearity (Belsley et al., 2004, pp. 112–114).\nCodefit_engine |&gt; olsrr::ols_eigen_cindex()\n\n\nTable F.35: Condition indexes and eigenvalues for each predictor variable.\n\n\n\n\n  \n\n\n\n\n\n\n\nF.8.3.6 Measures of Influence\nIn this section, I check several measures of influence that can be used to assess the impact of individual observations on the model estimates.\n\n\n\n\n\n\n\nLeverage points\n\nLeverage is a measure of the distance between individual values of a predictor and other values of the predictor. In other words, a point with high leverage has an x-value far away from the other x-values. Points with high leverage have the potential to influence the model estimates (Hair, 2019, p. 262; Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\n\n\n\n\n\n\nInfluence points\n\nInfluence is a measure of how much an observation affects the model estimates. If an observation with large influence were removed from the dataset, we would expect a large change in the predictive equation (Nahhas, 2024; Struck, 2024).\n\n\n\n\n\n\nF.8.3.6.1 Standardized Residuals\nStandardized residuals are a rescaling of the residual to a common basis by dividing each residual by the standard deviation of the residuals (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstandard() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.14468004161  1.91162049123 -0.05544044493  0.15282975955  0.86927363116 \n#&gt;              6 \n#&gt;  0.29011581217\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstandard(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Standardized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure F.41: Standardized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.6.2 Studentized Residuals\nStudentized residuals are a commonly used variant of the standardized residual. It differs from other methods in how it calculates the standard deviation used in standardization. To minimize the effect of any observation on the standardization process, the standard deviation of the residual for observation \\(i\\) is computed from regression estimates omitting the \\(i\\)th observation in the calculation of the regression estimates (Hair, 2019, p. 264).\n\nfit_engine |&gt; stats::rstudent() |&gt; head()\n#&gt;              1              2              3              4              5 \n#&gt;  1.14468273994  1.91165903926 -0.05544002506  0.15282862565  0.86927201744 \n#&gt;              6 \n#&gt;  0.29011379371\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  std = stats::rstudent(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Studentized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure F.42: Studentized residuals for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  dplyr::mutate(\n    std = stats::rstudent(fit_engine)\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, std)) +\n  ggplot2::geom_point(color = \"black\") +\n  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color(\"grey\")) +\n  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color(\"orange\")) +\n  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +\n  ggplot2::labs(\n    x = \"Predicted value\",\n    y = \"Studentized residual\"\n  )\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\n\nFigure F.43: Relation between studentized residuals and fitted values.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_lev(threshold = 2, print_plot = FALSE)\n\nplot$plot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Studentized residual\"\n  )\n\n\nFigure F.44: Relation between studentized residuals and their leverage points.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.6.3 Hat Values\nThe hat value indicates how distinct an observation’s predictor values are from those of other observations. Observations with high hat values have high leverage and may be, though not necessarily, influential. There is no fixed threshold for what constitutes a “large” hat value; instead, the focus must be on observations with hat values significantly higher than the rest (Hair, 2019, p. 261; Nahhas, 2024).\n\nfit_engine |&gt; stats::hatvalues() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt; 0.00011154585920 0.00005304234042 0.00001890262352 0.00028116509697 \n#&gt;                5                6 \n#&gt; 0.00011264255448 0.00010994057380\n\nCodedplyr::tibble(\n  x = seq_len(nrow(data)),\n  hat = stats::hatvalues(fit_engine)\n) |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = x, y = hat, ymin = 0, ymax = hat)\n  ) +\n  ggplot2::geom_linerange(color = \"black\") +\n  ggplot2::labs(\n    x = \"Observation\",\n    y = \"Hat value\"\n  )\n\n\nFigure F.45: Hat values for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.6.4 Cook’s Distance\nThe Cook’s D measures each observation’s influence on the model’s fitted values. It is considered one of the most representative metrics for assessing overall influence (Hair, 2019).\nA common practice is to flag observations with a Cook’s distance of 1.0 or greater. However, a threshold of \\(4 / (n - k - 1)\\), where \\(n\\) is the sample size and \\(k\\) is the number of independent variables, is suggested as a more conservative measure in small samples or for use with larger datasets (Hair, 2019).\nLearn more about Cook’s D in: Cook (1977); Cook (1979).\n\nfit_engine |&gt; stats::cooks.distance() |&gt; head()\n#&gt;                    1                    2                    3 \n#&gt; 0.000024362332733250 0.000032307088331269 0.000000009683502249 \n#&gt;                    4                    5                    6 \n#&gt; 0.000001094833663704 0.000014187738483447 0.000001542400998445\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_cooksd_bar(type = 2, print_plot = FALSE)\n\n# The following procedure changes the plot aesthetics.\nq &lt;- plot$plot + ggplot2::labs(title = ggplot2::element_blank())\nq &lt;- q |&gt; ggplot2::ggplot_build()\nq$data[[5]]$label &lt;- \"\"\n\nq |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure F.46: Cook’s distance for each observation along with a threshold line at \\(4 / (n - k - 1)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCodediag_sum_plots &lt;-\n  fit_engine_2 |&gt;\n  performance::check_model(\n    panel = FALSE,\n    colors = c(\"blue\", \"black\", \"black\")\n  ) |&gt;\n  plot() |&gt;\n  rutils::shush()\n\nplot &lt;-\n  diag_sum_plots$OUTLIERS +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    subtitle = ggplot2::element_blank(),\n    x = \"Leverage\",\n    y = \"Studentized residuals\"\n  ) +\n  ggplot2::theme_get() +\n  ggplot2::theme(\n    legend.position = \"right\",\n    axis.title = ggplot2::element_text(size = 11, colour = \"black\"),\n    axis.text = ggplot2::element_text(colour = \"gray25\"),\n    axis.text.y = ggplot2::element_text(size = 9)\n  )\n\nplot &lt;- plot |&gt; ggplot2::ggplot_build()\n\n# The following procedure changes the plot aesthetics.\nfor (i in c(1:9)) {\n  # \"#1b6ca8\" \"#3aaf85\"\n  plot$data[[i]]$colour &lt;- dplyr::case_when(\n    plot$data[[i]]$colour == \"blue\" ~\n      ifelse(i == 4, brandr::get_brand_color(\"grey\"), brandr::get_brand_color(\"orange\")),\n    plot$data[[i]]$colour == \"#1b6ca8\" ~ \"black\",\n    plot$data[[i]]$colour == \"darkgray\" ~ \"black\",\n    TRUE ~ plot$data[[i]]$colour\n  )\n}\n#&gt; ! The color grey was not found in the `_brand.yml` file.\n\nplot |&gt; ggplot2::ggplot_gtable() |&gt; ggplotify::as.ggplot()\n\n\nFigure F.47: Relation between studentized residuals and their leverage points. The blue line represents the Cook’s distance. Any points outside the contour lines are influential observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.6.5 Influence on Prediction (DFFITS)\nDFFITS (difference in fits) is a standardized measure of how much the prediction for a given observation would change if it were deleted from the model. Each observation’s DFFITS is standardized by the standard deviation of fit at that point (Struck, 2024).\nThe best rule of thumb is to classify as influential any standardized values that exceed \\(2 \\sqrt{(p / n)}\\), where \\(p\\) is the number of independent variables + 1 and \\(n\\) is the sample size (Hair, 2019, p. 261).\nLearn more about DDFITS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dffits() |&gt; head()\n#&gt;                1                2                3                4 \n#&gt;  0.0120902723520  0.0139230150390 -0.0002410396927  0.0025629874183 \n#&gt;                5                6 \n#&gt;  0.0092263814598  0.0030420843540\n\nCodeplot &lt;- fit_engine |&gt;\n  olsrr::ols_plot_dffits(print_plot = FALSE)\n\nplot$plot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure F.48: Standardized DFFITS (difference in fits) for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.6.6 Influence on Parameter Estimates (DFBETAS)\nDFBETAS are a measure of the change in a regression coefficient when an observation is omitted from the regression analysis. The value of the DFBETA is in terms of the coefficient itself (Hair, 2019, p. 261). A cutoff for what is considered a large DFBETAS value is \\(2 / \\sqrt{n}\\), where \\(n\\) is the number of observations. (Struck, 2024).\nLearn more about DFBETAS in: Welsch & Kuh (1977) and Belsley et al. (2004).\n\nfit_engine |&gt; stats::dfbeta() |&gt; head()\n#&gt;      (Intercept)              age        sexMale        longitude\n#&gt; 1 -2.54516556048 -0.0104159004421 0.163686331678 -0.0321073321519\n#&gt; 2  1.95618198928 -0.0085954402186 0.243827233815 -0.0043297795972\n#&gt; 3 -0.03511637212  0.0002244958451 0.004780796264 -0.0001208221006\n#&gt; 4  0.42841706573 -0.0023785372907 0.034396923476  0.0072539325804\n#&gt; 5 -0.89699162117 -0.0083558454040 0.137757787162 -0.0112431701669\n#&gt; 6 -0.72959370828 -0.0016201765585 0.045788583490 -0.0081334151655\n#&gt;            ghi_month          latitude\n#&gt; 1  0.000239856585333 -0.01292950048716\n#&gt; 2 -0.000332972558663  0.00409258781030\n#&gt; 3  0.000003450115093  0.00008356212680\n#&gt; 4 -0.000003282495903 -0.00004156645975\n#&gt; 5  0.000086219957654 -0.01403997622748\n#&gt; 6  0.000066627687460 -0.00419536421718\n\n\n\nIntercept\nage\nsexMale\nlongitude\nghi_month\nlatitude\n\n\n\nCodeplots$plots[[1]] +\nggplot2::labs(title = \"Intercept coefficient\")\n\n\nTable F.36: Standardized DFBETAS values for each observation concerning the Intercept coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[2]] +\nggplot2::labs(title = \"age coefficient\")\n\n\nTable F.37: Standardized DFBETAS values for each observation concerning the age coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[3]] +\nggplot2::labs(title = \"sexMale coefficient\")\n\n\nTable F.38: Standardized DFBETAS values for each observation concerning the sexMale coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[4]] +\nggplot2::labs(title = \"longitude coefficient\")\n\n\nTable F.39: Standardized DFBETAS values for each observation concerning the longitude coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[5]] +\nggplot2::labs(title = \"ghi_month coefficient\")\n\n\nTable F.40: Standardized DFBETAS values for each observation concerning the ghi_month coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplots$plots[[6]] +\nggplot2::labs(title = \"latitude coefficient\")\n\n\nTable F.41: Standardized DFBETAS values for each observation concerning the latitude coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF.8.3.6.7 Hadi’s Measure\nHadi’s measure of influence is based on the idea that influential observations can occur in either the response variable, the predictors, or both.\nLearn more about Hadi’s measure in: Chatterjee & Hadi (2012).\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_hadi(print_plot = FALSE)\n\nplot +\n  ggplot2::labs(\n    title = ggplot2::element_blank(),\n    y = \"Hadi's measure\"\n  )\n\n\nFigure F.49: Hadi’s influence measure for each observation.\n\n\n\n\n\n\n\n\n\n\n\n\nCodeplot &lt;-\n  fit_engine |&gt;\n  olsrr::ols_plot_resid_pot(print_plot = FALSE)\n\nplot + ggplot2::labs(title = ggplot2::element_blank())\n\n\nFigure F.50: Potential-residual plot classifying unusual observations as high-leverage points, outliers, or a combination of both.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#hypothesis-testing",
    "href": "qmd/supplementary-material-6.html#hypothesis-testing",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.9 Hypothesis Testing",
    "text": "F.9 Hypothesis Testing\nFollowing the criteria outlined in the methodology supplementary material, we now address the hypothesis for this test:\n\nHypothesis\n\nLatitude is associated with chronotype distributions, with populations closer to the equator exhibiting, on average, a shorter or more morning-oriented circadian phenotype compared to those residing near the poles.\n\n\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} \\leq \\text{MES} \\quad \\text{or} \\quad \\text{F-test is not significant} \\ (\\alpha \\geq 0.05) \\\\\n\\text{H}_{a}: \\Delta \\ \\text{Adjusted} \\ \\text{R}^{2} &gt; \\text{MES} \\quad \\text{and} \\quad \\text{F-test is significant} \\ (\\alpha &lt; 0.05)\n\\end{cases}\n\\]\n\nF.9.1 F-Test\nThe results indicate that the F-test is significant (\\(\\alpha &lt; 0.05\\)), meaning that the model including the latitude variable differs from the model without it.\n\\[\n\\text{F} = \\cfrac{\\text{R}^{2}_{f} - \\text{R}^{2}_{r} / (k_{f} - k_{R})}{(1 - \\text{R}^{2}_{f}) / (\\text{N} - k_{f} - 1)}\n\\]\n\nf_test &lt;- stats::anova(fit_engine_restricted, fit_engine_full)\n\nprint(f_test)\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: msf_sc ~ age + sex + longitude + ghi_month\n#&gt; Model 2: msf_sc ~ age + sex + longitude + ghi_month + latitude\n#&gt;   Res.Df           RSS Df Sum of Sq        F          Pr(&gt;F)    \n#&gt; 1  65818 1355087927709                                          \n#&gt; 2  65817 1354319891756  1 768035954 37.32488 0.0000000010056 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nn &lt;- nrow(data)\nk_res &lt;- length(stats::coefficients(fit_engine_restricted)) - 1\nk_full &lt;- length(stats::coefficients(fit_engine_full)) - 1\n\nr_squared_restricted &lt;- summary(fit_engine_restricted)$r.squared\nr_squared_full &lt;- summary(fit_engine_full)$r.squared\n\n((r_squared_full - r_squared_restricted) /\n    (k_full - k_res)) / ((1 - r_squared_full) / (n  - k_full - 1))\n#&gt; [1] 37.32487625\n\n\nF.9.2 Effect Size\nThe results show that the \\(\\Delta \\ \\text{Adjusted} \\ \\text{R}^{2}\\) value is below the Minimal Effect Size (MES) threshold (\\(\\text{R}^{2} \\approx 0.01960784\\)), indicating that adding latitude does not meaningfully improve the model’s fit.\n\\[\n\\text{Cohen's } f^2 = \\cfrac{\\text{R}^{2}_{f} - \\text{R}^{2}_{r}}{1 - \\text{R}^{2}_{f}} = \\cfrac{\\Delta \\text{R}^{2}}{1 - \\text{R}^{2}_{f}}\n\\]\n\\[\n\\text{MES} = \\text{Cohen's } f^2 \\text{small threshold} = 0.02 \\\\\n\\]\n\\[\n0.02 = \\cfrac{\\text{R}^{2}}{1 - \\text{R}^{2}} \\quad \\text{or} \\quad \\text{R}^{2} = \\cfrac{0.02}{1.02} \\eqsim 0.01960784\n\\]\nCodeadj_r_squared_restricted &lt;-\n  fit_engine_restricted |&gt;\n  rutils:::summarize_r2(\n    n = nrow(data),\n    k = length(fit_engine_restricted$coefficients) - 1,\n    ci_level = 0.95\n  )\n\nadj_r_squared_restricted\n\n\nTable F.42: Confidence interval for the adjusted R-squared of the restricted model. LCL correspond to the lower limit, and UCL to the upper limit.\n\n\n\n\n  \n\n\n\n\n\n\nCodeadj_r_squared_full &lt;-\n  fit_engine_full |&gt;\n  rutils:::summarize_r2(\n    n = nrow(data),\n    k = length(fit_engine_full$coefficients) - 1,\n    ci_level = 0.95\n  )\n\nadj_r_squared_full\n\n\nTable F.43: Confidence interval for the adjusted R-squared of the full model. LCL correspond to the lower limit, and UCL to the upper limit.\n\n\n\n\n  \n\n\n\n\n\n\nCodedplyr::tibble(\n  name = c(\n    \"adj_r_squared_res\",\n    \"adj_r_squared_full\",\n    \"diff\"\n  ),\n  value = c(\n    adj_r_squared_restricted$value[1],\n    adj_r_squared_full$value[1],\n    adj_r_squared_full$value[1] - adj_r_squared_restricted$value[1]\n  )\n)\n\n\nTable F.44: Comparison between the coefficients of determination (\\(\\text{R}^2\\)) of the restricted and full models.\n\n\n\n\n  \n\n\n\n\n\n\nCodeeffect_size &lt;- rutils:::cohens_f_squared_summary(\n  base_r_squared = adj_r_squared_restricted,\n  new_r_squared = adj_r_squared_full\n)\n\neffect_size |&gt; rutils:::list_as_tibble()\n\n\nTable F.45: Effect size between the restricted and full models based on Cohen’s \\(f^2\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-6.html#conclusion",
    "href": "qmd/supplementary-material-6.html#conclusion",
    "title": "Appendix F — Hypothesis Test B",
    "section": "\nF.10 Conclusion",
    "text": "F.10 Conclusion\nBased on the hypothesis test results, we must reject the alternative hypothesis in favor of the null hypothesis. Latitude does not meaningful contribute to explaining the variance in chronotype.\n\nFor questions regarding these computations, please contact the author at danvartan@gmail.com.\n\n\n\n\nAllaire, J. J., Teague, C., Xie, Y., & Dervieux, C. (n.d.). Quarto [Computer software]. Zenodo. https://doi.org/10.5281/ZENODO.5960048\n\n\nAnderson, T. W. (1962). On the distribution of the two-sample Cramér-von Mises criterion. The Annals of Mathematical Statistics, 33(3), 1148–1159. https://doi.org/10.1214/aoms/1177704477\n\n\nAnderson, T. W., & Darling, D. A. (1952). Asymptotic theory of certain \"goodness of fit\" criteria based on stochastic processes. The Annals of Mathematical Statistics, 23(2), 193–212. https://www.jstor.org/stable/2236446\n\n\nAnderson, T. W., & Darling, D. A. (1954). A test of goodness of fit. Journal of the American Statistical Association, 49(268), 765–769. https://doi.org/10.1080/01621459.1954.10501232\n\n\nBelsley, D. A., Kuh, E., & Welsch, R. E. (2004). Regression diagnostics: Identifying influential data and sources of collinearity (Print ISBN: 9780471058564\nOnline ISBN: 9780471725152). John Wiley & Sons. https://doi.org/10.1002/0471725153\n\n\nBera, A. K., & Jarque, C. M. (1981). Efficient tests for normality, homoscedasticity and serial independence of regression residuals: Monte Carlo evidence. Economics Letters, 7(4), 313–318. https://doi.org/10.1016/0165-1765(81)90035-5\n\n\nBonett, D. G., & Seier, E. (2002). A test of normality with high uniform power. Computational Statistics & Data Analysis, 40(3), 435–445. https://doi.org/10.1016/S0167-9473(02)00074-9\n\n\nBox, G. E. P., & Pierce, D. A. (1970). Distribution of residual autocorrelations in autoregressive-integrated moving average time series models. Journal of the American Statistical Association, 65(332), 1509–1526. https://doi.org/10.1080/01621459.1970.10481180\n\n\nBreusch, T. S., & Pagan, A. R. (1979). A simple test for heteroscedasticity and random coefficient variation. Econometrica, 47(5), 1287–1294. https://doi.org/10.2307/1911963\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example (5th ed.). Wiley.\n\n\nCook, R. D. (1977). Detection of influential observation in linear regression. Technometrics, 19(1), 15–18. https://doi.org/10.1080/00401706.1977.10489493\n\n\nCook, R. D. (1979). Influential observations in linear regression. Journal of the American Statistical Association, 74(365), 169–174. https://doi.org/10.1080/01621459.1979.10481634\n\n\nCramér, H. (1928). On the composition of elementary errors: First paper: Mathematical deductions. Scandinavian Actuarial Journal, 1928(1), 13–74. https://doi.org/10.1080/03461238.1928.10416862\n\n\nD’Agostino, R. B. (1971). An omnibus test of normality for moderate and large size samples. Biometrika, 58(2), 341–348. https://doi.org/10.1093/biomet/58.2.341\n\n\nD’Agostino, R. B., & Belanger, A. (1990). A suggestion for using powerful and informative tests of normality. The American Statistician, 44(4), 316–321. https://doi.org/10.2307/2684359\n\n\nD’Agostino, R. B., & Pearson, E. S. (1973). Tests for departure from normality. Empirical results for the distributions of b2 and √b1. Biometrika, 60(3), 613–622. https://doi.org/10.1093/biomet/60.3.613\n\n\nDallal, G. E., & Wilkinson, L. (1986). An analytic approximation to the distribution of Lilliefors’s test statistic for normality. The American Statistician, 40(4), 294–296. https://doi.org/10.1080/00031305.1986.10475419\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and statistics (OCLC: ocn502674206) (4th ed.). Addison-Wesley.\n\n\nDurbin, J., & Watson, G. S. (1950). Testing for serial correlation in least squares regression. I. Biometrika, 37(3–4), 409–428. https://doi.org/10.1093/biomet/37.3-4.409\n\n\nDurbin, J., & Watson, G. S. (1951). Testing for serial correlation in least squares regression. II. Biometrika, 38(1–2), 159–178. https://doi.org/10.1093/biomet/38.1-2.159\n\n\nDurbin, J., & Watson, G. S. (1971). Testing for serial correlation in least squares regression. III. Biometrika, 58(1), 1–19. https://doi.org/10.1093/biomet/58.1.1\n\n\nFox, J. (2016). Applied regression analysis and generalized linear models (3rd ed.). Sage.\n\n\nGreener, R. (2020, August 4). Stop testing for normality. Medium. https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90\n\n\nHair, J. F. (2019). Multivariate data analysis (8th ed.). Cengage.\n\n\nJarque, C. M., & Bera, A. K. (1980). Efficient tests for normality, homoscedasticity and serial independence of regression residuals. Economics Letters, 6(3), 255–259. https://doi.org/10.1016/0165-1765(80)90024-5\n\n\nJarque, C. M., & Bera, A. K. (1987). A test for normality of observations and regression residuals. International Statistical Review, 55(2), 163–172. https://doi.org/10.2307/1403192\n\n\nKoenker, R. (1981). A note on studentizing a test for heteroscedasticity. Journal of Econometrics, 17(1), 107–112. https://doi.org/10.1016/0304-4076(81)90062-2\n\n\nKolmogorov, A. (1933). Sulla determinazione empirica di una legge di distribuzione. Giornale dell’Istituto Italiano degli Attuari, 4.\n\n\nKozak, M., & Piepho, H.-P. (2018). What’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions. Journal of Agronomy and Crop Science, 204(1), 86–98. https://doi.org/10.1111/jac.12220\n\n\nLilliefors, H. W. (1967). On the Kolmogorov-Smirnov test for normality with mean and variance unknown. Journal of the American Statistical Association, 62(318), 399–402. https://doi.org/10.1080/01621459.1967.10482916\n\n\nLjung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. Biometrika, 65(2), 297–303. https://doi.org/10.1093/biomet/65.2.297\n\n\nMassey, F. J. (1951). The Kolmogorov-Smirnov test for goodness of fit. Journal of the American Statistical Association, 46(253), 68–78. https://doi.org/10.1080/01621459.1951.10500769\n\n\nNahhas, R. W. (2024). Introduction to regression methods for public health using R. https://www.bookdown.org/rwnahhas/RMPH/\n\n\nNewey, W. K., & West, K. D. (1987). A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix. Econometrica, 55(3), 703–708. https://doi.org/10.2307/1913610\n\n\nNewey, W. K., & West, K. D. (1994). Automatic lag selection in covariance matrix estimation. The Review of Economic Studies, 61(4), 631–653. https://doi.org/10.2307/2297912\n\n\nPearson, K. (1900). X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 50(302), 157–175. https://doi.org/10.1080/14786440009463897\n\n\nR Core Team. (n.d.). R: A language and environment for statistical computing [Computer software]. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nRamsey, J. B. (1969). Tests for specification errors in classical linear least-squares regression analysis. Journal of the Royal Statistical Society. Series B (Methodological), 31(2), 350–371. https://doi.org/10.1111/j.2517-6161.1969.tb00796.x\n\n\nSchucany, W. R., & Ng, H. K. T. (2006). Preliminary goodness-of-fit tests for normality do not validate the one-sample Student t. Communications in Statistics - Theory and Methods, 35(12), 2275–2286. https://doi.org/10.1080/03610920600853308\n\n\nShapiro, S. S., & Francia, R. S. (1972). An approximate analysis of variance test for normality. Journal of the American Statistical Association, 67(337), 215–216. https://doi.org/10.1080/01621459.1972.10481232\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples)†. Biometrika, 52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591\n\n\nShatz, I. (2024). Assumption-checking rather than (just) testing: The importance of visualization and effect size in statistical diagnostics. Behavior Research Methods, 56(2), 826–845. https://doi.org/10.3758/s13428-023-02072-x\n\n\nSmirnov, N. (1948). Table for estimating the goodness of fit of empirical distributions. Annals of Mathematical Statistics, 19, 279–281.\n\n\nStruck, J. (2024). Regression Diagnostics with R. University of Wisconsin-Madison. https://sscc.wisc.edu/sscc/pubs/RegDiag-R/\n\n\nThode, H. C. (2002). Testing for normality. Marcel Dekker.\n\n\nWelsch, R., & Kuh, E. (1977). Linear regression diagnostics (Working Paper 0173; p. 44). National Bureau of Economic Research. https://doi.org/10.3386/w0173",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Hypothesis Test B</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-7.html",
    "href": "qmd/supplementary-material-7.html",
    "title": "Appendix G — Comparative Assessment of QualoCEP and Google Geocoding API Performance",
    "section": "",
    "text": "G.1 Overview\nThis document provides a workflow for testing QualoCEP geocoding against Google Geocoding API, comparing latitude and longitude values to assess consistency, as both services are used for geocoding data in this thesis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Comparative Assessment of QualoCEP and Google Geocoding API Performance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-7.html#overview",
    "href": "qmd/supplementary-material-7.html#overview",
    "title": "Appendix G — Comparative Assessment of QualoCEP and Google Geocoding API Performance",
    "section": "",
    "text": "To run this notebook you will need to have a Google Geocoding API key in your .Renviron file. Learn more about it here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Comparative Assessment of QualoCEP and Google Geocoding API Performance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-7.html#setting-the-enviroment",
    "href": "qmd/supplementary-material-7.html#setting-the-enviroment",
    "title": "Appendix G — Comparative Assessment of QualoCEP and Google Geocoding API Performance",
    "section": "\nG.2 Setting the enviroment",
    "text": "G.2 Setting the enviroment\n\nCodelibrary(leaflet)\nlibrary(targets)\nlibrary(tidygeocoder)\n\n\n\nCodesource(here::here(\"R\", \"get_qualocep_data.R\"))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Comparative Assessment of QualoCEP and Google Geocoding API Performance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-7.html#importing-qualocep-data",
    "href": "qmd/supplementary-material-7.html#importing-qualocep-data",
    "title": "Appendix G — Comparative Assessment of QualoCEP and Google Geocoding API Performance",
    "section": "\nG.3 Importing QualOCep data",
    "text": "G.3 Importing QualOCep data\n\nCodequalocep_data &lt;- get_qualocep_data()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Comparative Assessment of QualoCEP and Google Geocoding API Performance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-7.html#random-test-1",
    "href": "qmd/supplementary-material-7.html#random-test-1",
    "title": "Appendix G — Comparative Assessment of QualoCEP and Google Geocoding API Performance",
    "section": "\nG.4 Random test 1",
    "text": "G.4 Random test 1\n\nG.4.1 QualOCep data\n\nCodesample_data &lt;- dplyr::sample_n(qualocep_data, 1)\n\nsample_data |&gt; as.list() |&gt; rutils:::list_as_tibble()\n\n\n  \n\n\n\n\nCodeleaflet::leaflet() |&gt;\n  leaflet::addTiles() |&gt;\n  leaflet::addMarkers(\n    lng = sample_data$longitude,\n    lat = sample_data$latitude,\n    popup = paste0(\n      \"Latitude: \", sample_data$latitude, \"&lt;br&gt;\",\n      \"Longitude: \", sample_data$longitude\n    )\n  )\n\n\n\n\n\n\nG.4.2 Google Geocoding API\n\nCodegoogle_data &lt;-\n  sample_data |&gt;\n  dplyr::mutate(\n    address = orbis::render_brazil_address(\n      street = sample_data$street,\n      complement = sample_data$complement,\n      neighborhood = sample_data$neighborhood,\n      municipality = sample_data$municipality,\n      state = sample_data$state,\n      postal_code = sample_data$postal_code\n    )\n  ) |&gt;\n  tidygeocoder::geocode(\n    address = address,\n    method = \"google\"\n  )\n#&gt; Passing 1 address to the Google single address geocoder\n#&gt; Query completed in: 0.7 seconds\n\ngoogle_data |&gt; as.list() |&gt; rutils:::list_as_tibble()\n\n\n  \n\n\n\n\nCodeleaflet::leaflet() |&gt;\n  leaflet::addTiles() |&gt;\n  leaflet::addMarkers(\n    lng = google_data$longitude,\n    lat = google_data$latitude,\n    popup = paste0(\n      \"Latitude: \", google_data$latitude, \"&lt;br&gt;\",\n      \"Longitude: \", google_data$longitude\n    )\n  )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Comparative Assessment of QualoCEP and Google Geocoding API Performance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-7.html#random-test-2",
    "href": "qmd/supplementary-material-7.html#random-test-2",
    "title": "Appendix G — Comparative Assessment of QualoCEP and Google Geocoding API Performance",
    "section": "\nG.5 Random test 2",
    "text": "G.5 Random test 2\n\nG.5.1 QualOCep data\n\nCodesample_data &lt;- dplyr::sample_n(qualocep_data, 1)\n\nsample_data |&gt; as.list() |&gt; rutils:::list_as_tibble()\n\n\n  \n\n\n\n\nCodeleaflet::leaflet() |&gt;\n  leaflet::addTiles() |&gt;\n  leaflet::addMarkers(\n    lng = sample_data$longitude,\n    lat = sample_data$latitude,\n    popup = paste0(\n      \"Latitude: \", sample_data$latitude, \"&lt;br&gt;\",\n      \"Longitude: \", sample_data$longitude\n    )\n  )\n\n\n\n\n\n\nG.5.2 Google Geocoding API\n\nCodegoogle_data &lt;-\n  sample_data |&gt;\n  dplyr::mutate(\n    address = orbis::render_brazil_address(\n      street = sample_data$street,\n      complement = sample_data$complement,\n      neighborhood = sample_data$neighborhood,\n      municipality = sample_data$municipality,\n      state = sample_data$state,\n      postal_code = sample_data$postal_code\n    )\n  ) |&gt;\n  tidygeocoder::geocode(\n    address = address,\n    method = \"google\"\n  )\n#&gt; Passing 1 address to the Google single address geocoder\n#&gt; Query completed in: 0.2 seconds\n\ngoogle_data |&gt; as.list() |&gt; rutils:::list_as_tibble()\n\n\n  \n\n\n\n\nCodeleaflet::leaflet() |&gt;\n  leaflet::addTiles() |&gt;\n  leaflet::addMarkers(\n    lng = google_data$longitude,\n    lat = google_data$latitude,\n    popup = paste0(\n      \"Latitude: \", google_data$latitude, \"&lt;br&gt;\",\n      \"Longitude: \", google_data$longitude\n    )\n  )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Comparative Assessment of QualoCEP and Google Geocoding API Performance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-7.html#random-test-3",
    "href": "qmd/supplementary-material-7.html#random-test-3",
    "title": "Appendix G — Comparative Assessment of QualoCEP and Google Geocoding API Performance",
    "section": "\nG.6 Random test 3",
    "text": "G.6 Random test 3\n\nG.6.1 QualOCep data\n\nCodesample_data &lt;- dplyr::sample_n(qualocep_data, 1)\n\nsample_data |&gt; as.list() |&gt; rutils:::list_as_tibble()\n\n\n  \n\n\n\n\nCodeleaflet::leaflet() |&gt;\n  leaflet::addTiles() |&gt;\n  leaflet::addMarkers(\n    lng = sample_data$longitude,\n    lat = sample_data$latitude,\n    popup = paste0(\n      \"Latitude: \", sample_data$latitude, \"&lt;br&gt;\",\n      \"Longitude: \", sample_data$longitude\n    )\n  )\n\n\n\n\n\n\nG.6.2 Google Geocoding API\n\nCodegoogle_data &lt;-\n  sample_data |&gt;\n  dplyr::mutate(\n    address = orbis::render_brazil_address(\n      street = sample_data$street,\n      complement = sample_data$complement,\n      neighborhood = sample_data$neighborhood,\n      municipality = sample_data$municipality,\n      state = sample_data$state,\n      postal_code = sample_data$postal_code\n    )\n  ) |&gt;\n  tidygeocoder::geocode(\n    address = address,\n    method = \"google\"\n  )\n#&gt; Passing 1 address to the Google single address geocoder\n#&gt; Query completed in: 0.2 seconds\n\ngoogle_data |&gt; as.list() |&gt; rutils:::list_as_tibble()\n\n\n  \n\n\n\n\nCodeleaflet::leaflet() |&gt;\n  leaflet::addTiles() |&gt;\n  leaflet::addMarkers(\n    lng = google_data$longitude,\n    lat = google_data$latitude,\n    popup = paste0(\n      \"Latitude: \", google_data$latitude, \"&lt;br&gt;\",\n      \"Longitude: \", google_data$longitude\n    )\n  )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Comparative Assessment of QualoCEP and Google Geocoding API Performance</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-8.html",
    "href": "qmd/supplementary-material-8.html",
    "title": "Appendix H — Overview of General Linear Models",
    "section": "",
    "text": "H.1 Overview\nThis document presents fundamental concepts and assumptions of general linear models, serving as a reference for model diagnostics in the supplementary materials.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Overview of General Linear Models</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-8.html#definitions",
    "href": "qmd/supplementary-material-8.html#definitions",
    "title": "Appendix H — Overview of General Linear Models",
    "section": "\nH.2 Definitions",
    "text": "H.2 Definitions\n\nDefinition H.1 (Response/Predictor/Regression) The variables \\(X_{1}, \\dots, \\text{X}_{k}\\) are called predictors, and the random variable \\(Y\\) is called the response. The conditional expectation of \\(Y\\) for given values \\(x_{1}, \\dots, \\text{x}_{k}\\) of \\(X_{1}, \\dots, \\text{X}_{k}\\) (\\(E(Y|x_{1}, \\dots, \\text{x}_{k})\\)) is called the regression function of \\(Y\\) on \\(X_{1}, \\dots, \\text{X}_{k}\\), or simply the regression of \\(Y\\) on \\(X_{1}, \\dots, \\text{X}_{k}\\). (DeGroot & Schervish, 2012, p. 699)\n\n\nDefinition H.2 (Linear Regression) A regression function (\\(E(Y|x_{1}, \\dots, \\text{x}_{k})\\)) in the form of a linear function of the the parameters \\(\\beta_{0}, \\dots, \\beta_{k}\\), having the following form: (DeGroot & Schervish, 2012, pp. 699–700)\n\\[\nE(Y|x_{1}, \\dots, \\text{x}_{k}) = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{k} x_{k}\n\\tag{H.1}\\]\n\n\nDefinition H.3 (Regression Coefficients) The coefficients \\(\\beta_{0}, \\dots, \\beta_{k}\\) in Equation H.1 are called regression coefficients (DeGroot & Schervish, 2012, pp. 699–700).\n\n\nDefinition H.4 (Simple Linear Regression) A regression of \\(Y\\) on just a single variable \\(X\\). For each value \\(X = x\\), the random variable \\(Y\\) can be represented in the form \\(Y = \\beta_{0} + \\beta_{1}x + \\epsilon\\), where \\(\\epsilon\\) is a random variable that has the normal distribution with mean \\(0\\) and variance \\(\\sigma^{2}\\). It follows from this assumption that the conditional distribution of \\(Y\\) given \\(X = x\\) is the normal distribution with mean \\(\\beta_{0} + \\beta_{1}x\\) and variance \\(\\sigma^{2}\\) (DeGroot & Schervish, 2012, p. 700).\n\n\nDefinition H.5 (Multiple Linear Regression) A linear regression of \\(Y\\) on \\(k\\) variables \\(X_{1}, \\dots, X_{k}\\), rather than on just a single variable \\(X\\). In a problem of multiple linear regressions, we obtain \\(n\\) vectors of observations (\\(x_{i1}. \\dots, x_{ik}, Y_{i}\\)), for \\(i = 1, \\dots, n\\). Here \\(x_{ij}\\) is the observed value of the variable \\(X_{j}\\) for the \\(i\\)th observation. The \\(E(Y_{i})\\) is given by the relation: (DeGroot & Schervish, 2012, p. 738)\n\\[\nE(Y_{i}) = \\beta_{0} + \\beta_{1} x_{i1} + \\dots + \\beta_{k} x_{ik}\n\\tag{H.2}\\]\n\n\nDefinition H.6 (Residuals/Fitted Values) For \\(i = 1, \\dots, n\\), the observed values of \\(\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\) are called fitted values. For \\(i = 1, \\dots, n\\), the observed values of \\(e_{i} = y_{i} - \\hat{y}_{i}\\) are called residuals (DeGroot & Schervish, 2012, p. 717).\n\n\nDefinition H.7 (General Linear Model) The statistical model in which the observations \\(Y_{1}, \\dots, Y_{n}\\) satisfy the following assumptions (DeGroot & Schervish, 2012, p. 738).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Overview of General Linear Models</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-8.html#assumptions",
    "href": "qmd/supplementary-material-8.html#assumptions",
    "title": "Appendix H — Overview of General Linear Models",
    "section": "\nH.3 Assumptions",
    "text": "H.3 Assumptions\nHere we shall assume that each observation \\(Y_{1}, \\dots, Y_{n}\\) has a normal distribution, that the observations \\(Y_{1}, \\dots, Y_{n}\\) are independent, and that the observations \\(Y_{1}, \\dots, Y_{n}\\) have the same variance \\(\\sigma^{2}\\). Instead of a single predictor being associated with each \\(Y_{i}\\), we assume that a \\(p\\)-dimensional vector \\(z_{i} = (z_{i0}, \\dots, z_{ip - 1})\\) is associated with each \\(Y_{i}\\) (DeGroot & Schervish, 2012, p. 736).\n\nAssumption 1\n\nPredictor is known. Either the vectors \\(z_{1}, \\dots , z_{n}\\) are known ahead of time, or they are the observed values of random vectors \\(Z_{1}, \\dots , Z_{n}\\) on whose values we condition before computing the joint distribution of (\\(Y_{1}, \\dots , Y_{n}\\)) (DeGroot & Schervish, 2012, p. 736).\n\nAssumption 2\n\nNormality. For \\(i = 1, \\dots, n\\), the conditional distribution of \\(Y_{i}\\) given the vectors \\(z_{1}, \\dots , z_{n}\\) is a normal distribution (DeGroot & Schervish, 2012, p. 737).\n\n\n(Normality of the error term distribution (Hair, 2019, p. 287))\n\nAssumption 3\n\nLinear mean. There is a vector of parameters \\(\\beta = (\\beta_{0}, \\dots, \\beta_{p - 1})\\) such that the conditional mean of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) has the form\n\n\n\\[\nz_{i0} \\beta_{0} + z_{i1} \\beta_{1} + \\cdots + z_{ip - 1} \\beta_{p - 1}\n\\]\nfor \\(i = 1, \\dots, n\\) (DeGroot & Schervish, 2012, p. 737).\n(Linearity of the phenomenon measured (Hair, 2019, p. 287))\n\n\n\n\n\n\nIt is important to clarify that the linear assumption pertains to linearity of the parameters or equivalently, linearity of the coefficients. This means that each predictor is multiplied by its corresponding regression coefficient. However, this does not necessarily imply that the relationship between the predictors and the response variable is linear. In fact, a linear model can still effectively capture non-linear relationships between predictors and the response variable by utilizing transformations of the predictors (Cohen et al., 2002, p. 195).\n\n\n\n\nAssumption 4\n\nCommon variance (homoscedasticity). There is as parameter \\(\\sigma^{2}\\) such the conditional variance of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) is \\(\\sigma^{2}\\) for \\(i = 1, \\dots, n\\).\n\n\n(Constant variance of the error terms (Hair, 2019, p. 287))\n\nAssumption 5\n\nIndependence. The random variables \\(Y_{1}, \\dots , Y_{n}\\) are independent given the observed \\(z_{1}, \\dots , z_{n}\\) (DeGroot & Schervish, 2012, p. 737).\n\n\n(Independence of the error terms (Hair, 2019, p. 287))\n\n\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2002). Applied multiple regression/correlation analysis for the behavioral sciences (OCLC: ocm49903199) (3rd ed.). Lawrence Erlbaum Associates.\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and statistics (OCLC: ocn502674206) (4th ed.). Addison-Wesley.\n\n\nHair, J. F. (2019). Multivariate data analysis (8th ed.). Cengage.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Overview of General Linear Models</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-9.html",
    "href": "qmd/supplementary-material-9.html",
    "title": "Appendix I — Critique of Leocadio-Miguel et al. (2017)",
    "section": "",
    "text": "I.1 Overview\nThis document presents some constructive critiques regarding Leocadio-Miguel et al. (2017) article, which this thesis respond to. It is made with the goal of add to the discussion of the latitude hypothesis and to to enhance understanding of the data depicted and highlight how it may lead to potential misinterpretations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Critique of Leocadio-Miguel et al. (2017)</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-9.html#inconsistent-data",
    "href": "qmd/supplementary-material-9.html#inconsistent-data",
    "title": "Appendix I — Critique of Leocadio-Miguel et al. (2017)",
    "section": "\nI.2 Inconsistent Data",
    "text": "I.2 Inconsistent Data\n\nWe conducted a first regression of HO scores only on the covariates (age, longitude, and solar irradiation when the subjects filled the online questionnaire) and cofactors (sex, daylight saving time (DST), and season) (\\(\\text{R}^{2} = 0.05964\\), \\(\\text{F}(8,12845) = 101.8\\), \\(p &lt; 2.2e-16\\)) (Leocadio-Miguel et al., 2017).\n\nThe \\(\\text{F}\\)-test indicates \\(8\\) predictors, yet only \\(6\\) are mentioned in the text. This discrepancy is not clarified, leaving uncertainty about the additional predictors included in the model.\n\n[continuing from the last quote] … and a second regression including the annual average solar irradiation, sunrise time, sunset time and daylight duration in March equinox, June, and December solstices for each volunteer (\\(\\text{R}^{2} = 0.06667\\), \\(\\text{F}(27,12853) = 33.8\\), \\(p &lt; 2.2e-16\\)) (Leocadio-Miguel et al., 2017).\n\nIn this case, the \\(\\text{F}\\)-test indicates \\(27\\) predictors. However, adding the \\(8\\) predictors from the first regression to the \\(10\\) newly mentioned variables yields a total of \\(18\\) predictors, not \\(27\\). This inconsistency is not addressed in the text.\n\nTesting for nested models, we obtained a significant reduction of residual sum of squares (\\(\\text{F}(2,12884) = 31.983\\), \\(p = 1.395e-14\\)), […] (Leocadio-Miguel et al., 2017).\n\nThe degrees of freedom difference (\\(df = 2\\)) suggests that the two previously described models were not directly compared in this nested test. This raises further questions about the models used, as the discrepancy is not explained.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Critique of Leocadio-Miguel et al. (2017)</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-9.html#statistic-ritual-or-lack-of-evidence",
    "href": "qmd/supplementary-material-9.html#statistic-ritual-or-lack-of-evidence",
    "title": "Appendix I — Critique of Leocadio-Miguel et al. (2017)",
    "section": "\nI.3 Statistic Ritual (or Lack of Evidence)",
    "text": "I.3 Statistic Ritual (or Lack of Evidence)\n\nWe also obtained an effect size estimate of Cohen’s \\(f^{2} = (0.06352 - 0.05964) / (1 - 0.06352) = 0.004143174\\) (Leocadio-Miguel et al., 2017).\n\nA first thing to notice is that the higher \\(\\text{R}^{2}\\) value (\\(0.06352\\)) differs from the previously reported \\(\\text{R}^{2}\\) (\\(0.06667\\)). This inconsistency is not justified.\nThe fomula that appears in the text is based on Cohen’s \\(f^{2}\\), which is calculated by the following:\n\\[\n\\text{Cohen's } f^2 = \\cfrac{\\text{R}^{2}}{1 - \\text{R}^{2}}\n\\]\nFor nested models, this can be adapted as:\n\\[\n\\text{Cohen's } f^2 = \\cfrac{\\text{R}^{2}_{f} - \\text{R}^{2}_{r}}{1 - \\text{R}^{2}_{f}} = \\cfrac{\\Delta \\text{R}^{2}}{1 - \\text{R}^{2}_{f}}\n\\]\nBefore diving into the \\(f^2\\) calculation, let’s first calculate the confidence interval of the \\(\\text{R}^{2}\\) for the restricted and full model using an estimate of the standard error. Note that all the parameters used in the calculations are provided in the text.\n\nadj_r_squared_restricted &lt;- rutils:::summarize_r2(\n  r2 = 0.05964,\n  n = 12845 + 8 + 1,\n  k = 8,\n  ci_level = 0.95,\n  rules = \"cohen1988\"\n)\n\nadj_r_squared_restricted\n\n\n  \n\n\n\n\nadj_r_squared_full &lt;- rutils:::summarize_r2(\n  r2 = 0.06352,\n  n = 12853 + 27 + 1,\n  k = 27,\n  ci_level = 0.95,\n  rules = \"cohen1988\"\n)\n\nadj_r_squared_full\n\n\n  \n\n\n\n\nCodedplyr::tibble(\n  name = c(\n    \"adj_r_squared_res\",\n    \"adj_r_squared_full\",\n    \"diff\"\n  ),\n  value = c(\n    adj_r_squared_restricted$value[1],\n    adj_r_squared_full$value[1],\n    adj_r_squared_full$value[1] - adj_r_squared_restricted$value[1]\n  )\n)\n\n\n  \n\n\n\nBased on this data, it’s also possible to estimate the confidence interval for the \\(\\Delta \\text{R}^{2}\\).\n\nCoderutils:::cohens_f_squared_summary(\n  base_r_squared = adj_r_squared_restricted,\n  new_r_squared = adj_r_squared_full\n) |&gt;\n  rutils:::list_as_tibble()\n\n\n  \n\n\n\nAs we can see, the authors found an negligible effect size and, considering its confidence interval, it goes to 0. Latitude-related predictors explained only \\(0.388\\%\\) of the variability (\\((0.06352 - 0.05964) \\times 100\\)). Nonetheless, the authors interpreted this result as evidence of an association.\nAt this point the reader could ask: “So why the F-test was significant?”\n\nTesting for nested models, we obtained a significant reduction of residual sum of squares (\\(\\text{F}(2,12884) = 31.983\\), \\(p = 1.395e-14\\)), […] (Leocadio-Miguel et al., 2017).\n\nThe significant p-value found in their F-test for nested models indicate a difference only because the p-value is very sensitive to sample size (\\(12,884\\), in their case) (Figure I.1). This is a common issue in large samples (commoly called the “p-value problem”), where even small differences (e.g., a difference of \\(0.00001\\)) can be detected as significant (Lin et al., 2013).\n\n\nFigure I.1: Comparison of a 95% of confidence level (\\(\\alpha = 0.05\\)) and an n-dependent p-value curve. The parameter \\(n_{\\alpha}\\) represents the minimum sample size to detect statistically significant differences among compared groups. The parameter \\(n_{\\gamma}\\) represents the convergence point of the p-value curve. When the p-value curve expresses practical differences, the area under the red curve (\\(A_{p(n)}\\)) is smaller than the area under the constant function \\(\\alpha = 0.05\\) (\\(A_{\\alpha = 0.05}\\)) when it is evaluated between \\(0\\) and \\(n_{\\gamma}\\).\n\n\nSource: Reproduced from Gómez-de-Mariscal et al. (2021, Figure 3).\n\n\n\nA p-value don not have any pratical significance, that is why the effect size is important. As Cohen put:\n\n[…] in many circumstances, all that is intended by “proving” the null hypothesis is that the ES [Effect Size] is not necessarily zero but small enough to be negligible […]. (p. 461).\n\nAuthors that rely only in the p-value indicate a statistical ritual other than a statistical thinking (Gigerenzer, 2004).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Critique of Leocadio-Miguel et al. (2017)</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-9.html#model-issues",
    "href": "qmd/supplementary-material-9.html#model-issues",
    "title": "Appendix I — Critique of Leocadio-Miguel et al. (2017)",
    "section": "\nI.4 Model Issues",
    "text": "I.4 Model Issues\n\nWe tested the residuals of the second regression with Kolmogorov-Smirnov normality test and obtained a significant result (\\(\\text{D} = 0.42598\\), \\(p &lt; 2.2e-16\\)) (Leocadio-Miguel et al., 2017).\n\nThis result violates the assumptions of linear regression and it’s not addressed in the text.\nObjective assumption tests (e.g., Anderson–Darling test) is not advisable for larger samples, since they can be overly sensitive to minor deviations. Additionally, they might overlook visual patterns that are not captured by a single metric (Kozak & Piepho, 2018; Schucany & Ng, 2006; Shatz, 2024).\nNevertheless, the authors did not mention this in the text.\n\nThe irradiation (W/m2) data by latitude was acquired from the NASA website (http://aom.giss.nasa.gov). We retrieved monthly average data and calculated the annual mean for each latitude degree (Leocadio-Miguel et al., 2017).\n\nThe type of irradiation (e.g., global, direct, or diffuse) was not specified, nor was it included in the supplementary materials. This omission is crucial, as different irradiation types can yield substantially different results.\n\n[…] a second regression including the annual average solar irradiation, sunrise time, sunset time and daylight duration in March equinox, June, and December solstices for each volunteer (Leocadio-Miguel et al., 2017).\n\nThe inclusion of sunrise time, sunset time, and daylight duration (sunset - sunrise) likely introduced multicollinearity due to their inherent correlation. The study does not acknowledge or address this potential issue.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Critique of Leocadio-Miguel et al. (2017)</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-9.html#misleading-visualization",
    "href": "qmd/supplementary-material-9.html#misleading-visualization",
    "title": "Appendix I — Critique of Leocadio-Miguel et al. (2017)",
    "section": "\nI.5 Misleading Visualization",
    "text": "I.5 Misleading Visualization\nThe Figure 2 shown in Leocadio-Miguel et al. (2017) provides a good example of how data can be misleading. This Figure was shown many times in presentations in chronobiology symposiums, without a proper care of understanding what it was showing.\nTo interpret this figure, it is essential to first comprehend the scale it represents. In Leocadio-Miguel et al. (2017), the authors used the Horne & Östberg (HO) chronotype scale (Horne & Östberg, 1976) (Figure I.2). This scale, one of the first developed to assess chronotype, is a self-report questionnaire designed to determine an individual’s preferred time of day for activities such as waking up, going to bed, and moment of peak performance. The HO scale consists of \\(19\\) items, with a total score ranging from \\(16\\) to \\(86\\). Lower scores indicate greater evening orientation, while higher scores reflect stronger morning orientation(Table I.1).\nCodedplyr::tibble(\n  Category = c(\n    \"Definitely morning type\",\n    \"Moderately morning type\",\n    \"Neither type\",\n    \"Moderately evening type\",\n    \"Definitely evening type\"\n  ),\n  Score = c(\n    \"70-86\",\n    \"59-69\",\n    \"42-58\",\n    \"31-41\",\n    \"16-30\"\n  )\n)\n\n\nTable I.1: Classifications of the Horne & Östberg Morningness-Eveningness scale with the corresponding scores.\n\n\n\n\n  \n\n\n\nSource: Adapted from Horne & Östberg (1976).\n\n\n\n\n\nFigure I.2: Distribution of HO scores for the entire sample of Leocadio-Miguel et al. (2017) data. Each bar for each score represents the frequency (number of subjects). Low scores represent evening types and high scores morning types.\n\n\nSource: Reproduced from Leocadio-Miguel et al. (2017, Figure 1).\n\n\n\n\nI.5.1 The Figure\nOne of the key issues with the data presented in Leocadio-Miguel et al. (2017) is that it relies on negligible associations, yet the authors present them in a way that suggests a strong effect. The article’s Figure 2 (Figure I.3) illustrates this. At first glance, it appears to show a good fit and a strong relationship between latitude and the chronotype proxy (in this case, the HO score). However, a closer analysis reveals that it is a clear example of the y-axis illusion.\n\n\nFigure I.3: Mean HO scores (±SE) along a latitudinal cline, showing corresponding annual average of solar irradiation level (W/m2).\n\n\nSource: Reproduced from Leocadio-Miguel et al. (2017, Figure 2).\n\n\n\nIn the following sections, I reproduce the data from the figure to provide a clearer representation of the actual relationship reported by the authors.\n\nI.5.2 Latitude Estimation (Upper x-Axis)\nAs shown in Figure I.4, the latitude does not follow a fixed scale. Instead, it is referenced relative to the second x-axis (solar irradiation). However, it is possible to estimate the latitude values reasonably well by measuring the pixel distances between points on the figure.\n\n\nFigure I.4: Figure 2 from Leocadio-Miguel et al. (2017) with guides for comparison.\n\n\nSource: Adapted by the author from Leocadio-Miguel et al. (2017, Figure 2).\n\n\n\nInterval distances\n\n\n\\(0\\text{-}13\\): \\(87\\) pixels. \\(87 / 13\\): 1 point \\(\\approx 6.692308\\) pixels.\n\n\\(13\\text{-}18\\): \\(88\\) pixels. \\(88 / (18 - 13)\\): \\(1\\) point \\(= 17.6\\) pixels.\n\n\\(18\\text{-}22\\): \\(87\\) pixels. \\(87 / (22 - 18)\\): \\(1\\) point \\(= 21.75\\) pixels.\n\n\\(22\\text{-}26\\): \\(87\\) pixels. \\(87 / (26 - 22)\\): \\(1\\) point \\(= 21.75\\) pixels.\n\n\\(26\\text{-}29\\): \\(88\\) pixels. \\(88 / (29 - 26)\\): \\(1\\) point \\(\\approx 29.333\\) pixels.\n\n\\(29\\text{-}32\\): \\(86\\) pixels. \\(86 / (32 - 29)\\): \\(1\\) point \\(\\approx 28.667\\) pixels.\n\nEstimation\n\n\npoint: \\(0\\) to point: \\(42\\) pixels. Estimation: \\(42 / (87 / 13) \\approx 6.275862\\).\n\n\npoint: \\(13\\) to point: \\(44\\) pixels. Estimation: \\(13 + (44 / (88 / (18 - 13))) = 15.5\\).\n\n\npoint: \\(18\\) to point: \\(43\\) pixels. Estimation: \\(18 + (43 / (87 / (22 - 18))) \\approx 19.97701\\).\n\n\npoint: \\(22\\) to point: \\(44\\) pixels. Estimation: \\(22 + (44 / (87 / (26 - 22))) \\approx 24.02299\\).\n\n\npoint: \\(26\\) to point: \\(44\\) pixels. Estimation: \\(26 + (44 / (87 / (29 - 26))) \\approx 27.51724\\).\n\n\npoint: \\(29\\) to point: \\(44\\) pixels. Estimation: \\(29 + (44 / (86 / (32 - 29))) \\approx 30.53488\\).\n\n\nI.5.3 HO Estimation (y-Axis)\nThe HO value scale is fixed, allowing the values to be accurately determined by measuring the pixel distances.\nPixel/point\n\n\n\\(44.5\\text{-}48.5\\): \\(402\\) pixels. \\(402 / (48.5 - 44.5)\\): \\(1\\) point \\(= 100.5\\) pixels.\n\nEstimation\n\n\npoint: \\(44.5\\) to point: \\(338\\) pixels. Estimation: \\(44.5 + (338 / (402 / (48.5 - 44.5))) \\approx 47.86318\\).\n\n\npoint: \\(44.5\\) to point: \\(316\\) pixels. Estimation: \\(44.5 + (316 / (402 / (48.5 - 44.5))) \\approx 47.64428\\).\n\n\npoint: \\(44.5\\) to point: \\(217\\) pixels. Estimation: \\(44.5 + (217 / (402 / (48.5 - 44.5))) \\approx 46.6592\\).\n\n\npoint: \\(44.5\\) to point: \\(155\\) pixels. Estimation: \\(44.5 + (155 / (402 / (48.5 - 44.5))) \\approx 46.04229\\).\n\n\npoint: \\(44.5\\) to point: \\(94\\) pixels. Estimation: \\(44.5 + (94 / (402 / (48.5 - 44.5))) \\approx 45.43532\\).\n\n\npoint: \\(44.5\\) to point: \\(94\\) pixels. Estimation: \\(44.5 + (94 / (402 / (48.5 - 44.5))) \\approx 45.43532\\).\n\n\nI.5.4 Standard error estimation (y-axis)\nThe standard error (SE) is displayed on the Y-axis, which has a fixed scale. Therefore, the values can be accurately determined by measuring the pixel distances.\nPixel/point\n\n\n\\(44.5\\text{-}48.5\\): \\(402\\) pixels. \\(402 / (48.5 - 44.5)\\): \\(1\\) point \\(= 100.5\\) pixels.\n\nEstimation\n\n\npoint SE: \\(44.5\\) to upper SE: \\(369\\) pixels. Estimation: \\(44.5 + (369 / (402 / (48.5 - 44.5))) \\approx 48.17164\\).\n\n\npoint SE: \\(44.5\\) to upper SE: \\(357\\) pixels. Estimation: \\(44.5 + (357 / (402 / (48.5 - 44.5))) \\approx 48.05224\\).\n\n\npoint SE: \\(44.5\\) to upper SE: \\(240\\) pixels. Estimation: \\(44.5 + (240 / (402 / (48.5 - 44.5))) \\approx 46.88806\\)\n\n\n\npoint SE: \\(44.5\\) to upper SE: \\(176\\) pixels. Estimation: \\(44.5 + (176 / (402 / (48.5 - 44.5))) \\approx 46.25124\\)\n\n\n\npoint SE: \\(44.5\\) to upper SE: \\(134\\) pixels. Estimation: \\(44.5 + (134 / (402 / (48.5 - 44.5))) \\approx 45.83333\\)\n\n\n\npoint SE: \\(44.5\\) to upper SE: \\(131\\) pixels. Estimation: \\(44.5 + (131 / (402 / (48.5 - 44.5))) \\approx 45.80348\\)\n\n\n\nI.5.5 The Extracted Data\nBy following the processes described above, we can obtained a reliable approximation of the data, as shown below.\nCodedata &lt;-\n  dplyr::tibble(\n    ho = c(\n      44.5 + (338 / (402 / (48.5 - 44.5))),\n      44.5 + (316 / (402 / (48.5 - 44.5))),\n      44.5 + (217 / (402 / (48.5 - 44.5))),\n      44.5 + (155 / (402 / (48.5 - 44.5))),\n      44.5 + (94 / (402 / (48.5 - 44.5))),\n      44.5 + (94 / (402 / (48.5 - 44.5)))\n    ),\n    latitude = c(\n      42 / (87 / 13),\n      13 + (44 / (88 / (18 - 13))),\n      18 + (43 / (87 / (22 - 18))),\n      22 + (44 / (87 / (26 - 22))),\n      26 + (44 / (87 / (29 - 26))),\n      29 + (44 / (86 / (32 - 29)))\n    )\n  ) |&gt;\n  dplyr::mutate(\n    se_upper = c(\n      44.5 + (369 / (402 / (48.5 - 44.5))),\n      44.5 + (357 / (402 / (48.5 - 44.5))),\n      44.5 + (240 / (402 / (48.5 - 44.5))),\n      44.5 + (176 / (402 / (48.5 - 44.5))),\n      44.5 + (134 / (402 / (48.5 - 44.5))),\n      44.5 + (131 / (402 / (48.5 - 44.5)))\n    ),\n    se = se_upper - ho\n  )\n\ndata\n\n\nTable I.2: Data extracted from Figure 2 present in Leocadio-Miguel et al. (2017).\n\n\n\n\n  \n\n\n\nSource: Created by the author, based on a data extracted from Leocadio-Miguel et al. (2017, Figure 2).\n\n\n\n\nI.5.6 Ploting the Extracted Data\nFirst, let’s examine the data in a format similar to what is shown in the Leocadio-Miguel et al. (2017) figure.\nNote that here, the x-axis follows a fixed scale, which is crucial to avoid any distortions.\nCodedata |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = latitude, y = ho)\n  ) +\n  ggplot2::geom_point(shape = 15, size = 3) +\n  ggplot2::geom_errorbar(\n    ggplot2::aes(\n      x = latitude,\n      y = ho,\n      ymin = ho - se,\n      ymax= ho + se\n    ),\n    data = data,\n    show.legend = FALSE,\n    inherit.aes = FALSE,\n    width = 0.5,\n    linewidth = 0.5\n  ) +\n  ggplot2::geom_smooth(\n    method = \"lm\",\n    formula = y ~ x,\n    se = FALSE,\n    color = brandr::get_brand_color(\"orange\"),\n    linewidth = 2\n  ) +\n  ggplot2::scale_x_continuous(\n    breaks = c(0, 13, 18, 22, 26, 29, 32),\n    labels = c(\"0º\", \"13º\", \"18º\", \"22º\", \"26º\", \"29º\", \"32º\"),\n    limits = c(0, 32)\n  ) +\n  ggplot2::labs(\n    x = \"Latitude (Degrees South)\",\n    y = \"HO Score\"\n  )\n\n\nFigure I.5: Reproduction of Figure 2 from Leocadio-Miguel et al. (2017) with a fixed x-axis scale.\n\n\n\n\n\n\n\n\n\nSource: Created by the author, based on a data extracted from Leocadio-Miguel et al. (2017, Figure 2).\n\n\n\nNow, it is possible to adjust the y-axis to reflect the minimum and maximum values of the HO scale.\nCodedata |&gt;\n  ggplot2::ggplot(\n    ggplot2::aes(x = latitude, y = ho)\n  ) +\n  ggplot2::geom_point(shape = 15, size = 3) +\n  ggplot2::geom_errorbar(\n    ggplot2::aes(\n      x = latitude,\n      y = ho,\n      ymin = ho - se,\n      ymax= ho + se\n    ),\n    data = data,\n    show.legend = FALSE,\n    inherit.aes = FALSE,\n    width = 0.5,\n    linewidth = 0.5\n  ) +\n  ggplot2::geom_smooth(\n    method = \"lm\",\n    formula = y ~ x,\n    se = FALSE,\n    color = brandr::get_brand_color(\"orange\")\n  ) +\n  ggplot2::scale_x_continuous(\n    breaks = c(0, 13, 18, 22, 16, 29, 32),\n    labels = c(\"0º\", \"13º\", \"18º\", \"22º\", \"26º\", \"29º\", \"32º\"),\n    limits = c(0, 32)\n  ) +\n  ggplot2::scale_y_continuous(\n    breaks = seq(16, 86, 10),\n    limits = c(16, 86)\n  ) +\n  ggplot2::labs(\n    x = \"Latitude (Degrees South)\",\n    y = \"HO Score\"\n  )\n\n\nFigure I.6: Reproduction of Figure 2 from Leocadio-Miguel et al. (2017) with a fixed x-axis scale and adjusted y-axis scale.\n\n\n\n\n\n\n\n\n\nSource: Created by the author, based on a data extracted from Leocadio-Miguel et al. (2017, Figure 2).\n\n\n\nWith Figure I.6, it becomes clear that the data provided by the authors does not demonstrate a relevant effect size.\n\n\n\n\nGigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587–606. https://doi.org/10.1016/j.socec.2004.09.033\n\n\nGómez-de-Mariscal, E., Guerrero, V., Sneider, A., Jayatilaka, H., Phillip, J. M., Wirtz, D., & Muñoz-Barrutia, A. (2021). Use of the p-values as a size-dependent function to address practical differences when analyzing large datasets. Scientific Reports, 11(1, 1), 20942. https://doi.org/10.1038/s41598-021-00199-5\n\n\nHorne, J. A., & Östberg, O. (1976). A self-assessment questionnaire to determine morningness-eveningness in human circadian rhythms. International Journal of Chronobiology, 4(2), 97–110. https://www.ncbi.nlm.nih.gov/pubmed/1027738\n\n\nKozak, M., & Piepho, H.-P. (2018). What’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions. Journal of Agronomy and Crop Science, 204(1), 86–98. https://doi.org/10.1111/jac.12220\n\n\nLeocadio-Miguel, M. A., Louzada, F. M., Duarte, L. L., Areas, R. P., Alam, M., Freire, M. V., Fontenele-Araujo, J., Menna-Barreto, L., & Pedrazzoli, M. (2017). Latitudinal cline of chronotype. Scientific Reports, 7(1), 5437. https://doi.org/10.1038/s41598-017-05797-w\n\n\nLin, M., Lucas, H. C., & Shmueli, G. (2013). Research commentary—Too big to fail: Large samples and the p-value problem. Information Systems Research, 24(4), 906–917. https://doi.org/10.1287/isre.2013.0480\n\n\nSchucany, W. R., & Ng, H. K. T. (2006). Preliminary goodness-of-fit tests for normality do not validate the one-sample Student t. Communications in Statistics - Theory and Methods, 35(12), 2275–2286. https://doi.org/10.1080/03610920600853308\n\n\nShatz, I. (2024). Assumption-checking rather than (just) testing: The importance of visualization and effect size in statistical diagnostics. Behavior Research Methods, 56(2), 826–845. https://doi.org/10.3758/s13428-023-02072-x",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Critique of Leocadio-Miguel et al. (2017)</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html",
    "href": "qmd/supplementary-material-10.html",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "",
    "text": "J.1 Courses\nDuring the development of this thesis, several activities have been accomplished. These activities are important to note, as they demonstrate the path taken to arrive at this final document. The following sections detail some of them.\nThe following graduate courses from the University of São Paulo (USP) were completed during the first year of the Master’s Program.\nPlease note that the unfortunate C concept above happened in the same semester when the author broke relations with his former supervisor.\n44 discipline credits were completed. An additional 12 special credits, related to an article publication (See Viana-Mendes et al. (2023)), were requested and approved by the Graduate Program Coordination Commission (CCP) in accordance with program regulations. In total, 56 credits were earned. A minimum of 50 credits is required for the thesis defense.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#courses",
    "href": "qmd/supplementary-material-10.html#courses",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "",
    "text": "2022/2: SCX5000 - Mathematical and Computational Methods I (10 credits) (Concept: C).\n2022/2: SCX5002 - Complex Systems I (10 credits) (Concept: A).\n2023/1: SCX5001 - Mathematical and Computational Methods II (10 credits) (Concept: A).\n2023/1: SCX5017 - Introduction to Data Science (10 credits) (Concept: A).\n2023/1: EAH5001 - Pedagogic Preparation (4 credits) (Concept: A).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#teaching-internship",
    "href": "qmd/supplementary-material-10.html#teaching-internship",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.2 Teaching Internship",
    "text": "J.2 Teaching Internship\nScholarship students under the Coordination for the Improvement of Higher Education Personnel (CAPES) are required to participate in the Teaching Improvement Program (PAE). This internship occurred in the 2º semester of 2023.\nThe internship responsibilities entailed serving as an Assistant Professor for the undergraduate course ACH0042 - Problem-Based Learning II at USP. A comprehensive teaching plan was formulated during enrollment in the aforementioned graduate course EAH5001, and it is accessible through the following link.\n\nVartanian, D., Rodrigues Neto, C, & Bernardes, M. E. M. (2023). Plano de ensino: ACH0042 - Resolução de Problemas II. https://doi.org/10.13140/RG.2.2.33335.50086",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#publications",
    "href": "qmd/supplementary-material-10.html#publications",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.3 Publications",
    "text": "J.3 Publications\nThe following article was published during the development of this thesis.\n\nViana-Mendes, J., Benedito-Silva, A. A., Andrade, M. A. M., Vartanian, D., Gonçalves, B. da S. B., Cipolla-Neto, J., & Pedrazzoli, M. (2023). Actigraphic characterization of sleep and circadian phenotypes of PER3 gene VNTR genotypes. Chronobiology International. https://doi.org/10.1080/07420528.2023.2256858",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#translations",
    "href": "qmd/supplementary-material-10.html#translations",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.4 Translations",
    "text": "J.4 Translations\nAs a member and package developer of the rOpenSci Initiative, the author is actively contributing to the ongoing translation of the rOpenSci Developer Guide into Portuguese. The aim is to create a more inclusive environment for individuals in Brazil and other Portuguese-speaking countries when developing for the R programming language.\nThis endeavor is linked to the thesis, as the author’s membership in rOpenSci began with the creation of the {mctq} R package (listed below).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#conferences",
    "href": "qmd/supplementary-material-10.html#conferences",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.5 Conferences",
    "text": "J.5 Conferences\nAn abstract pertaining to the primary investigation was published and presented on a poster at the Sao Paulo School of Advanced Science on Ecology of Human Sleep and Biological Rhythms organized by the São Paulo Research Foundation (FAPESP). This international school hosted 100 participants, including students and young researchers, with a diverse representation of 50 individuals from various states within Brazil and an additional 50 from international backgrounds. The event was held from November 16 to 26, 2022.\n\nVartanian, D., & Pedrazzoli, M. (2022). Ecology of sleep and circadian phenotypes of the Brazilian population [Poster]. São Paulo Research Foundation; São Paulo School of Advanced Science on Ecology of Human Sleep and Biological Rhythms. https://doi.org/10.13140/RG.2.2.25343.07840\nIn the same semester (2022/2), the author also participated in USP’s International Symposium on Scientific and Technological Initiation (SIICUSP) as both an examiner and a participant. As a participant, the author presented a research abstract related to the {actverse} R package for actigraphy data analysis, as detailed in Matias et al. (2022) and Vartanian (n.d.). This project was conceived and developed by the author of this thesis and involved collaboration with two undergraduate students. Notably, this project achieved recognition, securing 2nd place in the category of Earth and Exact Sciences.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#research-compendium",
    "href": "qmd/supplementary-material-10.html#research-compendium",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.6 Research Compendium",
    "text": "J.6 Research Compendium\nThis thesis, along with all the accompanying research, is structured and organized within the research compendium provided below.\n\nVartanian, D. (2025). Is latitude associated with chronotype? [Research compendium]. https://doi.org/10.17605/OSF.IO/YGKTS",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#data-plans",
    "href": "qmd/supplementary-material-10.html#data-plans",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.7 Data Plans",
    "text": "J.7 Data Plans\nThis research has also produced and published the following data plan.\n\nVartanian, D. (2024). Is latitude associated with chronotype? [Data Management Plan]. DMPHub. https://doi.org/10.48321/D1DW8P",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#softwares",
    "href": "qmd/supplementary-material-10.html#softwares",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.8 Softwares",
    "text": "J.8 Softwares\nThe following R packages, Python package, Quarto format (being used to write this thesis), and Espanso package were developed during this thesis.\n\nVartanian, D. (n.d.). {entrainment}: A rule-based model of the 24h light/dark cycle entrainment phenomenon [Software, Python Package]. https://github.com/danielvartan/entrainment\n\nVartanian, D1. (n.d.). {mctq}: Munich ChronoType Questionnaire (MCTQ) tools [Software, R Package]. https://docs.ropensci.org/mctq (The package has already surpassed 9,000 downloads)\n\nVartanian, D. (n.d.). {brandr}: Brand identity management using brand.yml standard [Software, R package]. https://doi.org/10.32614/CRAN.package.brandr\n\nVartanian, D. (n.d.). {lubritime}: An extension for the lubridate package [Software, R package]. https://github.com/danielvartan/lubritime\n\nVartanian, D. (n.d.). {orbis}: Spatial data analysis tools [Software, R package]. https://danielvartan.github.io/orbis\n\nVartanian, D. (n.d.). {quartor}: Tools for the Quarto publishing system [Software, R package]. https://danielvartan.github.io/quartor\n\nVartanian, D. (n.d.). {lockr}: Easily encrypt/decrypt files [Software, R package]. https://github.com/danielvartan/lockr\n\nVartanian, D. (n.d.). {prettycheck}: Pretty assertive programming [Software, R package]. https://github.com/danielvartan/prettycheck\n\nVartanian, D. (n.d.). {groomr}: Tidy functions [Software, R package]. https://github.com/danielvartan/groomr\n\nVartanian, D. (n.d.). {plotr}: Plots made easy [Software, R package]. https://danielvartan.github.io/plotr\n\nVartanian, D. (n.d.). {abnt}: Quarto format for ABNT theses and dissertations [Software, LaTeX/R]. https://github.com/danielvartan/abnt\n\nVartanian, D. (n.d.). HTML color codes in hex and RGB formats for Espanso [Software, Espanso package]. https://hub.espanso.org/html-colors",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#exercises",
    "href": "qmd/supplementary-material-10.html#exercises",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.9 Exercises",
    "text": "J.9 Exercises\nSome important exercises made public during the development of this thesis are listed below.\n\nVartanian, D. (2024). Illustration of the Lotka–Volterra’s predator–prey model. https://danielvartan.github.io/lotka-volterra\n\nVartanian, D. (2024). Illustration of the Lorenz system. https://danielvartan.github.io/lorenz-system\n\nVartanian, D. (2024). Illustration of the SIR model. https://danielvartan.github.io/sir\n\nVartanian, D. (2024). A demonstration of general linear models using penguins. https://danielvartan.github.io/linear-models\n\nVartanian, D. (2024). Demonstrations of the central limit theorem. https://danielvartan.github.io/central-limit-theorem",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#other-projects",
    "href": "qmd/supplementary-material-10.html#other-projects",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "\nJ.10 Other Projects",
    "text": "J.10 Other Projects\nThe author is also currently working on the development of the two projects below.\nCarvalho, A. M. de, Klapka, C. S., Magalhães, A. R., Barbosa, B. B., Vartanian, D., Schettino, J. P. J., & Pereira, E. B. (2023). Global syndemic: The impact of anthropogenic climate change on the health and nutrition of children under five years old attended by Brazil’s public health system (SUS) [CNPq project, University of São Paulo]. https://doi.org/10.17605/OSF.IO/8W36C\nSales, A. R. V., Vartanian, D., Andrade, M. A. M., Pedrazzoli, M. (2023). Associations between the duration and quality of sleep in third-trimester pregnant women and the duration of labor [Master’s project, University of São Paulo]. https://doi.org/10.17605/OSF.IO/S4TBZ\n\n\n\n\nMatias, V. A., Serrano, C., Vartanian, D., Pedrazzoli, M., & Benedito-Silva, A. A. (2022, September 3). {actverse}: An R package for actigraphy data analysis [Poster]. 30th USP International Symposium of Undergraduate Research (SIICUSP), São Paulo. http://dx.doi.org/10.13140/RG.2.2.12760.16643\n\n\nVartanian, D. (n.d.). {actverse}: Tools for actigraphy data analysis [Computer software]. https://docs.ropensci.org/actverse/\n\n\nViana-Mendes, J., Benedito-Silva, A. A., Andrade, M. A. M., Vartanian, D., Gonçalves, B. da S. B., Cipolla-Neto, J., & Pedrazzoli, M. (2023). Actigraphic characterization of sleep and circadian phenotypes of PER3 gene VNTR genotypes. Chronobiology International, 40(9), 1244–1250. https://doi.org/10.1080/07420528.2023.2256858",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  },
  {
    "objectID": "qmd/supplementary-material-10.html#footnotes",
    "href": "qmd/supplementary-material-10.html#footnotes",
    "title": "Appendix J — Works and Activities Developed During the Thesis",
    "section": "",
    "text": "The package has already surpassed 9,000 downloads.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Works and Activities Developed During the Thesis</span>"
    ]
  }
]