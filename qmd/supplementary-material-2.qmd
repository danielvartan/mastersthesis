<!-- %:::% .common h1 begin %:::% -->
# Methods
<!-- %:::% .common h1 end %:::% -->

```{r}
#| label: setup
#| include: false

source(here::here("R", "_setup.R"))
```

## Overview

This document provides a detailed explanation of the methods and steps involved in building the models and testing the thesis hypothesis.

For a comprehensive review of the thesis question and hypothesis, please refer to other supplementary materials.

## Approach and procedure method

This study adopted the hypothetical-deductive method, also known as the method of conjecture and refutation [@popper1979, p. 164], to approach problem-solving. As a procedural method, it utilized an enhanced version of Null Hypothesis Significance Testing (NHST), grounded in the original Neyman-Pearson framework for data testing [@neyman1928; @neyman1928a; @perezgonzalez2015].

## Measurement instrument

Chronotypes were assessed using a sleep log based on the core version of the standard Munich ChronoType Questionnaire (MCTQ) [@roenneberg2003], a well-validated and widely applied self-report tool for measuring sleep-wake cycles and chronotypes [@roenneberg2019]. The MCTQ captures chronotype as a biological circadian phenotype, determined by the sleep-corrected midpoint of sleep (MSF~sc~) on work-free days, accounting for any potential sleep compensation due to sleep deficits on workdays [@roenneberg2012].

Participants completed an online questionnaire, which included the sleep log as well as sociodemographic (e.g., age, sex), geographic (e.g., full residential address), anthropometric (e.g., weight, height), and data on work or study routines. A sample version of the questionnaire, stored independently by the [Internet Archive](https://archive.org/) organization can be viewed at <https://web.archive.org/web/20171018043514/each.usp.br/gipso/mctq>.

::: {#fig-appendice-1-age-sex-chronotype-series}
![](images/mctq-figure-1.png)

Source: Create by the author.

Variables of the Munich ChronoType Questionnaire scale (a sleep log). In its standard version, these variables are collected in the context of workdays and work-free days. BT = Local time of going to bed. SPrep = Local time of preparing to sleep. SLat = Sleep latency *or* time to fall asleep after preparing to sleep. SO = Local time of sleep onset. SD = Sleep duration. MS = Local time of mid-sleep. SE = Local time of sleep. Alarm = A logical value indicating if the respondent uses an alarm clock to wake up. SE = Local time of sleep end. SI = "Sleep inertia" (despite the name, this variable represents the time the respondent takes to get up after sleep end). GU = Local time of getting out of bed. TBT = Total time in bed.
:::

## Sample

The validated dataset used for analysis is made up of `{r} weighted_data |> nrow() |> format(big.mark = ",") |> format_to_md_latex()` Brazilian individuals aged 18 or older, residing in the UTC-3 timezone, who completed the survey between October 15 and 21, 2017.

The unfiltered valid sample comprises `{r} geocoded_data |> nrow() |> format(big.mark = ",") |> format_to_md_latex()` participants from all Brazilian states, while the raw sample is composed of $120,265$ individuals. The majority of the sample data was obtained in 2017 from October 15th to 21st by [a broadcast](https://globoplay.globo.com/v/6219513/) of the online questionnaire on a popular Brazil's Sunday TV show with national reach [@redeglobo2017]. This amount of data collected in such a short time gave the sample a population cross-sectional characteristic.

::: {#fig-appendice-1-age-sex-chronotype-series}
![](images/globo-2017-figure-1.png)

Source: Reproduction from @redeglobo2017.

Screenshots from the Fantástico TV show, aired on Rede Globo on October 15, 2017, starting at 9 PM, where the online questionnaire was presented.
:::

A survey conducted in 2019 by the Brazilian Institute of Geography and Statistics (IBGE) [-@ibge2021] found that $82.17\%$ of Brazilian households had access to an internet connection. Therefore, this sample is likely to have a good representation of Brazil’s population.

Daylight Saving Time (DST) began in Brazil at midnight on October 15th, 2017. Residents from the Midwest, Southeast, and South regions were instructed to set the clock forward by 1 hour. We believe that this event did not contaminate the data since it started on the same day of the data collection. It’s important to notice that we asked subjects to relate their routine behavior, not how they behaved in the last few days. A possible effect of the DST on the sample would be the production of an even later chronotype for populations near the planet's poles, amplifying a possible latitude effect. However, this was not shown on the data.

Based on the 2022 census [@ibgea], Brazil had $52.263\%$ of females and $47.737\%$ of males with an age equal to or greater than 18 years old. The sample is skewed for female subjects, with $66.297\%$ of females and $33.703\%$ of male subjects.

The subjects' mean age is $32.015$ years ($\text{SD} = 9.252$; $\text{Max.} = 58.786$). Female subjects have a mean age of $31.787$ years ($\text{SD} = 9.364$; $\text{Max.} = 58.786$) and male subjects $32.464$ years ($\text{SD} = 9.012$; $\text{Max.} = 58.772$). For comparison, based on the 2022 census [@ibgeb], Brazil’s population with an age equal to or greater than $18$ years old had a mean age of $44.277$ years ($\text{SD} = 17.221$), with a mean age of $44.987$ years ($\text{SD} = 17.511$) for female subjects and a mean age of $43.499$ years ($\text{SD} = 16.864$) for male subjects.

Considering the five regions of Brazil, the sample is mostly skewed for the Southeast, the most populated region. According to Brazil’s 2022 census [@ibge2022], the Southeast region is home to $41.784\%$ of Brazil’s population, followed by the Northeast ($26.910\%$), South ($14.741\%$), North ($8.544\%$), and Midwest ($8.021\%$) regions. $62.454\%$ of the sample is located in the Southeast region, $11.797\%$ in the Northeast, $17.861\%$ in the South, $1.682\%$ in the North, and $6.205\%$ in the Midwest region. Note that a lack of subjects in the North and Midwest region is justified by the sample timezone inclusion criteria (UTC-3).

The sample latitudinal range was $30.211$ decimal degrees ($\text{Min.} = -30.109$; $\text{Max.} = 0.10177$) with a longitudinal span of $16.378$ decimal degrees ($\text{Min.} = -51.342$; $\text{Max.} = -34.964$). For comparison, Brazil has a latitudinal range of $39.024$ decimal degrees ($\text{Min.} = -33.752$; $\text{Max.} = 5.2719$) and a longitudinal span of $39.198$ decimal degrees ($\text{Min.} = -34.793$; $\text{Max.} = -73.991$). 

## Data wrangling

The data wrangling and analysis followed the data science program proposed by Hadley Wickham and Garrett Grolemund [@wickham2016] [@fig-sm-2-wickham-at-al-2024-figure-1]. All processes were made with the help of the R programming language [@rcoreteam], RStudio IDE [@positteam], and several R packages. The [tidyverse](https://www.tidyverse.org/) and [rOpenSci](https://ropensci.org/) peer-reviewed package ecosystem and other R packages adherents of the tidy tools manifesto [@wickham2023a] were prioritized. The MCTQ data was analyzed using the `mctq` rOpenSci peer-reviewed package [@vartanian2023]. All processes were made in order to provide result reproducibility and to be in accordance with the FAIR principles [@wilkinson2016].

::: {#fig-sm-2-wickham-at-al-2024-figure-1}
![](images/wickham-at-al-2024-figure-1.png){width=75%}

Source: Reproduced from @wickham2023b.

Model of the data science process created by Wickham, Çetinkaya-Runde, and Grolemund [-@wickham2023b].
:::

The [`targets` R package manual](https://books.ropensci.org/targets/walkthrough.html) provided a pipeline for data munging. You can see this pipeline on the `_target.R` file in the root of thesis code repository or by clicking [here](https://github.com/danielvartan/mastersthesis/blob/main/_targets.R).

````{r}
#| output: asis

glue::glue("
  ``` {{.r filename='_targets.R'}}
  {paste0(readLines(here::here('_targets.R')), collapse = '\n')}
  ```
")
````

Along with the data cleaning procedures, lookup tables were used to clean the text/character variables. These tables were created by manually inspecting the **unique values of the raw data** and adjusting common misspellings, synonyms, and other issues.

Text/character variables: `track`, `names`, `email`, `country`, `state`, `municiplality`, `postalcode`, `sleep_drugs_which`, `sleep_disorder_which`, `medication_which`.

The lookup tables are available in the research compendium of this thesis. They had to be encrypted because of the sensitive information they contain. If you need access, please contact the author.

The matching of the variables `sleep_drugs_which`, `sleep_disorder_which`, `medication_which` is not complete. This variables were not used in the analysis, but they are available in the research compendium.

Read the section about geographical information to learn more about the matching process.

## Secondary data

### Geocode data

Geographic data were collected by the variables `country`, `state`, `municipality`, and `postal code`. The unique values of those data were manually inspected and adjusted using lookup tables. The `municiplaity` values were first matched using string distance algorithms present in the [`stringdist`](https://github.com/markvanderloo/stringdist) R package and data from the Brazilian Institute of Geography and Statistics (IBGE) via the [`geobr`](https://ipeagit.github.io/geobr/index.html>) R package., other processes were then performed manually.

This was a hard task involving crossing information from different sources, such as the [QualoCEP](https://www.qualocep.com/) [@qualocep2024], [Google Geocoding](https://developers.google.com/maps/documentation/geocoding/overview) [google], [ViaCEP](https://viacep.com.br/) [@viacep], and [OpenStreetMap](https://www.openstreetmap.org/) [@openstreetmap] databases, along with the Brazilian postal service ([Correios](https://www.correios.com.br/enviar/precisa-de-ajuda/tudo-sobre-cep)) postal code documentation. Hence, the data matching shown in the lookup table was gathered considering not only one variable, but the whole set of geographical information provided by the respondent. Special cases were coded in the lookup table named *special_cases*

All values were also checked for ambiguities, including the municipalities names (e.g., the name *Aracoiba* could refer to the municipality of *Aracoiaba* in the state of *Ceará*, but could also refer to the municipality of *Araçoiaba da Serra* in the state of *São Paulo*). All values that had a similarity or pattern matching with one or more municipalities were manually inspected to avoid errors (see `get_more_than_one_geographical_match` function in the code repository). 

#### Postal codes

After removing all non-numeric characters from the Brazilian postal codes (Código de Endereçamento Postal ([CEP](https://pt.wikipedia.org/wiki/C%C3%B3digo_de_Endere%C3%A7amento_Postal))), they were processed by the following rules:

- If they had 9 or more digits, they were truncated to 8 digits (the first 8 digits are the postal code).
- If they between 5 and 7 digits, they were complemented with `0`s at the ending.
- If they had less than 5 digits, they were discarded.

In addition, a visual inspection was performed to check for inconsistencies.

After this process, the postal codes were matched with the [QualoCEP](https://www.qualocep.com/) database [@qualocep2024]. Existing postal codes were than validated by the following rules:

- If the postal code had **not** been modified **and** the state **or** the municipality was the same, it was considered valid.
- If the postal code had been modified **and** the state and municipality were the same, it was considered valid.
- Else, it was considered invalid. Invalid CEPs were discarded in the processed dataset.

Invalid postal codes were then matched with data derived from reverse geocoding using [Google Geocoding API](https://developers.google.com/maps/documentation/geocoding/overview) [@google] via the [`tidygeocoder`](https://jessecambon.github.io/tidygeocoder/) R package [@cambon2021], which is stored in the thesis lookup tables. After that, the same process of validation was performed. The API data was able to reduce invalid postal codes from 563 ro 292, a 48.13% reduction. The postal codes that were not validated were discarded from the final data.

It's important to note that some postal codes could not be evaluated because they were missing the state or municipality information. These postal codes were maintained, but no geocode data was associated with them.

Finally, the `state` and `municipality` variables were adjusted using the data from the valid postal codes.

Non-Brazilian postal codes were not validated, but they were cleaned by removing non-digit characters and codes with 3 digits or less. They also went through a process da cleaning via visual inspection. Their values can be found in the `special_cases` lookup table.

#### Latitudes and longitudes

Latitudes and longitudes values consider only the `municiplaity` and `postal_code` variables. They were extracted from the QualoCEP database, which is the result of a reverse geocoding using the Google Geocoding API. See Appendice 6 to side by side comparison of the latitudes and longitudes from QualoCEP and Google Geocoding API.

For respondents  who did not provide a valid postal code, the latitude and longitude were extracted via the mean of the latitudes and longitudes associated with the municipality in the QualoCEP database.

Finally, Google Geocoding API, via the `tidygeocoder` R package, was used on cases that didn't had a match in the QualoCEP database.

### Solar irradiance data

The solar irradiance data is based on Brazil's National Institute for Space Research (INPE) 2017 Laboratory of Modeling and Studies of Renewable Energy Resources (LABREN) 2017 [Solar Energy Atlas](https://labren.ccst.inpe.br/atlas_2017.html) [@pereira2017]. In particular, it was used the Global Horizontal Irradiance (GHI) data, which is the total amount of irradiance received from above by a surface horizontal to the ground.

The data comprise annual and monthly averages of the daily total irradiation in Wh/m².day with spatial resolution of 0.1° x 0.1° (latitude/longitude) (about 10km x 10km). It's important to note that the sample was collected in the same year of the radiance data.

::: {#fig-sm-2-pereira-2017-figure-13}
![](images/pereira-2017-figure-13.png)

Source: Adapted from @pereira2017.

Components of solar irradiance. $G_{0}$ = Extraterrestrial irradiance. $G_{n}$ = Direct normal irradiance. $G_{dif}$ = Diffuse horizontal irradiance. $G_{dir}$ = Direct horizontal irradiance. $G$ = Global horizontal irradiance. $G_{i}$ = Inclined plane irradiance.
:::

### Solar time data

The [`suntools`](https://doi.org/10.32614/CRAN.package.suntools) R package [@bivand] was used to calculate the local of time of sunrise, sunset, and daylight duration for a given date and location. `suntools` functions are based on equations provided by @meeus1991 and by the United States's National Oceanic & Atmospheric Administration ([NOAA](https://gml.noaa.gov/grad/solcalc/calcdetails.html)).

The data and time of the equinox and solstices were gathered from the [Time and Date AS](https://www.timeanddate.com/calendar/seasons.html?year=2000&n=1440) service [@timeanddateas]. For validity, this data was checked with the equations from @meeus1991 and the results of the National Aeronautics and Space Administration (NASA) [ModelE AR5 Simulations](https://data.giss.nasa.gov/modelE/ar5plots/srvernal.html) [@nasa].

## Hypothesis test

The study hypothesis was tested using nested models general linear models of multiple linear regressions. The main idea of nested models is to verify the effect of the inclusion of one or more predictors in the model variance explanation (i.e., the $\text{R}^{2}$) [@allen1997; @maxwell2018]. This can be made by creating a restricted model and then comparing it with a full model. Hence, the hypothesis can be schematized as follows.

- __Null hypothesis__ ($\text{H}_{0}$): Adding *latitude* does not meaningfully improve the model’s fit, indicating that the change in adjusted $\text{R}^{2}$ is negligible or the F-test is not significant (considering a type I error probability ($\alpha$) of $0.05$).

- __Alternative Hypothesis__ ($\text{H}_{a}$): Adding *latitude* significantly improves the model’s fit, indicating that the change in adjusted $\text{R}^{2}$ is greater than the Minimum Effect Size (MES), and the F-test is significant  (considering a type I error probability ($\alpha$) of $0.05$).

$$
\begin{cases}
\text{H}_{0}: \Delta \ \text{Adjusted} \ \text{R}^{2} \leq \text{MES} \quad \text{or} \quad \text{F-test is not significant} \ (\alpha \geq 0.05) \\
\text{H}_{a}: \Delta \ \text{Adjusted} \ \text{R}^{2} > \text{MES} \quad \text{and} \quad \text{F-test is significant} \ (\alpha < 0.05)
\end{cases}
$$

Where:

$$
\Delta \ \text{Adjusted} \ \text{R}^{2} = \text{Adjusted} \ \text{R}^{2}_{f} - \text{Adjusted} \ \text{R}^{2}_{r}
$$

The general equation for the F-test for nested models [@allen1997, p. 113] is:

$$
\text{F} = \cfrac{\text{R}^{2}_{f} - \text{R}^{2}_{r} / (k_{f} - k_{R})}{(1 - \text{R}^{2}_{f}) / (\text{N} - k_{f} - 1)}
$$

Where:

* $\text{R}^{2}_{F}$ = Coefficient of determination for the __full__ model;
* $\text{R}^{2}_{R}$ = Coefficient of determination for the __restricted__ model;
* $k_{F}$ = Number of independent variables in the full model;
* $k_{R}$ = Number of independent variables in the restricted model;
* $\text{N}$ = Number of observations in the sample.

$$
\text{F} = \cfrac{\text{Additional Var. Explained} / \text{Additional d.f. Expended}}{\text{Var. unexplained} / \text{d.f. Remaining}}
$$

A MES must always be used in any data testing. The effect-size was present in the original Neyman and Pearson framework [@neyman1928; @neyman1928a], but unfortunately this practice fade away with the use of p-values, one of the many issues that came with the Null Hypothesis Significance Testing (NHST) [@perezgonzalez2015]. While p-values are estimates of type 1 error (in Neyman–Pearson’s approaches, or like-approaches), that's not the main thing we are interested while doing a hypothesis test, what is really being test is the effect size (i.e., a practical significance). Another major issue to only relying on p-values is that the estimated p-value tends to decrease when the sample size is increased, hence, focusing just on p-values with large sample sizes results in the rejection of the null hypothesis, making it not meaningful in this specific situation [@lin2013; @mariscal2021].

Publications related to issues regarding the misuse of p-value are plenty. For more on the matter, I recommend @perezgonzalez2015 review of Fisher's and Neyman-Pearson's data test proposals, @lin2013 and @mariscal2021 studies about large Samples and the p-value problem, and Cohen's essays on the subject (like @cohen1990 and @cohen1994).

It's important to note that, in addition to the F-test, it's assumed that for $\text{R}^{2}_{\text{r}}$ to differ significantly from $\text{R}^{2}_{\text{f}}$, there must be a non-negligible effect size between them. This effect size can be calculated using Cohen's $f^{2}$ [@cohen1988; @cohen1992]:

$$
\text{Cohen's } f^2 = \cfrac{\text{R}^{2}}{1 - \text{R}^{2}}
$$

For nested models, this can be adapted as follows:

$$
\text{Cohen's } f^2 = \cfrac{\text{R}^{2}_{f} - \text{R}^{2}_{r}}{1 - \text{R}^{2}_{f}} = \cfrac{\Delta \text{R}^{2}}{1 - \text{R}^{2}_{f}}
$$

$$
f^{2} = \cfrac{\text{Additional Var. Explained}}{\text{Var. unexplained}}
$$

Considering the particular emphasis that the solar zeitgeber has on the entrainment of biological rhythms (as demonstrated in many experiments), it would not be reasonable to assume that the latitude hypothesis could be supported without at least a non-negligible effect size. With this in mind, this analysis will use Cohen's threshold for small/negligible effects, the Minimum Effect Size (MES) ($\delta$) is defined as 0.02 [@cohen1988, p. 413; @cohen1992, p. 157].

In Cohen's words: 

> What is really intended by the invalid affirmation of a null hypothesis is not that the population ES [Effect Size] is literally zero, but **rather that it is negligible, or trivial** [@cohen1988, p. 16].

> SMALL EFFECT SIZE: $f^2 = .02$. Translated into \text{R}^{2} (9.2.5) or partial \text{R}^{2} for Case 1 (9.1.8), this gives $.02 / (1 + .02) = .0196$. We thus define a small effect as one that accounts for 2% of the $\text{Y}$ variance (in contrast with 1% for $r$), and translate to an $\text{R} = \sqrt{0196} = .14$ (compared to .10 for $r$). This is a modest enough amount, **just barely escaping triviality** and (alas!) all too frequently in practice represents the true order of magnitude of the effect being tested [@cohen1988, p. 413].

> But in many circumstances, all that is intended by "proving" the null hypothesis is that the ES is not necessarily zero but **small enough to be negligible**, i.e., no larger than $i$ (Section 1.5.5). How large $i$ is will vary with the substantive context. Assume, for example, that ES is expressed as $f^2$, and that the context is such as to consider $f^2$ no larger than $.02$ to be negligible; thus $i$ = .02  [@cohen1988, p. 461].

$$
\text{MES} = \text{Cohen's } f^2 \text{small threshold} = 0.02 \\
$$

For comparison, Cohen's threshold for medium effects is $0.15$, and for large effects is $0.35$ [@cohen1988, p. 413-414; @cohen1992, p. 157].

Knowing Cohen's $f^2$, is possible to calculated the equivalent $\text{R}^{2}$:

$$
0.02 = \cfrac{\text{R}^{2}}{1 - \text{R}^{2}} \quad \text{or} \quad \text{R}^{2} = \cfrac{0.02}{1.02} \eqsim 0.01960784
$$

In other words, the latitude must explain at least $1.960784\%$ of the variance in the dependent variable to be considered non-negligible. This is the Minimum Effect Size (MES) for this analysis.

In summary, the decision rule for the hypothesis test is as follows:

- **Reject** $\text{H}_{0}$ **if both**:
  - The F-test is significant.
  - $\Delta \ \text{Adjusted} \ \text{R}^{2} > 0.01960784$;
- **Fail to reject** $\text{H}_{0}$ **if either**:
  - The F-test is not significant, or
  - The F-test is significant, but $\Delta \ \text{Adjusted} \ \text{R}^{2} \leq 0.01960784$.

As usual, the significance level ($\alpha$) was set at $0.05$, allowing a 5% chance of a [Type I error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). A power analysis will be performed to determine the necessary sample size for detecting a significant effect, targeting a power ($1 - \beta$) of $0.8$.

Assumption checks include:

- Assessing the normality of residuals through visual inspections, such as [Q-Q plots](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot), and statistical tests like the [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test).

- Multicollinearity was assed by calculating variance inflation factors ([VIF](https://en.wikipedia.org/wiki/Variance_inflation_factor)), with a VIF above 10 indicating potential issues. Influential points will be examined using [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance) and leverage values to identify any points that may disproportionately affect model outcomes.

::: {.callout-warning}
It is important to note that relying on objective assumption tests is not advisable in this context. In larger samples, these tests can be overly sensitive to minor deviations, and in smaller samples, they may fail to detect significant ones. Furthermore, they can miss visual patterns that a single metric cannot capture [@shatz2024; @kozak2018; @schucany2006]. Therefore, visual inspection of diagnostic plots was deemed a more effective method for assessing the model assumptions in this case.
:::

::: {.callout-warning}
It's also important to emphasize that this thesis is not trying to establishing causality, only association. Predictive models alone should never be used to infer causal relationships [@arif2022].
:::

## Predictors

In order to replicate @leocadio-miguel2017 article, it was used the covariates age, longitude, and solar irradiation when the subjects filled the online questionnaire, with sex, daylight saving time (DST), and season as a cofactors. The thesis sample have data for all these variables with exception of DST and season. Since the data was collected in a single week, it wouldn't make sense to use these two variables, hence, they were not included in the model.

As a latitude proxy, the same authors used the annual average solar irradiation, along with local time of sunrise, sunset, and daylight duration in March equinox and in June and December solstices. All these variables can be integrated with the sample. This analysis will also perform another test using only the latitude variable as a new predictor in the full model. Therefore, the same hypothesis will be tested three times with different predictors, as follows.

There are many types of solar radiation, unfortunately, @leocadio-miguel2017 did not declare what type was used, but is reasonable to think they used the global horizontal solar irradiation (GHI) as a proxy for solar radiation. The GHI is the total amount of irradiation received from above by a surface horizontal to the ground. You can find more information in about these metrics in @pereira2017.

### Model and test A

- **Predictors for the restricted model**: age, sex, longitude, and GHI when the subjects filled the online questionnaire (by the GHI month average in their geographic coordinates).

- **Predictors for the full model**: *predictors for the restricted model* + annual average GHI; and local time of sunrise, sunset, and daylight duration in the closest March and September equinox and June and December solstices.

### Model and test B

- **Predictors for the restricted model**: age, sex, longitude, and GHI when the subjects filled the online questionnaire (by the GHI month average in their geographic coordinates).

- **Predictors for the full model**: *predictors for the restricted model* + annual average GHI; local time of sunrise, sunset, and daylight duration in the closest March and September equinox and June and December solstices; **and** latitude.

### Model and test C

- **Predictors for the restricted model**: age, sex, longitude, and GHI when the subjects filled the online questionnaire (by the GHI month average in their geographic coordinates).

- **Predictors for the full model**: (predictors for the restricted model) **and** latitude.

It's important to note that the inclusion of some these variables may produce issues regarding multicollinearity. If this occur, the variables will be removed from the model.

A power analysis will be performed to determine the necessary sample size for each test. Considering the original sample size this probably will not be an issue.
