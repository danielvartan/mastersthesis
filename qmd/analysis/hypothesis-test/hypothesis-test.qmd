---
title: "Hypothesis test"
description: |
  Draft notebook.
bibliography: ../references.bib
format:
  html:
    citations-hover: true
    code-copy: true
    code-overflow: wrap
    code-tools:
      source: https://github.com/danielvartan/mastersthesis
      toggle: false
      caption: none
    footnotes-hover: true
editor: source
highlight-style: arrow
lang: en
---

<!-- Go to <https://quarto.org/docs/guide/> for Quarto's documentation. -->

::: {.callout-note}
This document is just a draft.
:::

## Some references

* <https://olsrr.rsquaredacademy.com>
* <https://www.r-bloggers.com/2021/05/how-to-compare-nested-models-in-r/>
* <https://www.r-bloggers.com/2022/10/box-cox-transformation-in-r/>
* <https://online.stat.psu.edu/stat501/>
* <Allen, M. P. (1997). Understanding regression analysis>
* <Bussab, W. O. (1988). Análise de variância e de regressão- uma introdução>
* <Dalpiaz, D. (n.d.). Applied statistics with R>
* <DeGroot, M. H. (2012). Probability and statistics>
* <Dudek, B. (2020). Linear models with R: emphasis on 2-IV models: basics of multiple regression>
* <Fox, J. (2016). Applied regression analysis and generalized linear models>
* <Hair, J. F. 2019. Multivariate data analysis>
* <Johnson, R. A. (2013). Applied multivariate statistical analysis>
* <Kuhn, M. (n.d.). Tidy modeling with R>

## Statement

$$
\begin{cases}
\text{H}_{0}: \text{R}^{2}_{\text{res}} >= \text{R}^{2}_{\text{full}} \\
\text{H}_{a}: \text{R}^{2}_{\text{res}} < \text{R}^{2}_{\text{full}}
\end{cases}
$$

General equation for the F-test [@allen_1997, pp. 113]:

$$
F = \cfrac{\text{R}^{2}_{F} - \text{R}^{2}_{R} / (k_{F} - k_{R})}{(1 - \text{R}^{2}_{F}) / (\text{N} - k_{F} - 1)}
$$

Where:

* $\text{R}^{2}_{F}$ = Coefficient of determination for the __full__ model
* $\text{R}^{2}_{R}$ = Coefficient of determination for the __restricted__ model
* $k_{F}$ = Number of independent variables in the full model
* $k_{R}$ = Number of independent variables in the restricted model
* $N$ = Number of observations in the sample

$$
F = \cfrac{\text{Additional Var. Explained} / \text{Additional d.f. Expended}}{\text{Var. unexplained} / \text{d.f. Remaining}}
$$

## Assumptions

[@degroot_2012, pp. 736-738]

::: {.callout-warning}
__The predictor is known__. Either the vectors $z_{1}, \dots , z_{n}$ are known ahead of time, or they are the observed values of random vectors $Z_{1}, \dots , Z_{n}$ on whose values we condition before computing the joint distribution of ($Y_{1}, \dots , Y_{n}$).
:::

::: {.callout-warning}
__Normality__. For $i = 1, \dots, n$, the conditional distribution of $Y_{i}$ given the vectors $z_{1}, \dots , z_{n}$ is a normal distribution (__normality assumption__).
:::

::: {.callout-warning}
__Linear mean__. There is a vector of parameters  $\beta = (\beta_{0}, \dots, \beta_{p - 1})$ such that the conditional mean of $Y_{i}$ given the values $z_{1}, \dots , z_{n}$ has the form

$$
z_{i0} \beta_{0} + z_{i1} \beta_{1} + \cdots + z_{ip - 1} \beta_{p - 1}
$$
for $i = 1, \dots, n$ (__zero error mean assumption__).
:::

::: {.callout-warning}
__Common variance__. The observations $Y_{1}, \dots , Y_{n}$ have the same variance $\sigma^{2}$ (__homoscedasticity assumption__).
:::

::: {.callout-warning}
__Independence__. The random variables $Y_{1}, \dots , Y_{n}$ are independent given the observed $z_{1}, \dots , z_{n}$ (__independent errors assumption__).
:::

## First things first

```{r}
#| eval: false
#| output: false

library(gutils)
library(gvlma)
library(here)
library(olsrr)
library(tidymodels)
library(tidyverse)
```

```{r}
#| include: false
#| warning: false

options(scipen = 10, digits = 5)
```

```{r}
#| eval: false
#| output: false
#| warning: false

data <- get_raw_data() |>
  tidy_data() |>
  validate_data() |>
  analyze_data() |>
  filter_data() |>
  add_geocode_data()
```

```{r}
model_data <- data |>
  dplyr::select(msf_sc, age, sex, state, latitude, longitude) |>
  dplyr::mutate(msf_sc = transform_time(msf_sc)) |>
  tidyr::drop_na(msf_sc, age, sex, latitude)
```

## Restricted model

```{r}
#| eval: false

box_cox <- MASS::boxcox(msf_sc ~ age + sex, data = model_data)
lambda <- box_cox$x[which.max(box_cox$y)]

lambda
```

```{r}
res_model <- stats::lm(
  ((msf_sc^lambda - 1) / lambda) ~ age + sex, data = model_data
  )
```

```{r}
# ?broom::tidy.lm
broom::tidy(res_model)
```

```{r}
# ?broom::glance.lm
broom::glance(res_model) |>
  tidyr::pivot_longer(cols = dplyr::everything())
```

```{r}
# res_model |> olsrr::ols_regress()
res_model |> summary()
```

### Residual diagnostics

> Normality and zero mean error assumption.

```{r}
res_model |>
  stats::residuals() |>
  stats_sum()
```

```{r}
# ?moments::agostino.test
# ?fBasics::dagoTest()

res_model |>
  stats::residuals() |>
  normality_sum()
```

Correlation between observed residuals and expected residuals under normality.

```{r}
res_model |> olsrr::ols_test_correlation()
```

```{r}
# res_model |> olsrr::ols_plot_resid_qq()

qq_plot <- res_model |> 
  stats::residuals() |>
  plot_qq(print = FALSE)

hist_plot <- res_model |>
  stats::residuals() |>
  plot_hist(print = FALSE)

cowplot::plot_grid(hist_plot, qq_plot, ncol = 2, nrow = 1)
```

```{r}
# Linear mean assumption

res_model |> olsrr::ols_plot_resid_fit()
```

```{r}
res_model |> plot(3)
```

### Heteroskedasticity

> Homoscedasticity assumption.

```{r}
# "It test whether variance of errors from a regression is dependent on the values of a independent variable."

res_model |> olsrr::ols_test_breusch_pagan()
```

```{r}
res_model |> olsrr::ols_test_score()
```

### Collinearity diagnostics

> Independence assumption.

```{r}
res_model |> olsrr::ols_coll_diag()
```

> The variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables (e.g., VIF equal to 1 = variables are not correlated).

```{r}
res_model |> car::vif()
```

### Measures of influence

```{r}
res_model |> olsrr::ols_plot_resid_lev()
```


## Full model

```{r}
box_cox <- MASS::boxcox(
  msf_sc ~ age + sex + latitude, data = model_data
  )
box_cox$x[which.max(box_cox$y)] # lambda
```

```{r}
lambda # The same lambda of the restricted model
```

```{r}
full_model <- stats::lm(
  ((msf_sc^lambda - 1) / lambda) ~ age + sex + latitude, 
  data = model_data
  )
```

```{r}
# ?broom::tidy.lm
broom::tidy(full_model)
```

```{r}
# ?broom::glance.lm
broom::glance(full_model) |>
  tidyr::pivot_longer(cols = dplyr::everything())
```

```{r}
# full_model |> olsrr::ols_regress()
full_model |> summary()
```

### Residual diagnostics

> Normality and zero mean error assumption.

```{r}
full_model |>
  stats::residuals() |>
  stats_sum()
```

```{r}
full_model |>
  stats::residuals() |>
  normality_sum()
```

Correlation between observed residuals and expected residuals under normality.

```{r}
full_model |> olsrr::ols_test_correlation()
```

```{r}
# full_model |> olsrr::ols_plot_resid_qq()

hist_plot <- full_model |>
  stats::residuals() |>
  plot_hist(print = FALSE)

qq_plot <- full_model |> 
  stats::residuals() |>
  plot_qq(print = FALSE)

cowplot::plot_grid(hist_plot, qq_plot, ncol = 2, nrow = 1)
```

```{r}
full_model |> olsrr::ols_plot_resid_fit()
```

```{r}
full_model |> plot(3)
```

### Heteroskedasticity

> Homoscedasticity assumption.

```{r}
full_model |> olsrr::ols_test_breusch_pagan()
```

```{r}
full_model |> olsrr::ols_test_score()
```

### Collinearity diagnostics

> Independence assumption.

```{r}
full_model |> olsrr::ols_coll_diag()
```

> The variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables (e.g., VIF equal to 1 = variables are not correlated).

```{r}
full_model |> car::vif()
```

### Measures of influence

```{r}
full_model |> olsrr::ols_plot_resid_lev()
```


## Nested regression models test

$$
\begin{cases}
\text{H}_{0}: \text{R}^{2}_{\text{res}} >= \text{R}^{2}_{\text{full}} \\
\text{H}_{a}: \text{R}^{2}_{\text{res}} < \text{R}^{2}_{\text{full}}
\end{cases}
$$

$$
F = \cfrac{\text{R}^{2}_{F} - \text{R}^{2}_{R} / (k_{F} - k_{R})}{(1 - \text{R}^{2}_{F}) / (\text{N} - k_{F} - 1)}
$$

$$
F = \cfrac{\text{Additional Var. Explained} / \text{Additional d.f. Expended}}{\text{Var. unexplained} / \text{d.f. Remaining}}
$$

```{r}
dplyr::tibble(
  name = c("r_squared_res", "r_squared_full", "diff"),
  value = c(
  r_squared(res_model), r_squared(full_model), 
  r_squared(full_model) - r_squared(res_model)
  )
)
```

```{r}
stats::anova(res_model, full_model)
```

```{r}
n <- nrow(model_data)
k_res <- length(stats::coefficients(res_model)) - 1
k_full <- length(stats::coefficients(full_model)) - 1

((r_squared(full_model) - r_squared(res_model)) / (k_full - k_res)) / ((1 - r_squared(full_model)) / (n  - k_full - 1))
```

$$
f^{2} = \cfrac{\text{R}^{2}_{F} - \text{R}^{2}_{R}}{1 - \text{R}^{2}_{F}}
$$

$$
f^{2} = \cfrac{\text{Additional Var. Explained}}{\text{Var. unexplained}}
$$

```{r}
cohens_f_squared_summary(
  adj_r_squared(res_model), 
  adj_r_squared(full_model)
  )
```


## Group test

$$
\begin{cases}
\text{H}_{0}: \text{MSF}^{2^{o}}_{\text{sc}} >= \text{MSF}^{30^{o}}_{\text{sc}} \\
\text{H}_{a}: \text{MSF}^{2^{o}}_{\text{sc}} < \text{MSF}^{30^{o}}_{\text{sc}}
\end{cases}
$$
```{r}
group_1 <- "Roraima" # Boa vista (2° 49' 12" N 60° 40' 19" O)

msf_sc_group_1 <- model_data |>
  dplyr::filter(state == group_1) |>
  magrittr::extract2("msf_sc")

stats_sum_group_1 <- 
  data |> 
  dplyr::filter(state == group_1) |>
  magrittr::extract2("msf_sc") |> 
  stats_sum()
```

```{r}
group_2 <- "Rio Grande do Sul" # Porto Alegre (30° 01' 58" S 51° 13' 48" O)

msf_sc_group_2 <- model_data |>
  dplyr::filter(state == group_2) |>
  magrittr::extract2("msf_sc")

stats_sum_group_2 <- 
  data |> 
  dplyr::filter(state == group_2) |>
  magrittr::extract2("msf_sc") |> 
  stats_sum()
```

```{r}
stats::t.test(msf_sc_group_1, msf_sc_group_2, alternative = "less")
```

```{r}
dplyr::tibble(
  name = c("mean_group_1", "mean_group_2", "diff"),
  value = c(
  stats_sum_group_1$mean, stats_sum_group_2$mean, 
  stats_sum_group_2$mean - stats_sum_group_1$mean
  )
)
```

[@frey_2022, pp. 224-227]

$$
d = \cfrac{\mu_{1} - \mu_{2}}{\sigma_{e}}
$$

```{r}
effsize::cohen.d(msf_sc_group_1, msf_sc_group_2)
```
