<!-- %:::% .common h1 begin %:::% -->
# Hypothesis Test A {#sec-sm-hypothesis-test-a}
<!-- %:::% .common h1 end %:::% -->

```{r}
#| label: setup
#| include: false

source(here::here("R", "_setup.R"))
```

## Overview

This document focuses on testing the thesis hypothesis (**Test A**) using the methods described in the supplemental material titled *Methods*.

The assumptions addressed here are those relevant to general linear models, with further information available in the supplemental material titled *Overview of General Linear Models*.

As with all analyses in this thesis, the process is fully reproducible and was conducted using the [R programming language](https://www.r-project.org/) [@rcoreteama] along with the [Quarto](https://quarto.org/) publishing system [@allaire].

## Setting the Environment

```{r}
#| eval: false

library(brandr)
library(broom)
library(dplyr)
library(forecast)
library(ggplot2)
library(ggplotify)
library(here)
library(hms)
library(janitor)
library(latex2exp)
library(lmtest)
library(lubritime) # github.com/danielvartan/lubritime
library(magrittr)
library(methods)
library(olsrr)
library(parsnip)
library(patchwork)
library(performance)
library(plotr) # github.com/danielvartan/plotr
library(pwrss)
library(quartor) # github.com/danielvartan/quartor
library(report)
library(rlang)
library(rutils) # github.com/danielvartan/rutils
library(see)
library(skedastic)
library(stats)
library(stringr)
library(tidyr)
library(workflows)
```

```{r}
#| include: false

library(ggplot2)
library(magrittr)
library(rlang)
```

## Loading and Processing the Data

::: {.callout-tip}
Assumption 1
: \hspace{20cm} __Predictor is known__. Either the vectors $z_{1}, \dots , z_{n}$ are known ahead of time, or they are the observed values of random vectors $Z_{1}, \dots , Z_{n}$ on whose values we condition before computing the joint distribution of ($Y_{1}, \dots , Y_{n}$) [@degroot2012a, p. 736].
:::

**Assumption 1** is satisfied, as the predictors are known.

```{r}
#| eval: false
#| output: false

targets::tar_make(script = here::here("_targets.R"))
```

This data processing is performed solely for the purpose of the analysis. The variables undergo numerical transformations to streamline the modeling process and minimize potential errors.

```{r}
#| output: false

data <-
  targets::tar_read("weighted_data", store = here::here("_targets")) |>
  dplyr::select(
    msf_sc, age, sex, longitude, ghi_month, ghi_annual,
    march_equinox_daylight, june_solstice_daylight,
    december_solstice_daylight, cell_weight
  ) |>
  dplyr::mutate(
    dplyr::across(
      .cols = dplyr::ends_with("_daylight"),
      .fns = ~ .x |> as.numeric()
    )
  ) |>
  dplyr::mutate(
    msf_sc =
      msf_sc |>
      lubritime:::link_to_timeline(threshold = hms::parse_hms("12:00:00")) |>
      as.numeric()
  ) |>
  tidyr::drop_na()
```

```{r}
#| code-fold: false

data |>
  dplyr::select(-cell_weight) |>
  report::report()
```

## Conducting a Power Analysis

```{r}
#| include: false

pwr_analysis <- pwrss::pwrss.f.reg(
  f2 = 0.02, # Minimal Effect Size (MES).
  k = length(data) - 2, # Number of predictors (-msf_sc, -cell_weight).
  power = 0.99,
  alpha = 0.01
)
```

The results indicate that at least `{r} pwr_analysis$n |> groomr:::format_to_md_latex()` observations per variable are required to achieve a power of $0.99$ ($1 - \beta$) (the probability of **not** committing a [type II error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)) and a significance level ($\alpha$) of $0.01$ ($0.99$ probability of **not** committing a [type I error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)). The dataset contains `{r} groomr:::format_to_md_latex(nrow(data))` observations, which exceeds this requirement.

```{r}
#| code-fold: false

pwr_analysis <- pwrss::pwrss.f.reg(
  f2 = 0.02, # Minimal Effect Size (MES).
  k = length(data) - 2, # Number of predictors (-msf_sc, -cell_weight).
  power = 0.99,
  alpha = 0.01
)
```

::: {#fig-sm-5-power-analysis}
```{r}
#| code-fold: true

pwrss::power.f.test(
  ncp = pwr_analysis$ncp,
  df1 = pwr_analysis$df1,
  df2 = pwr_analysis$df2,
  alpha = pwr_analysis$parms$alpha,
  plot = TRUE
)
```

[Source: Created by the author.]{.legend}

Power analysis for the hypothesis test.
:::

```{r}
#| include: false

quartor:::write_in_results_yml(
  list(
    hta_pwr_analysis = pwr_analysis
  )
)
```

## Examining Distributions

```{r}
#| eval: false
#| include: false

data |>
  quartor:::tabset_panel_var_dist(
    data_name = "data",
    cols =
      data |>
      names() |>
      stringr::str_subset("^sex$|^cell_weight$", negate = TRUE),
    col_labels = c(
      "MSF~sc~ (Chronotype proxy) (Seconds)",
      "Age (years)",
      "Longitude (Decimal degrees)",
      "Monthly average global horizontal irradiance (Wh/m²)",
      "Annual average global horizontal irradiance (Wh/m²)",
      "Daylight on the March equinox (Seconds)",
      "Daylight on the June solstice (Seconds)",
      "Daylight on the December solstice (Seconds)"
    ),
    heading = "###",
    suffix = "sm-5",
    root = "..",
    summarytools = FALSE
  )
```

{{< include ../qmd/_tabset-panel-sm-5-var-dist.qmd >}}

## Examining Correlations

::: {#fig-sm-5-correlation-matrix}
```{r}
data |>
  plotr:::plot_ggally(
    cols = c(
      "msf_sc",
      "age",
      "sex",
      "longitude",
      "ghi_month",
      "ghi_annual"
    ),
    mapping = ggplot2::aes(colour = sex)
  ) |>
  rutils::shush()
```

[Source: Created by the author.]{.legend}

Correlation matrix of the main predictors.
:::

## Building the Restricted Model

### Fitting the Model

```{r}
#| code-fold: false

form <- formula(msf_sc ~ age + sex + longitude + ghi_month)
```

```{r}
#| code-fold: false

model <-
  parsnip::linear_reg() |>
  parsnip::set_engine("lm") |>
  parsnip::set_mode("regression")
```

```{r}
#| code-fold: false

workflow <-
  workflows::workflow() |>
  workflows::add_case_weights(cell_weight) |>
  workflows::add_formula(form) |>
  workflows::add_model(model)

workflow
```

```{r}
#| code-fold: false

fit <- workflow |> parsnip::fit(data)
fit_restricted <- fit
```

::: {#tbl-sm-5-restricted-model-fit-1}
```{r}
#| code-fold: true

fit_coefs <- fit |> broom::tidy()

fit_coefs |> janitor::adorn_rounding(5)
```

Output from the model fitting process showing the estimated coefficients, standard errors, test statistics, and p-values for the terms in the linear regression model.
:::

```{r}
#| eval: false
#| include: false
#| code-fold: true

fit |>
  broom::augment(data) |>
  janitor::adorn_rounding(5)
```

::: {#tbl-sm-5-restricted-model-fit-2}
```{r}
#| code-fold: true

fit_stats <- fit |> broom::glance()

fit_stats |>
  tidyr::pivot_longer(cols = dplyr::everything()) |>
  janitor::adorn_rounding(10)
```

Summary of model fit statistics showing key metrics including R-squared, adjusted R-squared, sigma, statistic, p-value, degrees of freedom, log-likelihood, AIC, BIC, and deviance.
:::

```{r}
#| code-fold: false

fit_engine <- fit |> parsnip::extract_fit_engine()

fit_engine |> summary()
```

```{r}
#| code-fold: true

# A jerry-rigged solution to fix issues related to modeling using the pipe.

fit_engine_2 <- lm(form, data = data, weights = cell_weight)
fit_engine_restricted <- fit_engine_2
```

```{r}
#| code-fold: false

report::report(fit_engine_2)
```

```{r}
#| include: false

quartor:::write_in_results_yml(
  list(
    hta_restricted_model_fit_coefs = fit_coefs,
    hta_restricted_model_fit_stats = fit_stats
  )
)
```

### Evaluating the Model Fit

#### Predictions

::: {#fig-sm-5-restricted-model-fit}
```{r}
#| code-fold: true

limits <-
  stats::predict(fit_engine, interval = "prediction") |>
  dplyr::as_tibble() |>
  rutils::shush()

fit |>
  broom::augment(data) |>
  dplyr::bind_cols(limits) |>
  ggplot2::ggplot(ggplot2::aes(msf_sc, .pred)) +
  # ggplot2::geom_ribbon(
  #   mapping = ggplot2::aes(ymin = lwr, ymax = upr),
  #   alpha = 0.2
  # ) +
  ggplot2::geom_ribbon(
    mapping = ggplot2::aes(
      ymin = stats::predict(stats::loess(lwr ~ msf_sc)),
      ymax = stats::predict(stats::loess(upr ~ msf_sc)),
    ),
    alpha = 0.2
  ) +
  ggplot2::geom_smooth(
    mapping = ggplot2::aes(y = lwr),
    se = FALSE,
    method = "loess",
    formula = y ~ x,
    linetype = "dashed",
    linewidth = 0.2,
    color = "black"
  ) +
  ggplot2::geom_smooth(
    mapping = ggplot2::aes(y = upr),
    se = FALSE,
    method = "loess",
    formula = y ~ x,
    linetype = "dashed",
    linewidth = 0.2,
    color = "black"
  ) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(
    intercept = 0,
    slope = 1,
    color = brandr::get_brand_color("orange")
  ) +
  ggplot2::labs(
    x = "Observed",
    y = "Predicted"
  )
```

Relation between observed and predicted values. The orange line is a 45-degree line originating from the plane's origin and represents a perfect fit. The shaded area depicts a smoothed version of the 95% confidence of the [prediction interval](http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/).
:::

#### Posterior Predictive Checks

Posterior predictive checks are a Bayesian technique used to assess model fit by comparing observed data to data simulated from the [posterior predictive distribution](https://en.wikipedia.org/wiki/Posterior_predictive_distribution) (i.e., the distribution of potential unobserved values given the observed data). These checks help identify systematic discrepancies between the observed and simulated data, providing insight into whether the chosen model (or distributional family) is appropriate. Ideally, the model-predicted lines should closely match the observed data patterns.

::: {#fig-sm-5-restricted-model-fit-comparison}
```{r}
#| code-fold: true

diag_sum_plots <-
  fit_engine_2 |>
  performance::check_model(
    panel = FALSE,
    colors = c(
      brandr::get_brand_color("orange"),
      brandr::get_brand_color("black"),
      "black"
    )
  ) |>
  plot() |>
  rutils::shush()

diag_sum_plots$PP_CHECK +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    subtitle = ggplot2::element_blank(),
    x = "MSFsc (Chronotype proxy) (s)",
  ) +
  ggplot2::theme_get()
```

Posterior predictive checks for the model. The orange line represents the observed data, while the black lines represent the model-predicted data.
:::

### Conducting Model Diagnostics

::: {.callout-warning}
It's important to note that objective assumption tests (e.g., Anderson–Darling test) is not advisable for larger samples, since they can be overly sensitive to minor deviations. Additionally, they might overlook visual patterns that are not captured by a single metric [@shatz2024; @kozak2018; @schucany2006].

I included those tests here **just for reference**. However, for the reason above, all assumptions were diagnosed by **visual assessment**.

For a straightforward critique of normality tests specifically, refer to [this](https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90) article by @greener2020.
:::

#### Normality

::: {.callout-tip}
Assumption 2
: \hspace{20cm} __Normality__. For $i = 1, \dots, n$, the conditional distribution of $Y_{i}$ given the vectors $z_{1}, \dots , z_{n}$ is a normal distribution [@degroot2012a, p. 737].

(Normality of the error term distribution [@hair2019, p. 287])
:::

**Assumption 2** is satisfied, as the residuals shown a fairly normal distribution by visual inspection.

##### Visual Inspection

::: {#fig-sm-5-restricted-model-residual-diag-normality-1}
```{r}
#| code-fold: true

fit_engine |>
  stats::residuals() |>
  dplyr::as_tibble() |>
  rutils:::test_normality(col = "value", name = "Residuals")
```

Histogram of the model residuals with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the residuals and the theoretical quantiles of the normal distribution.
:::

::: {#fig-sm-5-restricted-model-residual-diag-normality-2}
```{r}
#| code-fold: true

fit |>
  broom::augment(data) |>
  dplyr::select(.resid) |>
  tidyr::pivot_longer(.resid) |>
  ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +
  ggplot2::geom_boxplot(
    outlier.colour = brandr::get_brand_color("orange"),
    outlier.shape = 1,
    width = 0.5
  ) +
  ggplot2::labs(x = "Variable", y = "Value") +
  ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title.y = ggplot2::element_blank(),
    axis.text.y = ggplot2::element_blank(),
    axis.ticks.y = ggplot2::element_blank()
  )
```

Boxplot of model residuals with outliers highlighted in orange.
:::

::: {#tbl-sm-5-restricted-model-residual-diag-normality-1}
```{r}
#| code-fold: true

fit_engine |>
  stats::residuals() |>
  dplyr::as_tibble() |>
  rutils:::stats_summary(col = "value", name = "Residuals")
```

Summary statistics of model residuals.
:::

##### Tests

It's important to note that the Kolmogorov-Smirnov and Pearson chi-square tests are included here just for reference, as many authors don't recommend using them when testing for normality [@dagostino1990]. Learn more about normality tests in @thode2002.

I also recommend checking the original papers for each test to understand their assumptions and limitations:

- [Anderson-Darling test](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test): @anderson1952; @anderson1954.
- Bonett-Seier test: @bonett2002.
- [Cramér-von Mises test](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion): @cramer1928; @anderson1962.
- [D'Agostino test](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test): @dagostino1971; @dagostino1973.
- [Jarque–Bera test](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test): @jarque1980; @bera1981; @jarque1987.
- [Lilliefors (K-S) test](https://en.wikipedia.org/wiki/Lilliefors_test):  @smirnov1948; @kolmogorov1933; @massey1951; @lilliefors1967; @dallal1986.
- [Pearson chi-square test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test): @pearson1900.
- [Shapiro-Francia test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Francia_test): @shapiro1972.
- [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test): @shapiro1965.

$$
\begin{cases}
\text{H}_{0}: \text{The data is normally distributed} \\
\text{H}_{a}: \text{The data is not normally distributed}
\end{cases}
$$

::: {#tbl-sm-5-restricted-model-residual-diag-normality-2}
```{r}
#| code-fold: true

fit_engine |>
  stats::residuals() |>
  dplyr::as_tibble() |>
  rutils:::normality_summary(col = "value")
```

Summary of statistical tests conducted to assess the normality of the residuals.
:::

#### Linearity

::: {.callout-tip}
Assumption 3
: \hspace{20cm} __Linear mean__. There is a vector of parameters  $\beta = (\beta_{0}, \dots, \beta_{p - 1})$ such that the conditional mean of $Y_{i}$ given the values $z_{1}, \dots , z_{n}$ has the form

$$
z_{i0} \beta_{0} + z_{i1} \beta_{1} + \cdots + z_{ip - 1} \beta_{p - 1}
$$

for $i = 1, \dots, n$ [@degroot2012a, p. 737].

(Linearity of the phenomenon measured [@hair2019, p. 287])
:::

**Assumption 3** is satisfied, as the relationship between the variables is fairly linear. As shown in @sec-on-the-latitude-hypothesis, the hypothesis implies linearity and the distribution of the residuals also support this.

::: {#fig-sm-5-restricted-model-residual-diag-fitted-values-1}
```{r}
#| code-fold: true

plot <-
  fit |>
  broom::augment(data) |>
  ggplot2::ggplot(ggplot2::aes(.pred, .resid)) +
  ggplot2::geom_point() +
  ggplot2::geom_hline(
    yintercept = 0,
    color = "black",
    linewidth = 0.5,
    linetype = "dashed"
  ) +
  ggplot2::geom_smooth(color = brandr::get_brand_color("orange")) +
  ggplot2::labs(x = "Fitted values", y = "Residuals")

plot |> print() |> rutils::shush()
```

Residual plot showing the relationship between fitted values and residuals. The dashed black line represent zero residuals, indicating an ideal model fit. The orange line indicate the conditional mean of residuals.
:::

::: {#fig-sm-5-restricted-model-residual-diag-linearity-1}
```{r}
#| code-fold: true

plots <- fit_engine |> olsrr::ols_plot_resid_fit_spread(print_plot = FALSE)

for (i in seq_along(plots)) {
  q <- plots[[i]] + ggplot2::labs(title = ggplot2::element_blank())

  q <- q |> ggplot2::ggplot_build()
  q$data[[1]]$colour <- brandr::get_brand_color("orange")
  q$plot$layers[[1]]$constructor$color <- brandr::get_brand_color("orange")

  plots[[i]] <- q |> ggplot2::ggplot_gtable() |> ggplotify::as.ggplot()
}

patchwork::wrap_plots(plots$fm_plot, plots$rsd_plot, ncol = 2)
```

Residual fit spread plots to detect non-linearity, influential observations, and outliers. The side-by-side plots show the centered fit and residuals, illustrating the variation explained by the model and what remains in the residuals. Inappropriately specified models often exhibit greater spread in the residuals than in the centered fit. "Proportion Less" indicates the cumulative distribution function, representing the proportion of observations below a specific value, facilitating an assessment of model performance.
:::

The [Ramsey's RESET test](https://en.wikipedia.org/wiki/Ramsey_RESET_test) indicates that the model has no omitted variables. This test examines whether non-linear combinations of the fitted values can explain the response variable.

Learn more about the Ramsey's RESET test in: @ramsey1969.

$$
\begin{cases}
\text{H}_{0}: \text{The model has no omitted variables} \\
\text{H}_{a}: \text{The model has omitted variables}
\end{cases}
$$

```{r}
#| code-fold: false

fit_engine |> lmtest::resettest(power = 2:3)
```

```{r}
#| code-fold: false

fit_engine |> lmtest::resettest(type = "regressor")
```

#### Homoscedasticity (Common Variance)

::: {.callout-tip}
Assumption 4
: \hspace{20cm} __Common variance__ (homoscedasticity). There is as parameter $\sigma^{2}$ such the conditional variance of $Y_{i}$ given the values $z_{1}, \dots , z_{n}$ is $\sigma^{2}$ for $i = 1, \dots, n$.

(Constant variance of the error terms [@hair2019, p. 287])
:::

**Assumption 4** is satisfied. When comparing the standardized residuals ($\sqrt{|\text{Standardized Residuals}|}$) spread to the fitted values, we can observe that the residuals are fairly constant across the range of values. This suggests that the residuals have a constant variance.

##### Visual Inspection

```{r}
#| eval: false
#| include: false

# Based on:
# https://sscc.wisc.edu/sscc/pubs/RegDiag-R/homoscedasticity.html#:~:text=We%20must%20plot%20the%20residuals%20against%20the%20fitted%20values%20and%20against%20each%20of%20the%20predictors.
```

::: {#fig-sm-5-restricted-model-diag-homoscedasticity-1}
```{r}
#| code-fold: true

plot <-
  fit |>
  stats::predict(data) |>
  dplyr::mutate(
    .sd_resid =
      fit_engine |>
      stats::rstandard() |>
      abs() |>
      sqrt()
  ) |>
  ggplot2::ggplot(ggplot2::aes(.pred, .sd_resid)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(color = brandr::get_brand_color("orange")) +
  ggplot2::labs(
    x = "Fitted values",
    y = latex2exp::TeX("$\\sqrt{|Standardized \\ Residuals|}$")
  )

plot |> print() |> rutils::shush()
```

Relation between the fitted values of the model and its standardized residuals.
:::

```{r}
#| eval: false
#| include: false

fit |>
  quartor:::tabset_panel_var_homoscedasticity(
    data = data,
    cols = c(
      "msf_sc",
      fit_engine |>
      stats::coef() |>
      names() |>
      magrittr::extract(-1) |>
      stringr::str_subset("^sex", negate = TRUE)
    ),
    col_labels = c(
      "MSF~sc~  (Chronotype proxy) (Seconds)",
      "Age (Years)",
      "Longitude (Decimal degrees)",
      "Monthly average global horizontal irradiance (Wh/m²)"
    ),
    heading = "######",
    suffix = "sm-5-rm",
    root = ".."
  )
```

{{< include ../qmd/_tabset-panel-sm-5-rm-var-homoscedasticity.qmd >}}

##### Breusch-Pagan Test

The [Breusch-Pagan test](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) test indicates that the residuals exhibit constant variance.

Learn more about the Breusch-Pagan test in: @breusch1979 and @koenker1981.

$$
\begin{cases}
\text{H}_{0}: \text{The variance is constant} \\
\text{H}_{a}: \text{The variance is not constant}
\end{cases}
$$

```{r}
#| code-fold: false

# With studentising modification of Koenker
fit_engine |> lmtest::bptest(studentize = TRUE)
```

```{r}
#| code-fold: false

fit_engine |> lmtest::bptest(studentize = FALSE)
```

```{r}
# Using the studentized modification of Koenker.
fit_engine |> skedastic::breusch_pagan(koenker = TRUE)
```

```{r}
fit_engine |> skedastic::breusch_pagan(koenker = FALSE)
```

```{r}
#| code-fold: false

fit_engine |> car::ncvTest()
```

```{r}
#| code-fold: false

fit_engine_2 |> olsrr::ols_test_breusch_pagan()
```

#### Independence

::: {.callout-tip}
Assumption 5
: \hspace{20cm} __Independence__. The random variables $Y_{1}, \dots , Y_{n}$ are independent given the observed $z_{1}, \dots , z_{n}$ [@degroot2012a, p. 737].

(Independence of the error terms [@hair2019, p. 287])
:::

**Assumption 5** is satisfied. Although the residuals show some autocorrelation, they fall within the acceptable range of the Durbin–Watson statistic ($1.5$ to $2.5$). It's also important to note that the observations for each predicted value are not related to any other prediction; in other words, they are not grouped or sequenced by any variable (by design) (see @hair2019[p. 291] for more information).

Many authors don't consider autocorrelation tests for linear regression models, as they are more relevant for time series data. They were include here just for reference.

##### Visual Inspection

::: {#fig-sm-5-restricted-model-diag-independence-1}
```{r}
#| code-fold: true

fit_engine |>
  residuals() |>
  forecast::ggtsdisplay(
    lag.max = 30,
    theme = ggplot2::theme_get()
  )
```

Time series plot of the residuals along with its AutoCorrelation Function (ACF) and Partial AutoCorrelation Function (PACF).
:::

##### Correlations

@tbl-sm-5-restricted-model-residual-diag-independence-1 shows the relative importance of independent variables in determining the response variable. It highlights how much each variable uniquely contributes to the R-squared value, beyond what is explained by the other predictors.

::: {#tbl-sm-5-restricted-model-residual-diag-independence-1}
```{r}
#| code-fold: true

fit_engine |> olsrr::ols_correlations()
```

Correlations between the dependent variable and the independent variables, along with the zero-order, part, and partial correlations. The zero-order correlation represents the Pearson correlation coefficient between the dependent and independent variables. Part correlations indicate how much the R-squared would decrease if a specific variable were removed from the model, while partial correlations reflect the portion of variance in the response variable that is explained by a specific independent variable, beyond the influence of other predictors in the model.
:::

##### Newey-West Estimator

The [Newey-West estimator](https://en.wikipedia.org/wiki/Newey%E2%80%93West_estimator) is a method used to estimate the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) of the coefficients in a regression model when the residuals are autocorrelated.

Learn more about the Newey-West estimator in: @newey1987 and @newey1994.

```{r}
#| code-fold: false

fit_engine |> sandwich::NeweyWest()
```

##### Durbin-Watson Test

The [Durbin-Watson test](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) is a statistical test used to detect the presence of autocorrelation at lag $1$ in the residuals from a regression analysis. The test statistic ranges from $0$ to $4$, with a value of $2$ indicating no autocorrelation. Values less than $2$ indicate positive autocorrelation, while values greater than $2$ indicate negative autocorrelation [@fox2016].

A common rule of thumb in the statistical community is that a Durbin-Watson statistic between $1.5$ and $2.5$ suggests little to no autocorrelation.

Learn more about the Durbin-Watson test in: @durbin1950; @durbin1951; and @durbin1971.

$$
\begin{cases}
\text{H}_{0}: \text{Autocorrelation of the disturbances is 0} \\
\text{H}_{a}: \text{Autocorrelation of the disturbances is not equal to 0}
\end{cases}
$$

```{r}
#| code-fold: false

car::durbinWatsonTest(fit_engine)
```

##### Ljung-Box Test

The Ljung–Box test is a statistical test used to determine whether any autocorrelations within a time series are significantly different from zero. Rather than testing randomness at individual lags, it assesses the "overall" randomness across multiple lags.

Learn more about the [Ljung-Box test](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test) in: @box1970 and @ljung1978.

$$
\begin{cases}
\text{H}_{0}: \text{Residuals are independently distributed} \\
\text{H}_{a}: \text{Residuals are not independently distributed}
\end{cases}
$$

```{r}
#| code-fold: false

fit_engine |>
  stats::residuals() |>
  stats::Box.test(type = "Ljung-Box", lag = 10)
```

#### Colinearity/Multicollinearity

No high degree of colinearity was observed among the independent variables.

##### Variance Inflation Factor (VIF)

The [Variance Inflation Factor (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor) indicates the effect of other independent variables on the standard error of a regression coefficient. The VIF is directly related to the tolerance value ($\text{VIF}_{i} = 1/\text{TO}L$). High VIF values (larger than ~5 [@struck2024]) suggest significant collinearity or multicollinearity among the independent variables [@hair2019, p. 265].

::: {#fig-model-diag-colinearity-1}
```{r}
#| code-fold: true

diag_sum_plots <-
  fit_engine_2 |>
  performance::check_model(panel = FALSE) |>
  plot() |>
  rutils::shush()

diag_sum_plots$VIF +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    subtitle = ggplot2::element_blank()
  ) +
  ggplot2::theme_get() +
  ggplot2::theme(
    legend.position = "right",
    axis.title = ggplot2::element_text(size = 11, colour = "black"),
    axis.text = ggplot2::element_text(colour = "black"),
    axis.text.y = ggplot2::element_text(size = 9),
    legend.text = ggplot2::element_text(colour = "black")
  )
```

Variance Inflation Factors (VIF) for each predictor variable. VIFs below 5 are considered acceptable. Between 5 and 10, the variable should be examined. Above 10, the variable must considered highly collinear.
:::

::: {#tbl-sm-5-restricted-model-residual-diag-colinearity-1}
```{r}
#| code-fold: true

fit_engine |> olsrr::ols_vif_tol()
```

Variance Inflation Factors (VIF) and tolerance values for each predictor variable.
:::

::: {#tbl-sm-5-restricted-model-residual-diag-colinearity-2}
```{r}
#| code-fold: true

fit_engine_2 |> performance::check_collinearity()
```

Variance Inflation Factors (VIF) and tolerance values for each predictor variable.
:::

##### Condition Index

The [condition index](https://en.wikipedia.org/wiki/Condition_number) is a measure of multicollinearity in a regression model. It is based on the [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of the correlation matrix of the predictors. A condition index of 30 or higher is generally considered indicative of significant collinearity [@belsley2004, p. 112-114].

::: {#tbl-sm-5-restricted-model-residual-diag-colinearity-2}
```{r}
#| code-fold: true

fit_engine |> olsrr::ols_eigen_cindex()
```

Condition indexes and eigenvalues for each predictor variable.
:::

#### Measures of Influence

In this section, I check several measures of influence that can be used to assess the impact of individual observations on the model estimates.

::: {.callout-tip}
Leverage points
: Leverage is a measure of the distance between individual values of a predictor and other values of the predictor. In other words, a point with high leverage has an x-value far away from the other x-values. Points with high leverage have the potential to influence the model estimates [@struck2024; @hair2019, p. 262; @nahhas2024].
:::

::: {.callout-tip}
Influence points
: Influence is a measure of how much an observation affects the model estimates. If an observation with large influence were removed from the dataset, we would expect a large change in the predictive equation [@struck2024; @nahhas2024].
:::

##### Standardized Residuals

Standardized residuals are a rescaling of the residual to a common basis by dividing each residual by the standard deviation of the residuals [@hair2019, p. 264].

```{r}
#| code-fold: false

fit_engine |> stats::rstandard() |> head()
```

::: {#fig-sm-5-restricted-model-diag-influence-1}
```{r}
#| code-fold: true

dplyr::tibble(
  x = seq_len(nrow(data)),
  std = stats::rstandard(fit_engine)
) |>
  ggplot2::ggplot(
    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)
  ) +
  ggplot2::geom_linerange(color = "black") +
  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color("orange")) +
  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color("orange")) +
  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +
  ggplot2::labs(
    x = "Observation",
    y = "Standardized residual"
  )
```

Standardized residuals for each observation.
:::

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_resid_stand()
```

##### Studentized Residuals

[Studentized residuals](https://en.wikipedia.org/wiki/Studentized_residual) are a commonly used variant of the standardized residual. It differs from other methods in how it calculates the standard deviation used in standardization. To minimize the effect of any observation on the standardization process, the standard deviation of the residual for observation $i$ is computed from regression estimates omitting the $i$th observation in the calculation of the regression estimates [@hair2019, p. 264].

```{r}
#| code-fold: false

fit_engine |> stats::rstudent() |> head()
```

::: {#fig-sm-5-restricted-model-diag-influence-2}
```{r}
#| code-fold: true

dplyr::tibble(
  x = seq_len(nrow(data)),
  std = stats::rstudent(fit_engine)
) |>
  ggplot2::ggplot(
    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)
  ) +
  ggplot2::geom_linerange(color = "black") +
  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color("orange")) +
  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color("orange")) +
  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +
  ggplot2::labs(
    x = "Observation",
    y = "Studentized residual"
  )
```

Studentized residuals for each observation.
:::

::: {#fig-sm-5-restricted-model-diag-influence-3}
```{r}
#| code-fold: true

fit |>
  broom::augment(data) |>
  dplyr::mutate(
    std = stats::rstudent(fit_engine)
  ) |>
  ggplot2::ggplot(ggplot2::aes(.pred, std)) +
  ggplot2::geom_point(color = "black") +
  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color("orange")) +
  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color("orange")) +
  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +
  ggplot2::labs(
    x = "Predicted value",
    y = "Studentized residual"
  )
```

Relation between studentized residuals and fitted values.
:::

::: {#fig-sm-5-restricted-model-diag-influence-4}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_resid_lev(threshold = 2, print_plot = FALSE)

plot$plot +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    y = "Studentized residual"
  )
```

Relation between studentized residuals and their leverage points.
:::

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_resid_stud()
```

```{r}
#| eval: false
#| include: false

fit_engine |>
  car::influenceIndexPlot(
    vars = "Studentized",
    id = FALSE,
    main = NULL
  )
```

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_resid_stud_fit()
```

##### Hat Values

The hat value indicates how distinct an observation’s predictor values are from those of other observations. Observations with high hat values have high leverage and may be, though not necessarily, influential. There is no fixed threshold for what constitutes a “large” hat value; instead, the focus must be on observations with hat values significantly higher than the rest [@nahhas2024; @hair2019, p. 261].

```{r}
#| code-fold: false

fit_engine |> stats::hatvalues() |> head()
```

::: {#fig-sm-5-restricted-model-diag-influence-5}
```{r}
#| code-fold: true

dplyr::tibble(
  x = seq_len(nrow(data)),
  hat = stats::hatvalues(fit_engine)
) |>
  ggplot2::ggplot(
    ggplot2::aes(x = x, y = hat, ymin = 0, ymax = hat)
  ) +
  ggplot2::geom_linerange(color = brandr::get_brand_color("black")) +
  ggplot2::labs(
    x = "Observation",
    y = "Hat value"
  )
```

Hat values for each observation.
:::

```{r}
#| eval: false
#| include: false

fit_engine |>
  car::influenceIndexPlot(
    vars = "hat",
    id = FALSE,
    main = NULL
  )
```

##### Cook's Distance

The [Cook's D](https://en.wikipedia.org/wiki/Cook%27s_distance) measures each observation's influence on the model's fitted values. It is considered one of the most representative metrics for assessing overall influence [@hair2019].

A common practice is to flag observations with a Cook's distance of 1.0 or greater. However, a threshold of $4 / (n - k - 1)$, where $n$ is the sample size and $k$ is the number of independent variables, is suggested as a more conservative measure in small samples or for use with larger datasets [@hair2019].

Learn more about Cook's D in: @cook1977; @cook1979.

```{r}
#| code-fold: false

fit_engine |> stats::cooks.distance() |> head()
```

::: {#fig-sm-5-restricted-model-diag-influence-6}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_cooksd_bar(type = 2, print_plot = FALSE)

# The following procedure changes the plot aesthetics.
q <- plot$plot + ggplot2::labs(title = ggplot2::element_blank())
q <- q |> ggplot2::ggplot_build()
q$data[[5]]$label <- ""

q |> ggplot2::ggplot_gtable() |> ggplotify::as.ggplot()
```

Cook's distance for each observation along with a threshold line at $4 / (n - k - 1)$.
:::

::: {#fig-sm-5-restricted-model-diag-influence-7}
```{r}
#| code-fold: true

diag_sum_plots <-
  fit_engine_2 |>
  performance::check_model(
    panel = FALSE,
    colors = c("blue", "black", "black")
  ) |>
  plot() |>
  rutils::shush()

plot <-
  diag_sum_plots$OUTLIERS +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    subtitle = ggplot2::element_blank(),
    x = "Leverage",
    y = "Studentized residuals"
  ) +
  ggplot2::theme_get() +
  ggplot2::theme(
    legend.position = "right",
    # axis.title = ggplot2::element_text(size = 11, colour = "black"),
    # axis.text = ggplot2::element_text(colour = "gray25"),
    axis.text.y = ggplot2::element_text(size = 9)
  )

plot <- plot |> ggplot2::ggplot_build()

# The following procedure changes the plot aesthetics.
for (i in c(1:9)) {
  # "#1b6ca8" "#3aaf85"
  plot$data[[i]]$colour <- dplyr::case_when(
    plot$data[[i]]$colour == "blue" ~
      ifelse(i == 4, brandr::get_brand_color("grey"), brandr::get_brand_color("orange")),
    plot$data[[i]]$colour == "#1b6ca8" ~ "black",
    plot$data[[i]]$colour == "darkgray" ~ "black",
    TRUE ~ plot$data[[i]]$colour
  )
}

plot |> ggplot2::ggplot_gtable() |> ggplotify::as.ggplot()
```

Relation between studentized residuals and their leverage points. The orange line represents the [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance). Any points outside the contour lines are influential observations.
:::

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_cooksd_chart()
```

```{r}
#| eval: false
#| include: false

fit_engine |>
  car::influenceIndexPlot(
    vars = "Cook",
    id = FALSE,
    main = NULL
  )
```

#####  Influence on Prediction (DFFITS)

[DFFITS](https://en.wikipedia.org/wiki/DFFITS) (difference in fits) is a standardized measure of how much the prediction for a given observation would change if it were deleted from the model. Each observation’s DFFITS is standardized by the standard deviation of fit at that point [@struck2024].

The best rule of thumb is to classify as influential any standardized values that exceed $2 \sqrt{(p / n)}$, where $p$ is the number of independent variables + 1 and $n$ is the sample size  [@hair2019, p. 261].

Learn more about DDFITS in: @welsch1977 and @belsley2004.

```{r}
#| code-fold: false

fit_engine |> stats::dffits() |> head()
```

::: {#fig-sm-5-restricted-model-diag-influence-8}
```{r}
#| code-fold: true

plot <- fit_engine |>
  olsrr::ols_plot_dffits(print_plot = FALSE)

plot$plot + ggplot2::labs(title = ggplot2::element_blank())
```

Standardized DFFITS (difference in fits) for each observation.
:::

#####  Influence on Parameter Estimates (DFBETAS)

[DFBETAS](https://en.wikipedia.org/wiki/Influential_observation#:~:text=measures%20of%20influence) are a measure of the change in a regression coefficient when an observation is omitted from the regression analysis. **The value of the DFBETA is in terms of the coefficient itself** [@hair2019, p. 261]. A cutoff for what is considered a large DFBETAS value is $2 / \sqrt{n}$, where $n$ is the number of observations. [@struck2024].

Learn more about DFBETAS in: @welsch1977 and @belsley2004.

```{r}
#| code-fold: false

fit_engine |> stats::dfbeta() |> head()
```

```{r}
#| eval: false
#| include: false

fit_engine |>
  quartor:::tabset_panel_coef_dfbetas(
    heading = "######",
    suffix = "sm-5-rm",
    root = ".."
  )
```

{{< include ../qmd/_tabset-panel-sm-5-rm-coef-dfbetas.qmd >}}

##### Hadi's Measure

Hadi’s measure of influence is based on the idea that influential observations can occur in either the response variable, the predictors, or both.

Learn more about Hadi's measure in: @chatterjee2012.

::: {#fig-sm-5-restricted-model-diag-influence-12}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_hadi(print_plot = FALSE)

plot +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    y = "Hadi's measure"
  )
```

Hadi's influence measure for each observation.
:::

::: {#fig-sm-5-restricted-model-diag-influence-13}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_resid_pot(print_plot = FALSE)

plot + ggplot2::labs(title = ggplot2::element_blank())
```

Potential-residual plot classifying unusual observations as high-leverage points, outliers, or a combination of both.
:::

## Building the Full Model

### Fitting the Model

<!-- See: <https://stats.stackexchange.com/questions/25804/why-would-r-return-na-as-a-lm-coefficient> -->

```{r}
#| eval: false
#| include: false
#| code-fold: false

form <- as.formula(
    paste0(
      "msf_sc ~ ",
      paste0(
        c(
          "age", "sex", "longitude", "ghi_month", "ghi_annual",
          "march_equinox_sunrise", "march_equinox_sunset",
          "march_equinox_daylight", "june_solstice_sunrise",
          "june_solstice_sunset", "june_solstice_daylight",
          "september_equinox_sunrise", "september_equinox_sunset",
          "september_equinox_daylight", "december_solstice_sunrise",
          "december_solstice_sunset", "december_solstice_daylight"
        ),
        collapse = " + "
      )
    )
)
```

```{r}
#| code-fold: false

form <- as.formula(
    paste0(
      "msf_sc ~ ",
      paste0(
        c(
          "age", "sex", "longitude", "ghi_month",
          "ghi_annual", "march_equinox_daylight", "june_solstice_daylight",
          "december_solstice_daylight"
        ),
        collapse = " + "
      )
    )
)
```

```{r}
#| code-fold: false

lm(form, data = data, weights = cell_weight)
```

```{r}
#| code-fold: false

model <-
  parsnip::linear_reg() |>
  parsnip::set_engine("lm") |>
  parsnip::set_mode("regression")
```

```{r}
#| code-fold: false

workflow <-
  workflows::workflow() |>
  workflows::add_case_weights(cell_weight) |>
  workflows::add_formula(form) |>
  workflows::add_model(model)

workflow
```

```{r}
#| code-fold: false

fit <- workflow |> parsnip::fit(data)
fit_full <- fit
```

::: {#tbl-sm-5-full-model-fit-1}
```{r}
#| code-fold: true

fit_coefs <- fit |> broom::tidy()

fit_coefs |> janitor::adorn_rounding(5)
```

Output from the model fitting process showing the estimated coefficients, standard errors, test statistics, and p-values for the terms in the linear regression model.
:::

```{r}
#| eval: false
#| include: false
#| code-fold: true

fit |>
  broom::augment(data) |>
  janitor::adorn_rounding(5)
```

::: {#tbl-sm-5-full-model-fit-2}
```{r}
#| code-fold: true

fit_stats <- fit |> broom::glance()

fit_stats |>
  tidyr::pivot_longer(cols = dplyr::everything()) |>
  janitor::adorn_rounding(10)
```

Summary of model fit statistics showing key metrics including R-squared, adjusted R-squared, sigma, statistic, p-value, degrees of freedom, log-likelihood, AIC, BIC, and deviance.
:::

```{r}
#| code-fold: false

fit_engine <- fit |> parsnip::extract_fit_engine()

fit_engine |> summary()
```

```{r}
#| code-fold: true

# A jerry-rigged solution to fix issues related to modeling using the pipe.

fit_engine_2 <- lm(form, data = data, weights = cell_weight)
fit_engine_full <- fit_engine_2
```

```{r}
#| code-fold: false

report::report(fit_engine_2)
```

```{r}
#| include: false

quartor:::write_in_results_yml(
  list(
    hta_full_model_fit_coefs = fit_coefs,
    hta_full_model_fit_stats = fit_stats
  )
)
```

### Evaluating the Model Fit

#### Predictions

::: {#fig-sm-5-full-model-fit}
```{r}
#| code-fold: true

limits <-
  stats::predict(fit_engine, interval = "prediction") |>
  dplyr::as_tibble() |>
  rutils::shush()

fit |>
  broom::augment(data) |>
  dplyr::bind_cols(limits) |>
  ggplot2::ggplot(ggplot2::aes(msf_sc, .pred)) +
  # ggplot2::geom_ribbon(
  #   mapping = ggplot2::aes(ymin = lwr, ymax = upr),
  #   alpha = 0.2
  # ) +
  ggplot2::geom_ribbon(
    mapping = ggplot2::aes(
      ymin = stats::predict(stats::loess(lwr ~ msf_sc)),
      ymax = stats::predict(stats::loess(upr ~ msf_sc)),
    ),
    alpha = 0.2
  ) +
  ggplot2::geom_smooth(
    mapping = ggplot2::aes(y = lwr),
    se = FALSE,
    method = "loess",
    formula = y ~ x,
    linetype = "dashed",
    linewidth = 0.2,
    color = "black"
  ) +
  ggplot2::geom_smooth(
    mapping = ggplot2::aes(y = upr),
    se = FALSE,
    method = "loess",
    formula = y ~ x,
    linetype = "dashed",
    linewidth = 0.2,
    color = "black"
  ) +
  ggplot2::geom_point() +
  ggplot2::geom_abline(
    intercept = 0,
    slope = 1,
    color = brandr::get_brand_color("orange")
  ) +
  ggplot2::labs(
    x = "Observed",
    y = "Predicted"
  )
```

Relation between observed and predicted values. The orange line is a 45-degree line originating from the plane's origin and represents a perfect fit. The shaded area depicts a smoothed version of the 95% confidence of the [prediction interval](http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/).
:::

#### Posterior Predictive Checks

Posterior predictive checks are a Bayesian technique used to assess model fit by comparing observed data to data simulated from the [posterior predictive distribution](https://en.wikipedia.org/wiki/Posterior_predictive_distribution) (i.e., the distribution of potential unobserved values given the observed data). These checks help identify systematic discrepancies between the observed and simulated data, providing insight into whether the chosen model (or distributional family) is appropriate. Ideally, the model-predicted lines should closely match the observed data patterns.

::: {#fig-sm-5-full-model-fit-comparison}
```{r}
#| code-fold: true

diag_sum_plots <-
  fit_engine_2 |>
  performance::check_model(
    panel = FALSE,
    colors = c(
      brandr::get_brand_color("orange"),
      brandr::get_brand_color("black"),
      "black"
    )
  ) |>
  plot() |>
  rutils::shush()

diag_sum_plots$PP_CHECK +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    subtitle = ggplot2::element_blank(),
    x = "MSFsc (Chronotype proxy) (s)",
  ) +
  ggplot2::theme_get()
```

Posterior predictive checks for the model. The orange line represents the observed data, while the black lines represent the model-predicted data.
:::

### Conducting Model Diagnostics

::: {.callout-warning}
It's important to note that objective assumption tests (e.g., Anderson–Darling test) is not advisable for larger samples, since they can be overly sensitive to minor deviations. Additionally, they might overlook visual patterns that are not captured by a single metric [@shatz2024; @kozak2018; @schucany2006].

I included those tests here **just for reference**. However, for the reason above, all assumptions were diagnosed by **visual assessment**.

For a straightforward critique of normality tests specifically, refer to [this](https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90) article by @greener2020.
:::

#### Normality

::: {.callout-tip}
Assumption 2
: \hspace{20cm} __Normality__. For $i = 1, \dots, n$, the conditional distribution of $Y_{i}$ given the vectors $z_{1}, \dots , z_{n}$ is a normal distribution [@degroot2012a, p. 737].

(Normality of the error term distribution [@hair2019, p. 287])
:::

**Assumption 2** is satisfied, as the residuals shown a fairly normal distribution by visual inspection.

##### Visual Inspection

::: {#fig-sm-5-full-model-residual-diag-normality-1}
```{r}
#| code-fold: true

fit_engine |>
  stats::residuals() |>
  dplyr::as_tibble() |>
  rutils:::test_normality(col = "value", name = "Residuals")
```

Histogram of the model residuals with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the residuals and the theoretical quantiles of the normal distribution.
:::

::: {#fig-sm-5-full-model-residual-diag-normality-2}
```{r}
#| code-fold: true

fit |>
  broom::augment(data) |>
  dplyr::select(.resid) |>
  tidyr::pivot_longer(.resid) |>
  ggplot2::ggplot(ggplot2::aes(x = name, y = value)) +
  ggplot2::geom_boxplot(
    outlier.colour = brandr::get_brand_color("orange"),
    outlier.shape = 1,
    width = 0.5
  ) +
  ggplot2::labs(x = "Variable", y = "Value") +
  ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title.y = ggplot2::element_blank(),
    axis.text.y = ggplot2::element_blank(),
    axis.ticks.y = ggplot2::element_blank()
  )
```

Boxplot of model residuals with outliers highlighted in orange.
:::

::: {#tbl-sm-5-full-model-residual-diag-normality-1}
```{r}
#| code-fold: true

fit_engine |>
  stats::residuals() |>
  dplyr::as_tibble() |>
  rutils:::stats_summary(col = "value", name = "Residuals")
```

Summary statistics of model residuals.
:::

##### Tests

It's important to note that the Kolmogorov-Smirnov and Pearson chi-square tests are included here just for reference, as many authors don't recommend using them when testing for normality [@dagostino1990]. Learn more about normality tests in @thode2002.

I also recommend checking the original papers for each test to understand their assumptions and limitations:

- [Anderson-Darling test](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test): @anderson1952; @anderson1954.
- Bonett-Seier test: @bonett2002.
- [Cramér-von Mises test](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion): @cramer1928; @anderson1962.
- [D'Agostino test](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test): @dagostino1971; @dagostino1973.
- [Jarque–Bera test](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test): @jarque1980; @bera1981; @jarque1987.
- [Lilliefors (K-S) test](https://en.wikipedia.org/wiki/Lilliefors_test):  @smirnov1948; @kolmogorov1933; @massey1951; @lilliefors1967; @dallal1986.
- [Pearson chi-square test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test): @pearson1900.
- [Shapiro-Francia test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Francia_test): @shapiro1972.
- [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test): @shapiro1965.

$$
\begin{cases}
\text{H}_{0}: \text{The data is normally distributed} \\
\text{H}_{a}: \text{The data is not normally distributed}
\end{cases}
$$

::: {#tbl-sm-5-full-model-residual-diag-normality-2}
```{r}
#| code-fold: true

fit_engine |>
  stats::residuals() |>
  dplyr::as_tibble() |>
  rutils:::normality_summary(col = "value")
```

Summary of statistical tests conducted to assess the normality of the residuals.
:::

#### Linearity

::: {.callout-tip}
Assumption 3
: \hspace{20cm} __Linear mean__. There is a vector of parameters  $\beta = (\beta_{0}, \dots, \beta_{p - 1})$ such that the conditional mean of $Y_{i}$ given the values $z_{1}, \dots , z_{n}$ has the form

$$
z_{i0} \beta_{0} + z_{i1} \beta_{1} + \cdots + z_{ip - 1} \beta_{p - 1}
$$

for $i = 1, \dots, n$ [@degroot2012a, p. 737].

(Linearity of the phenomenon measured [@hair2019, p. 287])
:::

**Assumption 3** is satisfied, as the relationship between the variables is fairly linear. As shown in @sec-on-the-latitude-hypothesis, the hypothesis implies linearity and the distribution of the residuals also support this.

::: {#fig-sm-5-full-model-residual-diag-fitted-values-1}
```{r}
#| code-fold: true

plot <-
  fit |>
  broom::augment(data) |>
  ggplot2::ggplot(ggplot2::aes(.pred, .resid)) +
  ggplot2::geom_point() +
  ggplot2::geom_hline(
    yintercept = 0,
    color = "black",
    linewidth = 0.5,
    linetype = "dashed"
  ) +
  ggplot2::geom_smooth(color = brandr::get_brand_color("orange")) +
  ggplot2::labs(x = "Fitted values", y = "Residuals")

plot |> print() |> rutils::shush()
```

Residual plot showing the relationship between fitted values and residuals. The dashed black line represent zero residuals, indicating an ideal model fit. The orange line indicate the conditional mean of residuals.
:::

::: {#fig-sm-5-full-model-residual-diag-linearity-1}
```{r}
#| code-fold: true

plots <- fit_engine |> olsrr::ols_plot_resid_fit_spread(print_plot = FALSE)

for (i in seq_along(plots)) {
  q <- plots[[i]] + ggplot2::labs(title = ggplot2::element_blank())

  q <- q |> ggplot2::ggplot_build()
  q$data[[1]]$colour <- brandr::get_brand_color("orange")
  q$plot$layers[[1]]$constructor$color <- brandr::get_brand_color("orange")

  plots[[i]] <- q |> ggplot2::ggplot_gtable() |> ggplotify::as.ggplot()
}

patchwork::wrap_plots(plots$fm_plot, plots$rsd_plot, ncol = 2)
```

Residual fit spread plots to detect non-linearity, influential observations, and outliers. The side-by-side plots show the centered fit and residuals, illustrating the variation explained by the model and what remains in the residuals. Inappropriately specified models often exhibit greater spread in the residuals than in the centered fit. "Proportion Less" indicates the cumulative distribution function, representing the proportion of observations below a specific value, facilitating an assessment of model performance.
:::

The [Ramsey's RESET test](https://en.wikipedia.org/wiki/Ramsey_RESET_test) indicates that the model has no omitted variables. This test examines whether non-linear combinations of the fitted values can explain the response variable.

Learn more about the Ramsey's RESET test in: @ramsey1969.

$$
\begin{cases}
\text{H}_{0}: \text{The model has no omitted variables} \\
\text{H}_{a}: \text{The model has omitted variables}
\end{cases}
$$

```{r}
#| code-fold: false

fit_engine |> lmtest::resettest(power = 2:3)
```

```{r}
#| code-fold: false

fit_engine |> lmtest::resettest(type = "regressor")
```

#### Homoscedasticity (Common Variance)

::: {.callout-tip}
Assumption 4
: \hspace{20cm} __Common variance__ (homoscedasticity). There is as parameter $\sigma^{2}$ such the conditional variance of $Y_{i}$ given the values $z_{1}, \dots , z_{n}$ is $\sigma^{2}$ for $i = 1, \dots, n$.

(Constant variance of the error terms [@hair2019, p. 287])
:::

**Assumption 4** is satisfied. When comparing the standardized residuals ($\sqrt{|\text{Standardized Residuals}|}$) spread to the fitted values, we can observe that the residuals are fairly constant across the range of values. This suggests that the residuals have a constant variance.

##### Visual Inspection

```{r}
#| eval: false
#| include: false

# Based on:
# https://sscc.wisc.edu/sscc/pubs/RegDiag-R/homoscedasticity.html#:~:text=We%20must%20plot%20the%20residuals%20against%20the%20fitted%20values%20and%20against%20each%20of%20the%20predictors.
```

::: {#fig-sm-5-full-model-diag-homoscedasticity-1}
```{r}
#| code-fold: true

plot <-
  fit |>
  stats::predict(data) |>
  dplyr::mutate(
    .sd_resid =
      fit_engine |>
      stats::rstandard() |>
      abs() |>
      sqrt()
  ) |>
  ggplot2::ggplot(ggplot2::aes(.pred, .sd_resid)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(color = brandr::get_brand_color("orange")) +
  ggplot2::labs(
    x = "Fitted values",
    y = latex2exp::TeX("$\\sqrt{|Standardized \\ Residuals|}$")
  )

plot |> print() |> rutils::shush()
```

Relation between the fitted values of the model and its standardized residuals.
:::

```{r}
#| eval: false
#| include: false

fit |>
  quartor:::tabset_panel_var_homoscedasticity(
    data = data,
    cols = c(
      "msf_sc",
      fit_engine |>
      stats::coef() |>
      names() |>
      magrittr::extract(-1) |>
      stringr::str_subset("^sex", negate = TRUE)
    ),
    col_labels = c(
      "MSF~sc~ (Chronotype proxy) (Seconds)",
      "Age (years)",
      "Longitude (Decimal degrees)",
      "Monthly average global horizontal irradiance (Wh/m²)",
      "Annual average global horizontal irradiance (Wh/m²)",
      "Daylight on the March equinox (Seconds)",
      "Daylight on the June solstice (Seconds)",
      "Daylight on the December solstice (Seconds)"
    ),
    heading = "######",
    suffix = "sm-5-fm",
    root = ".."
  )
```

{{< include ../qmd/_tabset-panel-sm-5-fm-var-homoscedasticity.qmd >}}

##### Breusch-Pagan Test

The [Breusch-Pagan test](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) test indicates that the residuals exhibit constant variance.

Learn more about the Breusch-Pagan test in: @breusch1979 and @koenker1981.

$$
\begin{cases}
\text{H}_{0}: \text{The variance is constant} \\
\text{H}_{a}: \text{The variance is not constant}
\end{cases}
$$

```{r}
#| code-fold: false

# With studentising modification of Koenker
fit_engine |> lmtest::bptest(studentize = TRUE)
```

```{r}
#| code-fold: false

fit_engine |> lmtest::bptest(studentize = FALSE)
```

```{r}
# Using the studentized modification of Koenker.
fit_engine |> skedastic::breusch_pagan(koenker = TRUE)
```

```{r}
fit_engine |> skedastic::breusch_pagan(koenker = FALSE)
```

```{r}
#| code-fold: false

fit_engine |> car::ncvTest()
```

```{r}
#| code-fold: false

fit_engine_2 |> olsrr::ols_test_breusch_pagan()
```

#### Independence

::: {.callout-tip}
Assumption 5
: \hspace{20cm} __Independence__. The random variables $Y_{1}, \dots , Y_{n}$ are independent given the observed $z_{1}, \dots , z_{n}$ [@degroot2012a, p. 737].

(Independence of the error terms [@hair2019, p. 287])
:::

**Assumption 5** is satisfied. Although the residuals show some autocorrelation, they fall within the acceptable range of the Durbin–Watson statistic ($1.5$ to $2.5$). It's also important to note that the observations for each predicted value are not related to any other prediction; in other words, they are not grouped or sequenced by any variable (by design) (see @hair2019[p. 291] for more information).

Many authors don't consider autocorrelation tests for linear regression models, as they are more relevant for time series data. However, I include them here just for reference.

##### Visual Inspection

::: {#fig-sm-5-full-model-diag-independence-1}
```{r}
#| code-fold: true

fit_engine |>
  residuals() |>
  forecast::ggtsdisplay(
    lag.max = 30,
    theme = ggplot2::theme_get()
  )
```

Time series plot of the residuals along with its AutoCorrelation Function (ACF) and Partial AutoCorrelation Function (PACF).
:::

##### Correlations

@tbl-sm-5-full-model-residual-diag-independence-1 shows the relative importance of independent variables in determining the response variable. It highlights how much each variable uniquely contributes to the R-squared value, beyond what is explained by the other predictors.

::: {#tbl-sm-5-full-model-residual-diag-independence-1}
```{r}
#| code-fold: true

fit_engine |> olsrr::ols_correlations()
```

Correlations between the dependent variable and the independent variables, along with the zero-order, part, and partial correlations. The zero-order correlation represents the Pearson correlation coefficient between the dependent and independent variables. Part correlations indicate how much the R-squared would decrease if a specific variable were removed from the model, while partial correlations reflect the portion of variance in the response variable that is explained by a specific independent variable, beyond the influence of other predictors in the model.
:::

##### Newey-West Estimator

The [Newey-West estimator](https://en.wikipedia.org/wiki/Newey%E2%80%93West_estimator) is a method used to estimate the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) of the coefficients in a regression model when the residuals are autocorrelated.

Learn more about the Newey-West estimator in: @newey1987 and @newey1994.

```{r}
#| eval: false
#| code-fold: false

fit_engine |> sandwich::NeweyWest()
```

##### Durbin-Watson Test

The [Durbin-Watson test](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) is a statistical test used to detect the presence of autocorrelation at lag $1$ in the residuals from a regression analysis. The test statistic ranges from $0$ to $4$, with a value of $2$ indicating no autocorrelation. Values less than $2$ indicate positive autocorrelation, while values greater than $2$ indicate negative autocorrelation [@fox2016].

A common rule of thumb in the statistical community is that a Durbin-Watson statistic between $1.5$ and $2.5$ suggests little to no autocorrelation.

Learn more about the Durbin-Watson test in: @durbin1950; @durbin1951; and @durbin1971.

$$
\begin{cases}
\text{H}_{0}: \text{Autocorrelation of the disturbances is 0} \\
\text{H}_{a}: \text{Autocorrelation of the disturbances is not equal to 0}
\end{cases}
$$

```{r}
#| code-fold: false

car::durbinWatsonTest(fit_engine)
```

##### Ljung-Box Test

The Ljung–Box test is a statistical test used to determine whether any autocorrelations within a time series are significantly different from zero. Rather than testing randomness at individual lags, it assesses the "overall" randomness across multiple lags.

Learn more about the [Ljung-Box test](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test) in: @box1970 and @ljung1978.

$$
\begin{cases}
\text{H}_{0}: \text{Residuals are independently distributed} \\
\text{H}_{a}: \text{Residuals are not independently distributed}
\end{cases}
$$

```{r}
#| code-fold: false

fit_engine |>
  stats::residuals() |>
  stats::Box.test(type = "Ljung-Box", lag = 10)
```

#### Colinearity/Multicollinearity

Daylight duration for the March equinox, June solstice, and December solstice exhibited high multicollinearity, with a variance inflation factor (VIF) greater than $1000$. However, since these variables are grouped as latitude proxies, this does not significantly impact the analysis. The primary focus is on the combined effect of the group, rather than the individual contributions of each variable.

##### Variance Inflation Factor (VIF)

The [Variance Inflation Factor (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor) indicates the effect of other independent variables on the standard error of a regression coefficient. The VIF is directly related to the tolerance value ($\text{VIF}_{i} = 1/\text{TO}L$). High VIF values (larger than ~5 [@struck2024]) suggest significant collinearity or multicollinearity among the independent variables [@hair2019, p. 265].

::: {#fig-model-diag-colinearity-1}
```{r}
#| code-fold: true

diag_sum_plots <-
  fit_engine_2 |>
  performance::check_model(panel = FALSE) |>
  plot() |>
  rutils::shush()

diag_sum_plots$VIF +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    subtitle = ggplot2::element_blank()
  ) +
  ggplot2::theme_get() +
  ggplot2::theme(
    legend.position = "right",
    axis.title = ggplot2::element_text(size = 11, colour = "black"),
    axis.text = ggplot2::element_text(colour = "black"),
    axis.text.y = ggplot2::element_text(size = 9),
    legend.text = ggplot2::element_text(colour = "black")
  )
```

Variance Inflation Factors (VIF) for each predictor variable. VIFs below 5 are considered acceptable. Between 5 and 10, the variable should be examined. Above 10, the variable must considered highly collinear.
:::

::: {#tbl-sm-5-full-model-residual-diag-colinearity-1}
```{r}
#| code-fold: true

fit_engine |> olsrr::ols_vif_tol()
```

Variance Inflation Factors (VIF) and tolerance values for each predictor variable.
:::

::: {#tbl-sm-5-full-model-residual-diag-colinearity-2}
```{r}
#| code-fold: true

fit_engine_2 |> performance::check_collinearity()
```

Variance Inflation Factors (VIF) and tolerance values for each predictor variable.
:::

##### Condition Index

The [condition index](https://en.wikipedia.org/wiki/Condition_number) is a measure of multicollinearity in a regression model. It is based on the [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of the correlation matrix of the predictors. A condition index of 30 or higher is generally considered indicative of significant collinearity [@belsley2004, p. 112-114].

::: {#tbl-sm-5-full-model-residual-diag-colinearity-2}
```{r}
#| code-fold: true

fit_engine |> olsrr::ols_eigen_cindex()
```

Condition indexes and eigenvalues for each predictor variable.
:::

#### Measures of Influence

In this section, I check several measures of influence that can be used to assess the impact of individual observations on the model estimates.

::: {.callout-tip}
Leverage points
: Leverage is a measure of the distance between individual values of a predictor and other values of the predictor. In other words, a point with high leverage has an x-value far away from the other x-values. Points with high leverage have the potential to influence the model estimates [@struck2024; @hair2019, p. 262; @nahhas2024].
:::

::: {.callout-tip}
Influence points
: Influence is a measure of how much an observation affects the model estimates. If an observation with large influence were removed from the dataset, we would expect a large change in the predictive equation [@struck2024; @nahhas2024].
:::

##### Standardized Residuals

Standardized residuals are a rescaling of the residual to a common basis by dividing each residual by the standard deviation of the residuals [@hair2019, p. 264].

```{r}
#| code-fold: false

fit_engine |> stats::rstandard() |> head()
```

::: {#fig-sm-5-full-model-diag-influence-1}
```{r}
#| code-fold: true

dplyr::tibble(
  x = seq_len(nrow(data)),
  std = stats::rstandard(fit_engine)
) |>
  ggplot2::ggplot(
    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)
  ) +
  ggplot2::geom_linerange(color = "black") +
  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color("orange")) +
  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color("orange")) +
  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +
  ggplot2::labs(
    x = "Observation",
    y = "Standardized residual"
  )
```

Standardized residuals for each observation.
:::

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_resid_stand()
```

##### Studentized Residuals

[Studentized residuals](https://en.wikipedia.org/wiki/Studentized_residual) are a commonly used variant of the standardized residual. It differs from other methods in how it calculates the standard deviation used in standardization. To minimize the effect of any observation on the standardization process, the standard deviation of the residual for observation $i$ is computed from regression estimates omitting the $i$th observation in the calculation of the regression estimates [@hair2019, p. 264].

```{r}
#| code-fold: false

fit_engine |> stats::rstudent() |> head()
```

::: {#fig-sm-5-full-model-diag-influence-2}
```{r}
#| code-fold: true

dplyr::tibble(
  x = seq_len(nrow(data)),
  std = stats::rstudent(fit_engine)
) |>
  ggplot2::ggplot(
    ggplot2::aes(x = x, y = std, ymin = 0, ymax = std)
  ) +
  ggplot2::geom_linerange(color = "black") +
  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color("orange")) +
  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color("orange")) +
  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +
  ggplot2::labs(
    x = "Observation",
    y = "Studentized residual"
  )
```

Studentized residuals for each observation.
:::

::: {#fig-sm-5-full-model-diag-influence-3}
```{r}
#| code-fold: true

fit |>
  broom::augment(data) |>
  dplyr::mutate(
    std = stats::rstudent(fit_engine)
  ) |>
  ggplot2::ggplot(ggplot2::aes(.pred, std)) +
  ggplot2::geom_point(color = "black") +
  ggplot2::geom_hline(yintercept = 2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = -2, color = brandr::get_brand_color("grey")) +
  ggplot2::geom_hline(yintercept = 3, color = brandr::get_brand_color("orange")) +
  ggplot2::geom_hline(yintercept = -3, color = brandr::get_brand_color("orange")) +
  ggplot2::scale_y_continuous(breaks = seq(-3, 3)) +
  ggplot2::labs(
    x = "Predicted value",
    y = "Studentized residual"
  )
```

Relation between studentized residuals and fitted values.
:::

::: {#fig-sm-5-full-model-diag-influence-4}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_resid_lev(threshold = 2, print_plot = FALSE)

plot$plot +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    y = "Studentized residual"
  )
```

Relation between studentized residuals and their leverage points.
:::

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_resid_stud()
```

```{r}
#| eval: false
#| include: false

fit_engine |>
  car::influenceIndexPlot(
    vars = "Studentized",
    id = FALSE,
    main = NULL
  )
```

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_resid_stud_fit()
```

##### Hat Values

The hat value indicates how distinct an observation’s predictor values are from those of other observations. Observations with high hat values have high leverage and may be, though not necessarily, influential. There is no fixed threshold for what constitutes a “large” hat value; instead, the focus must be on observations with hat values significantly higher than the rest [@nahhas2024; @hair2019, p. 261].

```{r}
#| code-fold: false

fit_engine |> stats::hatvalues() |> head()
```

::: {#fig-sm-5-full-model-diag-influence-5}
```{r}
#| code-fold: true

dplyr::tibble(
  x = seq_len(nrow(data)),
  hat = stats::hatvalues(fit_engine)
) |>
  ggplot2::ggplot(
    ggplot2::aes(x = x, y = hat, ymin = 0, ymax = hat)
  ) +
  ggplot2::geom_linerange(color = brandr::get_brand_color("black")) +
  ggplot2::labs(
    x = "Observation",
    y = "Hat value"
  )
```

Hat values for each observation.
:::

```{r}
#| eval: false
#| include: false

fit_engine |>
  car::influenceIndexPlot(
    vars = "hat",
    id = FALSE,
    main = NULL
  )
```

##### Cook's Distance

The [Cook's D](https://en.wikipedia.org/wiki/Cook%27s_distance) measures each observation's influence on the model's fitted values. It is considered one of the most representative metrics for assessing overall influence [@hair2019].

A common practice is to flag observations with a Cook's distance of 1.0 or greater. However, a threshold of $4 / (n - k - 1)$, where $n$ is the sample size and $k$ is the number of independent variables, is suggested as a more conservative measure in small samples or for use with larger datasets [@hair2019].

Learn more about Cook's D in: @cook1977; @cook1979.

```{r}
#| code-fold: false

fit_engine |> stats::cooks.distance() |> head()
```

::: {#fig-sm-5-full-model-diag-influence-6}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_cooksd_bar(type = 2, print_plot = FALSE)

# The following procedure changes the plot aesthetics.
q <- plot$plot + ggplot2::labs(title = ggplot2::element_blank())
q <- q |> ggplot2::ggplot_build()
q$data[[5]]$label <- ""

q |> ggplot2::ggplot_gtable() |> ggplotify::as.ggplot()
```

Cook's distance for each observation along with a threshold line at $4 / (n - k - 1)$.
:::

::: {#fig-sm-5-full-model-diag-influence-7}
```{r}
#| code-fold: true

diag_sum_plots <-
  fit_engine_2 |>
  performance::check_model(
    panel = FALSE,
    colors = c("blue", "black", "black")
  ) |>
  plot() |>
  rutils::shush()

plot <-
  diag_sum_plots$OUTLIERS +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    subtitle = ggplot2::element_blank(),
    x = "Leverage",
    y = "Studentized residuals"
  ) +
  ggplot2::theme_get() +
  ggplot2::theme(
    legend.position = "right",
    axis.title = ggplot2::element_text(size = 11, colour = "black"),
    axis.text = ggplot2::element_text(colour = "gray25"),
    axis.text.y = ggplot2::element_text(size = 9)
  )

plot <- plot |> ggplot2::ggplot_build()

# The following procedure changes the plot aesthetics.
for (i in c(1:9)) {
  # "#1b6ca8" "#3aaf85"
  plot$data[[i]]$colour <- dplyr::case_when(
    plot$data[[i]]$colour == "blue" ~
      ifelse(i == 4, brandr::get_brand_color("orange"), "blue"),
    plot$data[[i]]$colour == "#1b6ca8" ~ "black",
    plot$data[[i]]$colour == "darkgray" ~ "black",
    TRUE ~ plot$data[[i]]$colour
  )
}

plot |> ggplot2::ggplot_gtable() |> ggplotify::as.ggplot()
```

Relation between studentized residuals and their leverage points. The blue line represents the [Cook's distance](https://en.wikipedia.org/wiki/Cook%27s_distance). Any points outside the contour lines are influential observations.
:::

```{r}
#| eval: false
#| include: false

fit_engine |> olsrr::ols_plot_cooksd_chart()
```

```{r}
#| eval: false
#| include: false

fit_engine |>
  car::influenceIndexPlot(
    vars = "Cook",
    id = FALSE,
    main = NULL
  )
```

#####  Influence on Prediction (DFFITS)

[DFFITS](https://en.wikipedia.org/wiki/DFFITS) (difference in fits) is a standardized measure of how much the prediction for a given observation would change if it were deleted from the model. Each observation’s DFFITS is standardized by the standard deviation of fit at that point [@struck2024].

The best rule of thumb is to classify as influential any standardized values that exceed $2 \sqrt{(p / n)}$, where $p$ is the number of independent variables + 1 and $n$ is the sample size  [@hair2019, p. 261].

Learn more about DDFITS in: @welsch1977 and @belsley2004.

```{r}
#| code-fold: false

fit_engine |> stats::dffits() |> head()
```

::: {#fig-sm-5-full-model-diag-influence-8}
```{r}
#| code-fold: true

plot <- fit_engine |>
  olsrr::ols_plot_dffits(print_plot = FALSE)

plot$plot + ggplot2::labs(title = ggplot2::element_blank())
```

Standardized DFFITS (difference in fits) for each observation.
:::

#####  Influence on Parameter Estimates (DFBETAS)

[DFBETAS](https://en.wikipedia.org/wiki/Influential_observation#:~:text=measures%20of%20influence) are a measure of the change in a regression coefficient when an observation is omitted from the regression analysis. **The value of the DFBETA is in terms of the coefficient itself** [@hair2019, p. 261]. A cutoff for what is considered a large DFBETAS value is $2 / \sqrt{n}$, where $n$ is the number of observations. [@struck2024].

Learn more about DFBETAS in: @welsch1977 and @belsley2004.

```{r}
#| code-fold: false

fit_engine |> stats::dfbeta() |> head()
```

```{r}
#| eval: false
#| include: false

fit_engine |>
  quartor:::tabset_panel_coef_dfbetas(
    heading = "######",
    suffix = "sm-5-fm",
    root = ".."
  )
```

{{< include ../qmd/_tabset-panel-sm-5-fm-coef-dfbetas.qmd >}}

##### Hadi's Measure

Hadi’s measure of influence is based on the idea that influential observations can occur in either the response variable, the predictors, or both.

Learn more about Hadi's measure in: @chatterjee2012.

::: {#fig-sm-5-full-model-diag-influence-12}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_hadi(print_plot = FALSE)

plot +
  ggplot2::labs(
    title = ggplot2::element_blank(),
    y = "Hadi's measure"
  )
```

Hadi's influence measure for each observation.
:::

::: {#fig-sm-5-full-model-diag-influence-13}
```{r}
#| code-fold: true

plot <-
  fit_engine |>
  olsrr::ols_plot_resid_pot(print_plot = FALSE)

plot + ggplot2::labs(title = ggplot2::element_blank())
```

Potential-residual plot classifying unusual observations as high-leverage points, outliers, or a combination of both.
:::

## Hypothesis Testing

Following the criteria outlined in the methodology supplementary material, we now address the hypothesis for this test:

__Hypothesis__
: *Latitude is associated with chronotype distributions*, with populations closer to the equator exhibiting, on average, a shorter or more morning-oriented circadian phenotype compared to those residing near the poles.

$$
\begin{cases}
\text{H}_{0}: \Delta \ \text{Adjusted} \ \text{R}^{2} < \text{MES} \\
\text{H}_{a}: \Delta \ \text{Adjusted} \ \text{R}^{2} \geq \text{MES}
\end{cases}
$$

### F-Test

The results indicate that the F-test is significant ($\alpha < 0.05$), meaning that the model including the latitude variables differs from the model without them.

$$
\text{F} = \cfrac{\text{R}^{2}_{f} - \text{R}^{2}_{r} / (k_{f} - k_{R})}{(1 - \text{R}^{2}_{f}) / (\text{N} - k_{f} - 1)}
$$

```{r}
#| code-fold: false

f_test <- stats::anova(fit_engine_restricted, fit_engine_full)

print(f_test)
```

```{r}
#| code-fold: false

n <- nrow(data)
k_res <- length(stats::coefficients(fit_engine_restricted)) - 1
k_full <- length(stats::coefficients(fit_engine_full)) - 1

r_squared_restricted <- summary(fit_engine_restricted)$r.squared
r_squared_full <- summary(fit_engine_full)$r.squared

((r_squared_full - r_squared_restricted) /
    (k_full - k_res)) / ((1 - r_squared_full) / (n  - k_full - 1))
```

```{r}
#| include: false

quartor:::write_in_results_yml(
  list(
    hta_f_test = f_test
  )
)
```

### Effect Size

The results show that the $\Delta \ \text{Adjusted} \ \text{R}^{2}$ value is below the Minimal Effect Size (MES) threshold ($\text{R}^{2} \approx 0.01960784$), indicating that adding latitude does not meaningfully improve the model’s fit.

$$
\text{Cohen's } f^2 = \cfrac{\text{R}^{2}_{f} - \text{R}^{2}_{r}}{1 - \text{R}^{2}_{f}} = \cfrac{\Delta \text{R}^{2}}{1 - \text{R}^{2}_{f}}
$$

$$
\text{MES} = \text{Cohen's } f^2 \text{small threshold} = 0.02 \\
$$

$$
0.02 = \cfrac{\text{R}^{2}}{1 - \text{R}^{2}} \quad \text{or} \quad \text{R}^{2} = \cfrac{0.02}{1.02} \eqsim 0.01960784
$$

::: {#tbl-sm-5-hypothesis-test-1}
```{r}
#| code-fold: true

adj_r_squared_restricted <-
  fit_engine_restricted |>
  rutils:::summarize_r2(
    n = nrow(data),
    k = length(fit_engine_restricted$coefficients) - 1,
    ci_level = 0.95
  )

adj_r_squared_restricted
```

Confidence interval for the adjusted R-squared of the **restricted model**. LCL correspond to the lower limit, and UCL to the upper limit.
:::

::: {#tbl-sm-5-hypothesis-test-2}
```{r}
#| code-fold: true

adj_r_squared_full <-
  fit_engine_full |>
  rutils:::summarize_r2(
    n = nrow(data),
    k = length(fit_engine_full$coefficients) - 1,
    ci_level = 0.95
  )

adj_r_squared_full
```

Confidence interval for the adjusted R-squared of the **full model**. LCL correspond to the lower limit, and UCL to the upper limit.
:::

::: {#tbl-sm-5-hypothesis-test-3}
```{r}
dplyr::tibble(
  name = c(
    "adj_r_squared_res",
    "adj_r_squared_full",
    "diff"
  ),
  value = c(
    adj_r_squared_restricted$value[1],
    adj_r_squared_full$value[1],
    adj_r_squared_full$value[1] - adj_r_squared_restricted$value[1]
  )
)
```

Comparison between the coefficients of determination ($\text{R}^2$) of the restricted and full models.
:::

::: {#tbl-sm-5-hypothesis-test-4}
```{r}
effect_size <- rutils:::cohens_f_squared_summary(
  base_r_squared = adj_r_squared_restricted,
  new_r_squared = adj_r_squared_full
)

effect_size |> rutils:::list_as_tibble()
```

Effect size between the restricted and full models based on Cohen's $f^2$.
:::

```{r}
#| include: false

quartor:::write_in_results_yml(
  list(
    hta_restricted_model_adj_r_squared = adj_r_squared_restricted,
    hta_full_model_adj_r_squared = adj_r_squared_full,
    hta_effect_size = effect_size
  )
)
```

## Conclusion

Based on the hypothesis test results, **we must reject the alternative hypothesis in favor of the null hypothesis**. Latitude does not meaningful contribute to explaining the variance in chronotype.

<br>

For questions regarding these computations, please contact the author at <danvartan@gmail.com>.
